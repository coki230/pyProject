{"docstore/metadata": {"d8047731-f86b-47f5-9208-b627a0d9a405": {"doc_hash": "4656ec06520331e6bc967dc9532ac72e1c1a7873c2a6ae627c015bc7ec2ee74c"}, "bf9bfd21-e034-4a71-9fd3-af6a09b58be6": {"doc_hash": "24b6c176e23e1f18f547fae5130ac691da04fdd9e8b89ef02c699711be0c4684"}, "53750600-526d-4e85-a907-f0c5560deac2": {"doc_hash": "15be7c4d0f8268163f43fb06dcc95d3e8d6328591162422ab7e3102ffbdddbc5"}, "794e12a2-dc7f-490f-9b9c-e6c0c5b713d8": {"doc_hash": "53ed719cbfa21670cd8a7a1def4b5a05a31aa32a2c5355090c685c58d4331ff6"}, "dcd5cab4-4994-4e98-a677-a4004d4510dd": {"doc_hash": "c0e26fa4f241a7bf9df43af1675e6a343f7d15f7244ac51c169d22051826698b"}, "12bb99a6-6534-4f08-8a28-adcc6179abd8": {"doc_hash": "2f774c8308b25823e7491d0d3a6cc73c8863f39fe98d501d0c37af4a0c2c0364"}, "095af734-1710-45f9-b28f-5380e3e8c94b": {"doc_hash": "e248a387075b53075732ec488a5a0b4732fca3f5d002b1015110ef9d1e6e91fb"}, "41d73e69-bf24-436f-8a6a-f0d38a33ed24": {"doc_hash": "edd7985b8fbced0e44da583050270469df8531a9851e5b61c8d96b672d7c2b4d"}, "000bae20-f570-4e2a-82eb-54c7762d1d14": {"doc_hash": "bd4d7b653f84e0ff4c9dd937dc6d6b4d5abe2411a648079e7f72fd89e963305f"}, "5c974810-a925-4419-b2dc-2920e9794f5a": {"doc_hash": "d0937a6519962a01fee9af63ed15c64139649fe7339f1e6cf757dc429c8a2d43"}, "d76d575c-27ca-400b-aa40-53acd3d5fa2c": {"doc_hash": "338d3f10528f06b34255e65271a275307bea7f09d77ff75ceec13983cb7d9459"}, "ccc2ca28-a567-4613-bada-74579d4a3f41": {"doc_hash": "925dd393b7458574e39cc48b1c50dd5c54c6bebd8d3bcc4546987311d8b13716"}, "a3c8ca37-a928-419f-9ec9-af78623d7387": {"doc_hash": "e6e5df5f464c898718fb23fae5b1920bba6e980c26a8e76aa65b6da0d8b060e4"}, "91c71f01-86f7-463a-9535-51387506932a": {"doc_hash": "f09ee596e588768d52e34574862792c4fb88c54c5a83dd2268f6413b96780fb4"}, "28094a3c-1118-4e0e-9f18-db3d88d6a805": {"doc_hash": "ac01893648a4bc7d88195dfe6b6acd5b1fa25f4124d33319ae347f3b44b96a59"}, "13b91b49-bb8d-499f-aa64-8a4fe3899b20": {"doc_hash": "a99f676a59d43faea3a03f7f23b379a648edd303b7605fffd1db8a46728d5f52"}, "184a637a-3adb-4e11-9f39-d4e7b8e927cd": {"doc_hash": "8ee04fa7bbdf119295b825bb5c18a894a98aec9abf58976a42d8604e4e121c75"}, "f27466b4-bc1b-45ce-95d0-10da581e0082": {"doc_hash": "8c19efce3c71e513a24c3029ae14987547cf89b3633afd9209af83c31f7a815f"}, "c4b0ddfa-a7c2-47c1-ab9f-799a5cef3b70": {"doc_hash": "82583219d29209125fc4f2ccf3a6576fc2214c2e03ca2e23d9a73888132ea09c"}, "0d237f8a-d803-4e5c-a620-dd5c08937c88": {"doc_hash": "3f2cfbf8f844fc8b0c2cee84e54d5c52b984c8ddea3846abce0974be417e89b7"}, "22e29a89-0ea5-435c-9231-ae85f0aa253f": {"doc_hash": "f59854ea5341b483010775620280fe2eec4d8aa7376781e822899328e58271ce"}, "369dfcf4-8fee-48ce-bab0-b705bacd1113": {"doc_hash": "473e2c429d18d596c7e58481b26efcb4834aebece97a0af321501c734d49665d"}, "6b4a884d-dff3-4500-8381-4bf04c2ebcf9": {"doc_hash": "75c0c0dd830084d78716be9297ebfdfdba1ea48972137b206af108342cfe0344"}, "e878a92b-77bd-4f46-bd89-27e41eaf3d37": {"doc_hash": "63f1ae6e44e80380b13b5d8fe3d33eab56dea101dae4131c6d3b312a1bf0ea61"}, "ddc4ec1e-4c5e-4303-8ce5-d01893570a3a": {"doc_hash": "1b31088e9a680037ca07891f940c17b96ec5b493fbe7266b8d654ac16d066041"}, "84c1476f-04c0-4513-ac00-8d57c2608ec4": {"doc_hash": "e2bb79a930a57b31d00088778fdc478a22d674f5cf3d3e3d9e78d68b13cc518d"}, "d4d848c0-2093-4308-aa3e-e837896edf23": {"doc_hash": "5c13051ad503c379115d7b92fcf4775e56585d9fe5b76ce06f27976eacbeb67c"}, "1d1e51dd-15f4-4ead-a918-a966a805368f": {"doc_hash": "13a6210e57c00ebfc9e8db93bff90012a866c86e358048051611207b4aa63f01"}, "7a543f68-cc1d-4963-ba9f-b1690bfe8d93": {"doc_hash": "55f509757441ddc0e4d3a6600e428a44cdd053344092f00189b3492545c47b49"}, "651ec303-63ef-4d10-a912-8b4f3f215f6a": {"doc_hash": "8eff74395fb76e29434bab4c8a7cd0220ea3269844f205a5d60eb7077d5336c3"}, "531ba80c-edf1-41af-9db0-c50e5d1cb618": {"doc_hash": "455619a151da20160593940f140c918ef1026014e345b6eb46d21e32d854d5ea"}, "64000ff4-a3c7-48e5-912b-18ac77dcbfc4": {"doc_hash": "d5efca1d43e0311a45bbcb94187a0e7d5b0ed2194c22f730adce291937d1c53b"}, "0731307d-2717-4ad4-887a-d4eb1a598ddb": {"doc_hash": "b8b6d1f325366e91e9c289d6e43a7fe2a8816d38cd2878a357e3685fd09b1538"}, "4ddffa52-f628-44c6-9fec-9df76e7c6dec": {"doc_hash": "4ae680798bf67a0091569d374859feceb34b6ba6995de4a38589389a951edbe4"}, "ed16e6c6-3853-48b7-ab78-1d5917c4fb53": {"doc_hash": "64d59e9dcb587b0b7d8d2c997df264653fa201c3e686466383118a1e99b687e0"}, "6ee774ed-f554-44fd-9e7c-cbec957821bc": {"doc_hash": "8d68015828496376c4618c56a64fa9712333e84339c3f9fe61f6c482d599fb40"}, "493874c9-fc7a-46a5-952d-4cc40d7b5f44": {"doc_hash": "def856cd44e70a8d0884ea59a13feae32b7280c0d02d77b97bba962b3200190b"}, "282cd0a3-6fea-423a-aff0-07ce06bfc6c8": {"doc_hash": "de2da7e97ce35558092162a0de41f2b3844fb79967b259c5a881557d78652294"}, "743ef640-2ca5-4ae4-b3f1-8389eb330a94": {"doc_hash": "4ce13e7978a41cc2372c71c1ffe285d95756b1d98d3f38084200cf3f4a9b104e"}, "144cf655-c064-4541-ba7a-f8fd1a26cd71": {"doc_hash": "7ec52780ebc82bcda8d689f3b38fd014f32297e05ee544f9c264f2072bf437a1"}, "83a88c77-8a6d-43d4-8ce1-bb6fa9f573ea": {"doc_hash": "968973fa723c0590ccbbed758935059f7b13b8c9a2713182739fe721c3a27959"}, "4b5305d9-064c-4cf7-bc3f-a73299ea7ce9": {"doc_hash": "73b4f3cd2f9898218d03ae796d5e9ce31e94d63fee2b2a5720f79960a3f2c166"}, "22a429bb-11a1-451a-85e1-6692cdb307f8": {"doc_hash": "0b41ce6bdada338443e1c0d5552a8ba7e8226eb37dff9e7b5824937acb03a202"}, "9decd28f-b1e6-4d2c-ac0e-5a5ef3b79ad9": {"doc_hash": "bec1f91806286dc6086af393a64f44f83c63e064a9dab91214c758cf4977cde6"}, "2afae01f-4b93-4f6a-9ac4-5aa43801a725": {"doc_hash": "ef69ee7bc35cddb31d1509549ec0cb1ebfad9866c878d2285d5506b6e1c85a69"}, "35474a31-1650-473f-a6a2-7d226b67a4db": {"doc_hash": "8eb13942f0f968f2438c373854d621250a8d0c80e989c5a3cd65f497180863fe"}, "cff52d8b-0a61-4d44-97b4-984ce9c21efa": {"doc_hash": "e46927411e02c0cd653d148fbbb3d6bd960ba6ec17eeced68ceb992c5bef8df9"}, "97e39be8-dab5-4c1d-b529-e582e146bdd1": {"doc_hash": "7e6023ac6ebad0613fbf760246057028461352e5533ef19b967b72f3100ea93b"}, "50ff17f2-18c0-4b6c-ad3c-b5051c6d0348": {"doc_hash": "71d8a32165e86c5e52bbad6675a9558e60e29cea6e219cf2638233854d0a6481"}, "a7ea82c2-fb81-481c-a092-c8425d27edc4": {"doc_hash": "d7c688ee78ba4cecb9da45ec4dca5d302230ee224c1f436cff0b768ada901dbe"}, "0b4184a0-8836-4710-8249-a1954c07279c": {"doc_hash": "94df491d4f9a858af23e22e0100f1f7292c0a5e4703276e07a531ea6bba24ba2"}, "0f2074fa-3df6-4cf8-a50f-bf32ab8ff17c": {"doc_hash": "9dccc70f572fc388138fa55c937bbd984dcf884aaa45367cb3eb9551e78ee65d"}, "f51e46b4-d1c6-49f8-bf21-278040d32ff7": {"doc_hash": "3e1cb35a02f4cbfa065fe1370174861216225c302c12d88f8d4200f265cd9d69"}, "d9ba5693-c090-498c-a2ff-3dccdd5e502c": {"doc_hash": "57fe0a3947198bc2d124afe986616a22460ca69c5a4b58207ad31e8fddf36a19"}, "3b1ee236-61e5-46ef-af8f-632cd957e2ff": {"doc_hash": "f3ef8ab8b63fb765908d4236391d268b2b53ad0afcd8b1313b31fe58344bd93c"}, "598c72cb-9121-430a-abcb-869d79fff9fd": {"doc_hash": "f9c6b4b9a96b8bcb3c0afe586ec9c442899bde1ffffbcf85e6c70dc53363e331"}, "d689d4bd-73e1-42bc-a78e-d3db93795f30": {"doc_hash": "9083abb0b846d42772ac865168d207e15edf073b420358657f64bbc240289d91"}, "b22ec1c1-c5d5-40d3-83ba-53760794e3da": {"doc_hash": "297240c6572b4491056ff92f762c0e169baf8b22c7eedf7c293308eabfd9fe0e"}, "e8c1ab1e-850a-49e4-9e1e-597421df5649": {"doc_hash": "b80579a746b49cd47d923ede3e971f0e2b2894923a8bab414d0dded6b2af99c6"}, "f10e9dc3-2a5a-468a-aaa1-b1d512035d41": {"doc_hash": "ffc953c960b593f55d14484626c35f9a9073e8b805acab7717e6183a716cc9d4"}, "4dcd26de-aad6-4043-8fb1-05429ef756b5": {"doc_hash": "f86ff1e003de361d960b4e089e14e47fc5dbc8e1ade6a6be10f487b4581f8ebf"}, "5342f389-52cc-4e81-a12e-9f1a299143a3": {"doc_hash": "0e97ffe208b0ae200775043d1f134aaa403058dc7ae83cae10147c579db175c9"}, "9f1ade06-2dd1-48d0-b7a3-bc36a540f6b8": {"doc_hash": "3c24ae62751c98fcdbbe7682b752059aea650dc9cb5a00f37ce20a5b26d9980a"}, "3d56ea21-939a-4134-95be-abb3686a9fa2": {"doc_hash": "03c67654c74f7b493529b9ed5f6801a2e03901f250c2c5b58b2061e2bcea8514"}, "3fb14a6b-6388-4877-b0bf-db13a8abf066": {"doc_hash": "f2a27d5e0a21cc10e4eb069cade6b5fa872be4f59262cc28db6bc15a09478136"}, "bddd971a-2008-42ac-9747-8bc363c7aceb": {"doc_hash": "78544b2e469ab8788c09b370742b71e7d2fbc9b16595dffb74eea68a4ba4b833"}, "06c61145-4bdd-4d04-8959-67ffc4fb5178": {"doc_hash": "bf569de94c60a3d2a7c4300cb4587a8090ac80f1b3c943073c15b651c0a07593"}, "ecc9298c-fa21-413c-aca9-b403893a6a67": {"doc_hash": "7c4cc4af72eb3f06823aec06514050f52c26a2b9d1d668b0fc2f20c8aa87ff62"}, "f9aca6cf-b9c3-4718-a1ba-6f8422582f27": {"doc_hash": "5d89eaa72bfa4ced62e66a52cc1f20c5056a04edb9d1e0ef7ef0bef0607af1e8"}, "02197a79-7292-489c-a3e3-b093f58eb254": {"doc_hash": "03f209e04017ffe2930b4b0e4c1cc5a0f391c8c9328b3c751f9186c50a64ed97"}, "f01006f3-e81c-448b-aaf2-ace749dcbc12": {"doc_hash": "7cf11f80e711f29d999fc8672991c8f03a8b82b435a911a468e6ac9bcba13a34"}, "b758595c-9fe9-4453-9f25-07e0f59cc78e": {"doc_hash": "d16be9f7460b442a63a180134a03238404d0da6d081b9681d49ccafcf26dbac2"}, "a8c4e4cc-3960-41b9-a068-cd651a991e71": {"doc_hash": "2981dd1c7b975d70a83033653607a8763bcdc603105c01fb5c44b02115c2aa2d"}, "a0bced67-df54-4844-8c99-32c94a26e079": {"doc_hash": "e037ca1cfdaaeb0129695748de3e5d09decee0211d35400b10ee7fc3cae695ac"}, "5591dfe6-336f-4aaf-9fbe-8803a4fdcc58": {"doc_hash": "94fefdb4c1270f52495d8231fc446dfd40b212215b876980f11ef78f36e581e6"}, "a31dff71-b5d6-4337-8b47-a4d535b4c5c5": {"doc_hash": "4024feeb64832b2831c4c3d2df461641850331585fc6ae27eef05ebf2b729ad3"}, "032c09ea-7615-4c94-9857-1a0f998a0d2f": {"doc_hash": "7a3054c1e2574e7625ee83d67b875aea4c927f35b153ba747460496ee4a498aa"}, "265dc7fd-e5e9-42a2-8d38-8ab7264c27db": {"doc_hash": "b0de1c54deebfd3534023ce76ea1c01163808964988b2b978de45320c0766293"}, "db6741ce-55ee-420d-bb37-8414013e6fc3": {"doc_hash": "7b3906ff204e79f42bd08f194764cbd71112763e9745e43789e7b19b5c7b43a1"}, "907c1d0c-b994-4985-a2ec-fa6898e493e6": {"doc_hash": "963b1776677f024636bb5e257b70bc564e28356f92ad03ff082c1000732d7daf"}, "3e8e16ce-9737-4e0d-ae5c-460e14a4b254": {"doc_hash": "252d69610bab69e1e40b28404064556c482b9192c8a0c4065b9a6a68493db5ec"}, "cc0ace0f-4425-41aa-ad09-94b82b6b300c": {"doc_hash": "a79917acc9da5e0c63cb9513899a9e849a7c1e4e393bd94add8a8b6b0dab4c16"}, "04f208d3-930b-4487-904f-35de70308c10": {"doc_hash": "0cbbddfb04cd6ca5e3fa0f562002fc3357fa5eff1ce76929559d6e07c01d3fae"}, "d8fcb4c7-820c-4627-8a61-c90a3d2474c9": {"doc_hash": "a13ed662f88ae06a86c7ae87514decd5be8e173ae090d4d2b6611d2bc82fb90e"}, "aeaba038-126b-481d-9977-6e49ca2805ab": {"doc_hash": "0e2e9fa47b1c506824d544372c9c976025f21ca571fc86d13e47a8feb306934d"}, "3277a520-1d7c-49c3-bc2f-f04e78b01107": {"doc_hash": "6e2f6a250f54c8b6aa0125021a0c9732ab14ec802b3157e455ce96ba6c917287"}, "a72a4de0-7137-4a59-beb1-74ae324df776": {"doc_hash": "3a5afc3fb980b952271edb75d9ea9086c2fa002f38f2a41100f60047e0f9e054"}, "f1ed563c-14c7-435b-aa8d-d9e2eaed4e62": {"doc_hash": "744aff158314b3a8b72457b63544310cce6b85d0ad736480f22ce1dad155f056"}, "f1259d0c-71a6-434f-8da3-1904ce7961a6": {"doc_hash": "f3f8fe7b10466c996732e2309204535fdfd09e613d68634cf75ca282d980fb1e"}, "bf6962ae-3762-4ff0-b100-7155941d0796": {"doc_hash": "785c3d8e4a072b578bf3f95a1ec6ebe6164a18118e3032ce8bf20fd80e7d5930"}, "5b604ca9-42f9-4a4a-9d21-fb4b8440e738": {"doc_hash": "b88f118e679578535a6745ea7692f07e901477758a7d5211c6415c32937be1c1"}, "730842b9-95f3-4ef6-9aea-37f0bb205b1b": {"doc_hash": "c284040afb624cbcaa306fc7cc0adb08fee0c50c6bae89b1aaf9b154ff118437"}, "12c875a7-0ba4-4910-b9a0-837d53d3bfb4": {"doc_hash": "f9dc85bd6ee993e09eeedbc2d487ff1329601915caa2dc4803a823c06adae4cc"}, "f3a9a72e-b45a-478a-9d4e-c38342d2494b": {"doc_hash": "f1bd580de5e9c08909a73c5ebc966089719339782fcb0bbd81500445eeba3229"}, "34c9675f-17b6-4882-a9b4-04e1d51c2265": {"doc_hash": "3919d62650d2c142d4ef7522ef29a1000d08deff8cec343c694bebdf2a72b1a1"}, "d6330fe0-a870-4567-abe7-ae77c30f0378": {"doc_hash": "96d72d7bb677c80367fb31a19dc0ab79f2e4b14b3a9b724b319fdbe63c1e8a69"}, "df6399d0-534d-4ba8-899d-20d0cc62bf77": {"doc_hash": "873f84f292623e614ba857451498f585b7fd8927638cd0323528d6301e48151f"}, "f5b2a9af-88bd-48b7-95e9-333bd1f1eb3b": {"doc_hash": "356c34f6c8014e9d12948a30f54560b48ec19460f32d55d6fa5da55eb5b3092f"}, "f98a2dfb-d725-424c-af17-2b263c963977": {"doc_hash": "3afd49006aa930f6404527e5b3b10f1c7461f405b0c13444a4a390c39c648267"}, "d6900a33-1c59-4ea3-bc89-37250cab5e54": {"doc_hash": "9944edd298b5f5e31cc1842cf3f5758ac4cca7b97d1279d6be29a038aeb42338"}, "67a5f6f5-c69b-430d-8519-8688e51feadb": {"doc_hash": "b127a0020beebaa0256866380d20b3da74bf67d3ea36e4924c16ff7b5e91a0c6"}, "9db7cb74-8532-4825-bc1c-e46dc2cc5282": {"doc_hash": "ed9de8ede003f06c06564cd6da708c6f57019a669145a85b23f9cff2671932b8"}, "1b8c466c-57cd-4fc9-821b-088f59bcf067": {"doc_hash": "8863ed520655a7ae9c68e087fa780eee242ab1d13ec283fd7dd4ad17d6803b6b"}, "7a115120-4afe-41cf-8de6-7f7b2b73df43": {"doc_hash": "c07a073f485211c6cfa48f33eeb283c4ca3b9fc60d53957b743de6bc7760bb89"}, "440a2d4c-60d8-471f-8199-ad6d5d5a7f35": {"doc_hash": "59ed451483ba5ca55720694ba3bdae860af0ff44ad5920a565a6f809445eec93"}, "1817412a-eff8-4669-9db7-3a87f3eb975d": {"doc_hash": "4e831c6df625446fe726419c84e388d979c5a4c4d88107080737d021c7ff82cd"}, "e8f8fa17-04e5-42f6-a76e-4da0c97ce9e0": {"doc_hash": "aa4f611a583258baa849046925a19fdffd7aba3f5b56ffa083bd2e06ed5fb33a"}, "5061526f-eb15-4894-be93-fdc86eb9ef1f": {"doc_hash": "3fde8c5b30dc96221db39aef860ee1fdfc8f37e54e78010a02a982edaa9a6cd0"}, "ea23fb20-3622-41be-9eeb-f7b1ffcd6f3e": {"doc_hash": "607a1d313db8059c6a871ff24cf63ea2ccaf612986da90ce3bb252b66db107cc"}, "7dbc7fdb-5578-43ad-98c6-24ba1c334a31": {"doc_hash": "b79061f1b34721bf2500f62ad97eba66b37487b068e3a1046d71f0a5de200efb"}, "f9073010-c58e-4f72-aaf8-3fc9fbdd9be8": {"doc_hash": "57b7eb1ff5dfcfc401bfd42e593f68c1d7501afcedbe4b4eb64fe8fd3af76c06"}, "7c947301-b1d4-4f9f-b0da-9d0638dd78ca": {"doc_hash": "df892c46c2130b7f2c0e0cfb7646ff9a4a01a657bb5231283c1607ca2c706de3"}, "c1f50016-dded-490c-b586-327d60cd24fd": {"doc_hash": "53cddb08615cc58ffacab7b234252dcdf7351fb648312aa8c11fdb5600cc6e10"}, "00a2b1cd-e5ec-4a1f-ba02-4d0937b9856f": {"doc_hash": "a5e5093491906f3873da4a13ae3b522a06bef3159cbb3d3bd090e23cde0370a7"}, "4fa9d533-1a89-41bc-82b0-df65b93469b7": {"doc_hash": "6fd07e337191932859375d0359cc57ea90c877bdae6dd8b93c601dfd4955d400"}, "eb05c358-6d20-4f48-976e-fb9949fe9f1e": {"doc_hash": "df8e43565d4e5e73b7430f28749a784414e11a66f2ad99e4ca05774adb497acd"}, "25b4ba66-98af-45a2-a5d0-24ac58e101ae": {"doc_hash": "9ba4ea655bd40ab5c260265352562781cb0c3465303353a88ecd5257fca566dc"}, "af6edcfa-ca2e-4446-a1bc-1ee64199c805": {"doc_hash": "b99183ca6c07ac712ef70db5e5125be17a73310378b065598679fa57da01aa85"}, "e45c5900-a7b3-4807-afee-8d21a304a178": {"doc_hash": "792bcbb1ae5ab7da2743cbcb76711b7df94c45c4cf7f79440657a2b5f768c266"}, "58af4e95-83ec-465c-b917-532eaf22841a": {"doc_hash": "68bb1a424bc045af75cbaa8ae80bd79d937a9272ce70caced2fbf8a09ddcc289"}, "43dd8565-3892-47ea-a3ce-1fe64ef5e9fd": {"doc_hash": "9f8fd9374183e71e41c4805b71dad1f83abf2141cb5b2e87bccdbf89e5815c8f"}, "8cf45b88-1dfb-4dc5-83ce-3228c341e2b0": {"doc_hash": "5e0147a22191da227ea4172ad731e6c089bad4052e05ec4012b90f446b42c25e"}, "188e509c-5349-406e-981a-ddb9c0deaf80": {"doc_hash": "2df1e8bb05067673d386ac5317173be427cd878fd854ad7fa52a832a88a0cc2f"}, "9fb8712d-f8a5-4e20-b1d1-a0889a32e6be": {"doc_hash": "c4283780f5974fafc6e680450267a277926c648893ebd91db231d222cf33b838"}, "e30d9eea-520f-402a-8e91-da6d4cd8e348": {"doc_hash": "dbd9c547a6c9c8dfd6860db554ae0afbcca6741387709df926f8aa1bebc02013"}, "5e2c8dd6-9a3c-4920-b630-4ad43af77f2f": {"doc_hash": "308f557522e89f18a9a01ca964dc647d3718e82c8cabb83960c44c99b1e70bab"}, "73362b9f-f597-4ee5-862f-802e822f499f": {"doc_hash": "7ad8dba3c84b13283b532d8103bcdb7302dede5c84ca630d717c6381a6fc8b8f"}, "14e656fc-262c-4b05-be19-3d8ae7ebaf0c": {"doc_hash": "ca6c046565e4c793d504293d666107aff48161f73bb03b4eae336a73bb0082d9"}, "c967edc4-6a3a-43bc-bb8f-0f443dc3ad91": {"doc_hash": "af9ba61d39ece2255af2e7cf26de649c51a5f3dad9c8a2f0367ed2e8ee9804c2"}, "b78c2017-73bb-4dbb-944a-cc3545317c0f": {"doc_hash": "c2acd05bc32cf611c8aa04513ef69ea51e22a57cf7a5a6cb45300174b59f4db7"}, "e79a7249-4bcc-4c2f-a34b-411c57c0e680": {"doc_hash": "b322a4d17bf9d7895f70c0a3a7c8cda693f99157ccf2289573dfbb862fd1b21e"}, "b7bd7f6f-6002-4892-af39-65dd40f50eb0": {"doc_hash": "72e92c9c00bdf56d895c7ce7f745256e84bc02853d8e618fcc857202231f1fb0"}, "64d808ba-1df0-4e93-a70a-6046f91fc9b6": {"doc_hash": "2071dd75c2cbd3abb4e870f09d73a80622e135bc2a4d9a5fee4480fa9420911d"}, "1e29db17-a67a-45bb-b96f-5126cca22574": {"doc_hash": "0af3b477f4d800dc08d7b076cac152af4038d70c5aabc3c8c0ff6a052fbb1e43"}, "54bd53fb-b3c9-43b0-92d1-cbc7984ce1b4": {"doc_hash": "66c76c34389d10764af49199e2352481be47cc1b24bf9a2ba80de988662ab87b"}, "90448a71-83ab-49dd-b4e8-c7bb32a0bb19": {"doc_hash": "997f264aeccbc2ad43c0fc2f59d14d2c86caabf0a93a26e45dcff53dda80527a"}, "7583019a-8eba-4fbf-b74a-3f19ff212db8": {"doc_hash": "39c77ebd3e109fb43f31351e47f4356a45dad9b7b37b2d0bbcc811f192aaccfc"}, "6b9ed295-4a6f-405f-8676-4e3c226d1d32": {"doc_hash": "ab6d3eae40661024b9da90012c3f4bc223807319de78893fc601cb0ae49ff958"}, "cf7d5fdc-0c39-4329-bc87-1e908af0b536": {"doc_hash": "5cc8bc3cf650c5853f840e6867368282af72c39c5eb883da04097c156bb32624"}, "fa79d2c3-2afc-4ade-8b45-f59d50e199ad": {"doc_hash": "30a8cb0bcf7ca8b3e61a3a916b5ea9a27f9be0c609b84d4e8581e279985ab406"}, "412db2f0-0a38-4e23-99fd-1ebf965970b0": {"doc_hash": "536e65b19c9ff869a20f61a0cca7af022d8f60ee19c622365b7b293ddeaef726"}, "0b458b23-f178-4f3c-98b8-afde6ae5afff": {"doc_hash": "4899d9ab5a65526d4450a16937cde86ea8dbd898a48208d224a4957bed3f123d"}, "83bb0a77-8bbd-45d6-8d8b-442686baa93f": {"doc_hash": "fd3bbc18612872dc0dbc4bb344534584cb2851da1cbf0eca4ad5987cc86e6cc8"}, "aa89f926-9c71-4505-9d25-560d16ed12b8": {"doc_hash": "a1022103dd518635fe255eb1e88cd1ca90eae3872a44f8311c3fdd3066e6ea40"}, "da579029-b1e5-494e-94da-2b4308c358fd": {"doc_hash": "44f93e63777ba1fab160287054909db2e5c3ca3d5da20aa32484383c1dd8a73c"}, "68b9ca2b-3cdc-4e47-ab12-b55a24d96994": {"doc_hash": "b040d38f0163e2709dadc2657547a362c10863be58f987c358a34066d40cba4e"}, "faa98306-561d-4487-bb4c-3457f11054e8": {"doc_hash": "1ad73b5a42b2a236946d0bf4923a35c0ff415da7d32fbdab74d17f59aee8099a"}, "3a4c0e3a-68f2-416b-b32e-a35b32c28d1e": {"doc_hash": "0c802f136ac5884fd7d59a7e98c94497beb6ec3b2babdb204dd944ef2e767b50"}, "120c6e9f-a10c-4bb1-b4a2-5d432821ef3c": {"doc_hash": "981d79f425a2ea690fecaa60482c053d9f487eb36e05803785d4b2b976f0cb44"}, "d53bbfd3-1a0a-4b43-ba15-e59e94eb55f6": {"doc_hash": "8b94359c5a31d945d003ae9151ff82c8fb16a76ce2572a81f3632224430e64ec"}, "482ed358-fee5-48d6-b8ae-28a68d81bd88": {"doc_hash": "da3a77dbdb52a5cd05b53517d93e8d9bb13ab92f8505e3a32dbc578a94c85792"}, "1720d139-5059-4eef-bc86-48cf875a2f3d": {"doc_hash": "212e8b3fc1f6af9132bda7ad6e1ffb809a84109db72605fe7314cfd33374dfdf"}, "79e0157c-ec8d-45ed-b9ae-b05e97cf40d0": {"doc_hash": "01e09a88c5f6196ab230c3e48640a913ea87a69c2dc0306335266de0da54394a"}, "2cc0ac07-6850-4445-af05-252842503dba": {"doc_hash": "17680ec62fced9e6d82f8a6a08dd048494e26bf840ae49d50f80093adb9e5a5e"}, "bf7665bc-412f-41de-ae27-21b1f6e753eb": {"doc_hash": "82d10bf2f554a83a49a5f15d25da33a5a99d25985e35d9a210a0d34d2bd2bd36"}, "88809d3f-2ef8-45ae-b4c1-b65680c677a2": {"doc_hash": "43dde5dfeb6d3fc03c9209284e1851bd27508c15bb7813c1b2b6cb4ed6fcd8c8"}, "db2aad2b-71ab-4233-9288-efe47b532330": {"doc_hash": "c3dbe85069b721eccefaad8933f89d1650a9cf723f903882639f60cf31a7ea25"}, "322b07fd-e851-4ecc-a25a-158ef097b19d": {"doc_hash": "b459ba45c0af58912a45c251794a72511a5cfa5d78dc703efd2b81305c29feff"}, "ff0457e6-8a07-421f-a198-602ffd46e3ca": {"doc_hash": "1955985d197331821cbe1b00bb6ac43e0c325a441ef34c45c72918a4a365f7d4"}, "867646ba-cf8a-4481-91ea-e3d92801ed56": {"doc_hash": "6af5c1d3e814136c70422f089e9713941a2dda6e57baecf6c805dcfd37f64417"}, "8f9c8a35-dcf5-4e3e-8dde-bb4225b64a02": {"doc_hash": "7a6b9a27a5f31ccf606b3d525910ad20f0732415e820fe4ec9c585565547383d"}, "3587c476-0adc-4da5-ac3d-a134a161679a": {"doc_hash": "2bf0df6381c0712c21a1dc2655af17143df66d3c37a1d87d05df0d3ed78113f6"}, "9d0626c5-a740-49b2-9fa3-81bec6a92524": {"doc_hash": "c3f6bb709b06d84295cadec9df0524089c5549da261c6613fd874449251e420b"}, "87da16f3-4133-4ca6-8b5b-86bed7e5d269": {"doc_hash": "97db4beb5789bd8578d6a558a8032ebc49ac03ebde4fa24b668b17b339ecb495"}, "6a077399-770d-4ab8-b0fa-b7f1eee32c1b": {"doc_hash": "50ab3ef92dbd516c8d2d10161b48fa42a1b395f56be040e88d2ec0edd94ceeb2"}, "d04fe36c-7c1e-4c5e-bbac-4b09e88491ff": {"doc_hash": "03f1d2d84590535964188168a842235a10558a914f107dcea3864b4589fad5ea"}, "c2a834bf-6d74-40b4-816e-d8c54cf34a3e": {"doc_hash": "3c92caa935ee11d3f7719c68e824b8d6f3715fd09800125b6a12cbe68157bef4"}, "09db70e1-3b7d-4701-b128-4698645f97dc": {"doc_hash": "aed01d944d1aa709fdbd5535ebadead687ae6659762250092fc2dfa7e17251ab"}, "fd90ccbb-7c15-42c9-86be-319284631003": {"doc_hash": "6f0c319eab6601bb92bdcab0500886e892f310d29bde53708aaf57ce43fa6aac"}, "23f48073-953d-4899-93ef-0651343e3e8f": {"doc_hash": "127e8b16a1b0d7764e6eaad512bf70d4d1d039ddc405f8987ba5d91cb8c2601c"}, "d1dd4c53-d2a6-4606-8bd9-bc5be7f62886": {"doc_hash": "bad0040be157c453e6a7fce53f170692e21dbd716a3995bce02fb0505af46b89"}, "fa3dd2b3-eaed-4004-8670-47e0622c88d4": {"doc_hash": "35fe982843a79cffcd9c7d8e7266fadc625030819fb3abbb870da1a2b9586782"}, "9bf1ed3f-0350-4171-b261-32fc86adf950": {"doc_hash": "d58582f54c2c9dedee45ff11569170a22f78472657be9784629ffc822bf955ae"}, "9c54a1e3-b4a3-4db2-a522-983b4aee673a": {"doc_hash": "871a61829e792d20472fa48da7c8a04c6e33b5d64a8eae508692294c6e05bd8e"}, "1ef3dec6-3329-47a8-8a9f-d19f50b2eac4": {"doc_hash": "1b2d91907a43b4fe87bacc769e1391b3034b5acb888249b33645cfed1a6226f0"}, "d35f839f-bda0-44e3-b13a-16d7442529b8": {"doc_hash": "419ab9f98996c5c66de08e0fd2a9fdf8a92f8ac601dee9f0fae008ba59063a9a"}, "2a2a826c-0d5d-4894-aca3-36f6217171e2": {"doc_hash": "24743ca6494ffda85963d4fdaad4a8ec05e9cdebd190fcb2758de1258632b896"}, "6d374a79-2716-4c7f-a9eb-4cc498671e55": {"doc_hash": "1031d8e7d5cb2d32037dc3dd353a1020e521e88ed7d4976958f51187d954a0cc"}, "72a7ede3-dc86-4a0d-b5be-09f55f33385c": {"doc_hash": "9364212f23db948fdb0b798971380bd7ad3461fdd64153798799330b6fca7b11"}, "560c1cab-acdd-48b5-acb2-adb67a47578f": {"doc_hash": "79971ec0e75e78086fda6e139e6fcaa3b36c574b51f9fc14de1a0324dc9d8c07"}, "515f0944-1f53-4423-8c6d-a87eab0f7f95": {"doc_hash": "5137eaf114cbf75230d6b4c0c9c8a79582a638b06861c37c33b4f24231ffd4ae"}, "73adafef-25da-451f-b493-566bd11d5c40": {"doc_hash": "9bf311ccf3edf21bc16fd51a96e6efe9366c7d9e3f753eab36545bdacc929e6f"}, "a4d672b8-ec88-44f3-bc90-9d86c5d48109": {"doc_hash": "143c08b7289764a0cedb3c1f9ffd0a5c1dfc89b296a1006791594b943b967397"}, "13970d91-016a-481f-b307-33161b97b8eb": {"doc_hash": "fd3db9141e5cebfdc27621d57262a458f07b8f8ec63ab322c65e233d57b79a43"}, "aac50e13-4da0-4918-a806-b3ba375fdb04": {"doc_hash": "824bcbd3969413cbe4135bb6d0b99ceda14e2492dcba117b5ddbf34a351493de"}, "c7789da2-8273-4181-b40c-9154cd41895c": {"doc_hash": "ca4eb178301ee7975b6db67d12e2d87ad4f4101fb1811146419e69b9655fe3f1"}, "651c4294-609b-410a-8e65-22a4caad1033": {"doc_hash": "36eba286cab2c82df9ae2fcaec55b3691c65a19e18b692b30a5ebe07056a3316"}, "fd23dcdb-4230-496a-a80f-e38106f2907a": {"doc_hash": "e82d5c4dc033447fed77e6d1c8768769fa3ccd465b20afde35022cf6238b04cd"}, "e8f8372e-28d4-459c-a3c4-acc0d9e92a7d": {"doc_hash": "e3937c377555e4d19308166d85f9fd0ac660feeff78a0124eba72cb3e24f7692"}, "d8382130-67d3-4698-8a03-9f5e6787daec": {"doc_hash": "c669cc7db7dfc9e427b05bf8332507f52f87c2a6cb4a5bbecaf97b155a7f0398"}, "f1999008-d6b1-4f19-a3d0-998631528ad6": {"doc_hash": "efea18c252157631d3d260b49c6aa87b1e4324d539c415f0e14954470c924b48"}, "852ba98d-947e-4f3e-bd60-85b47344026d": {"doc_hash": "d7b90b81b178e288545eaaeb1ce4b21766b18727b00213ce2a7bb4d937de0422"}, "803f1263-84ad-4d19-9e7e-9ee38a82ab5c": {"doc_hash": "ea55b8a9a0d7bb1722f88cbf5e2722fda1cb30000b87d51d80d650fef3717cb6"}, "1af94268-07e7-437d-a480-295645bf151c": {"doc_hash": "7d57abba2f574601c3fb13b8ecee08745c9b9b930a1b70d94c6c570e249a5ccd"}, "3edb30d8-c393-4de2-b658-be1a77b4a650": {"doc_hash": "7ef5bbbbec048bcd4c6365e326e1d4e69dcd9e0df18743961fd02ffdf02ce54a"}, "9acc31be-fb6d-4ad3-9653-3b8567a06c7f": {"doc_hash": "b30fb5ec20a657e268f4af480ec9ea7f58db90cf407a7de71e471c94bb50bada"}, "acb8b2fe-19dd-4459-8e9f-e244c68af468": {"doc_hash": "faca7fe007b1d1f1e834d6d657cfc227d9c05645a7029ed475d6b2e4580f93d4"}, "aa2afdb0-3728-4031-a02a-204891e8c9b3": {"doc_hash": "cd19ef37efa9514caf323abf71e5ccfd0d722d9954947708b64779a4d392b9e0"}, "9a4d7b75-e30a-4b37-9bce-dcbbbae793de": {"doc_hash": "f22e28c2d83948ca1373f6913759a2e6c0fe6d2584a297430de97a20c7b407d3"}, "f568230d-3a92-4131-a13c-e1a0b50a617e": {"doc_hash": "5c5fb67e484cf72af868dd7a97561cf490360efc94f796ab9d9f74763c7980a6"}, "445396a4-bf24-41df-bb07-c1006109eca4": {"doc_hash": "64465723cd7ad9da1ec5a0f5bb123bba7afd83bfb75cf1e727fed42101141a94"}, "0410652a-7d46-411e-bd7e-f8b3011562dd": {"doc_hash": "cc1f129cce7e890c9c93a9c50ec9bb274852207e8a956ceede9f309963c9d4ec"}, "160275d7-e07b-4540-b4fb-84e02885897b": {"doc_hash": "1e656224f6cec3de6a0dbf0ed874da7ca9ed4d45c8cddff3f26ef30ee9a2afe8"}, "eccbc706-8b36-4643-ac26-35e67e98a2ee": {"doc_hash": "722e9e5b94938ee735382ebb560841d19120d7fbc3731f476e6a6ce1a9c241ae"}, "6076a4a4-5a25-4e15-a6e8-3eca1aa2cf6c": {"doc_hash": "1fd2b9554cb9085688337f87b93a780c579576cf50d746b7eccf4953c1178b35"}, "eb977b19-ef57-4f49-bb58-c50a9eb3bef1": {"doc_hash": "2c6cfbf78d9aa1fd5a16b7c52da4361caec57fdcb35037f09948ce7ef63ecb17"}, "8e5efcaa-2557-437b-98d7-f2856ebad259": {"doc_hash": "26e51e0f78ee51d7f3c67b172167c9565d07a434fcc76593edf314b3bfb183c0"}, "153ac4c6-6ee3-4845-a037-aa7f54d2c5f2": {"doc_hash": "73dfa90a882a4cd18c7b5ca46b3f98523a81c500d2cf193e0fad91ff151cd0c4"}, "5d2fade2-e276-42e7-a997-cc4ae8877b43": {"doc_hash": "4f2928333d70ae12057eb9a4b205d7c0b4962d01d25c827dd58661852e10bac4"}, "e77a9e55-53ef-4dba-9d06-1b7d6f0f255c": {"doc_hash": "2b781d246536eed9a57210d6343f82d5054b1c9b3e6fa0e9ee4ac5560a2628c7"}, "9c491f2d-3df4-4af7-951f-5228dfc4fa95": {"doc_hash": "08d278e9de7f8f7b0e3629db0a467e9b57c3f0fe9a9d5c739e244a6aaaa2cd90"}, "4750bf8b-3a68-4c1e-9779-bf8d701a9b22": {"doc_hash": "26e814b53b9e2b2558c6704b34400893bf17769e36e1d9a12bd10b24299d9de8"}, "1c6439f7-2cb9-47db-b37e-559a7b28adc2": {"doc_hash": "d4b9166b186da99f9df979b6fd10e683d90289f2135489a2f49571189e67529c"}, "4e61c6e8-2b9a-4906-b237-821659570f63": {"doc_hash": "1fbc359c5a525853be3ed450a3bf6a931ea61951bce6b5eadf24ab1e3b96c940"}, "d4fe3759-8fde-47ee-a62c-293b66b8da52": {"doc_hash": "ef6cd1bcf4b470043255e88175394d25fb2c992cfbd9dcc7e13a51dc9c645d93"}, "e3e5aa5a-c374-4387-99a4-a9cc19d555a5": {"doc_hash": "655deb092aed692995d023f0e87e9c1c08bc6c4d77c2de0c7d65444a176e7516"}, "1ce3c3fe-f692-44a2-9dde-97a2ae12cd75": {"doc_hash": "7c9be5f1fd8e02129f3fc7d92a8cb31d8b79bfb6929e4c93b7873ee5458d9f55"}, "c9fafd24-eee8-49a4-8eb2-ce712306c18f": {"doc_hash": "7ea60710a7dd1ad6667d6fd8b036f8d8c7e85343722b0df3d0939651336fb042"}, "3aa269f0-73fa-4dc0-8c3c-f3266ad8ba61": {"doc_hash": "4f003ae9c086a0fffdb1899829ad478635146abca9ae25608a21073f5ecc194f"}, "f20fd27b-836f-4a34-8f7f-45cb35b82c5d": {"doc_hash": "e0557ffea9a4cfd7edd8099e9d49af1ad69ef3eb2754c0edd5079764c59a44e8"}, "65cb6302-11ec-410a-a09d-d363e6681b30": {"doc_hash": "d347f2fb94bc56856d6e302b0ac78ffcd49db018d5b92b53095ebfe0676379a0"}, "60c59991-ce38-4238-8ec0-9c246db55037": {"doc_hash": "7b47204aabf8498e2546aaecf27e1fb0f58146d61792d843a62d5ab0170d7bec"}, "fe439298-cfe0-4463-97ab-99bc311cc931": {"doc_hash": "eec2254603325da65b0b4a162deea0426c365977d2249a254ea4dba9ed1a70aa"}, "d7821250-0b01-4a2b-b2bf-6dd63571a49c": {"doc_hash": "a0048d22c8e1fe126814417ab4c0861c4bb8021fe36aba55d1adf0540514db53"}, "72de8e2d-7d97-49fb-b8b7-f70a531dc765": {"doc_hash": "a8d25575b1bad736e3729dc67c99b2973465f6e8cfeda65f9ceb929119f1f3ae"}, "c61a6dc0-556d-4c76-8a8f-55b540a044a5": {"doc_hash": "e7aa39a244b9c6e91cd2a5661f901920f5e14dd0ff7476f4d908d6109a8c01d8"}, "a8b5e6cb-2084-4194-b3f9-2e0d28a2ace9": {"doc_hash": "972a6c1c79f65e078e7538979c3b1138cbf513ae0f85042d10ceef549c413d49"}, "b972f8f1-81bc-4f21-8e89-af214d24de53": {"doc_hash": "b880cb4f1e063e9015721943b0c928499cee367e2fb1cbdc5e0de277febeb3c9"}, "387d9dbc-ed88-4627-8c63-ef80b0c85ca6": {"doc_hash": "156da341d733cf99921a04d119c6cc4a4dfbcb71434604d8d9e0c32c1e823d98"}, "53e143b1-a573-4c0c-8bda-5a12b768494f": {"doc_hash": "91af6bbfd314275e8d18e7325b537885c78e8dc06f049da653fb79086f492687"}, "9ebb6fa7-18d2-45c8-9731-195261311d58": {"doc_hash": "4af8e0f8d1d0ec0b35e091b3d5ba68fec75b6346e3f0d5aeeb27a00c3a3af7d1"}, "f9a8ade6-6a44-4351-b38e-5d7af3a0d849": {"doc_hash": "e06139659923713b4972dd6fa6955564cc96969da83da99de3c69b4ea312b615"}, "c04d2823-7a3b-4c65-8ac6-b5c059b0d586": {"doc_hash": "df002405daab63f402f7aa0d47bf24d409234c93fd8af1105307dcb456149b00"}, "19e899f6-58ed-4912-b450-067a8dc6ec76": {"doc_hash": "a283cc9fb847bbe3d3f7d624957dc0d5e96d087b044d08ae0b715d42c9f3ffd3"}, "990864f0-e4a3-491a-80d8-ef1f888d2243": {"doc_hash": "77a58a5ecefdcaf47e843eecba1593eb3f826611fa1b1bbb02e9ff51885ffe60"}, "b7a5fa17-2272-4e4f-b9ff-42eb33351f00": {"doc_hash": "a5f09bf61a22a728adc9391603c6ad1237d3efb102cdf22ba0d6545723fb322c"}, "6bee99d0-8710-4e05-b350-1a7b57085706": {"doc_hash": "aefa3a87e92f7fc26238cb7d9c1f09e57a754b15e779450ae3c67ef88e32354f"}, "e9a340b0-98de-4ddb-912b-980b3e4c3e0b": {"doc_hash": "fa49c40408a158d7d3f1b10e38a0ca590976e6e24986fee694add3339b6102e1"}, "156107b4-353f-4f28-918a-3c38c94f1ef9": {"doc_hash": "f235ab0fcf6c8e85a282819a4f9196e3d13c556d2538cc5a793ba2622075e598"}, "c0c0822f-7956-4b29-83a5-dbbd48fa902f": {"doc_hash": "eccc2f89018d0a03a77b5a98df60fc51026d0b198b0cf46a4fb9be138a69d227"}, "d655ab0b-800d-40ff-9906-a46518fc0a1a": {"doc_hash": "cdbf87f2973bb0ed7665ca1f22cab3ce6afd6381a6c11a593d17b74140f30af0"}, "bd1a7fd4-54d6-49af-851d-bbfa6d40550a": {"doc_hash": "c048567bdae7d59aa2f18a36c930c67458015ff0da02dee56768be4baa531678"}, "94b9351a-4afa-4bd5-adf6-1cf345338c7f": {"doc_hash": "cbc3736e17bc43076f2b9a5eddb7e47916ec7ebabe2c7a4f3ece48a11aa66593"}, "178aaf7f-bf08-461a-a57f-c12c4f9ce958": {"doc_hash": "28036a72e5dea4671f8bacd8a786f41d448a50b50037bafb2bab80535b3774b0"}, "80a5ac38-0c86-4869-b25a-e8d6092cbb24": {"doc_hash": "526cf748db65f2ad5618fd429341c5543d21c766085d6cd2f858c1238aa8e671"}, "2e961a04-5386-45cd-81a9-071811a90251": {"doc_hash": "0ace241b53228d4b66359318c6cfc09cefb73d97014e4c456f20b7138f313786"}, "da4c5e64-8583-4745-b487-525f62c94d4a": {"doc_hash": "0e3aaf8192ad04bb7843ae9010c9bac25f87195086bee2bd8794692df0856574"}, "f17a5f6e-aba6-43dd-b16c-ce2f8f0ccda5": {"doc_hash": "27402312a928621c660976438ff31ae668d16c871f6b7c82d3c72ea8476fa9f0"}, "227d6bfd-12ef-4b74-976a-7e9a1172a4c0": {"doc_hash": "2fe205e4178cb6c320375352e91406b6c54b50cc1e527eae2762dbdf166ec8fc"}, "baa93e5e-7e1b-4cfe-b6ec-84a834f32975": {"doc_hash": "65ab312faea935c83eb6f0a4e22bcab4fe3933e591b66141bf2324551aa6f591"}, "a4c97274-2275-4e9b-bc89-c25afe47e7bf": {"doc_hash": "78a54c0a96ea72b9afb24e05fb00261b4f7414a1a7febe859feabb4b24ec1e45"}, "8cb2d50a-b403-4f28-a5df-fa8fb3e5b5fa": {"doc_hash": "cf8bcf1e076066e768c3377d43771449a8899f09cedcf81583e666bd94012ea9"}, "8d7c013c-ce75-407d-b7e4-61a1f819cad7": {"doc_hash": "fb2dca05c72a5c19b68ec22152ac8409132f62790021f4e42c3afd11ad40ffbd"}, "9360c30c-183b-45b2-ad4e-c6e285753388": {"doc_hash": "15feed0d1703338f26ef87c8759c4d29aeb01e22e5c9fcddff82b706eec504b9"}, "25de1824-7c41-471e-a874-be67ccfcd70e": {"doc_hash": "eb51a7477a2f2c661476ebd40431bd6504b806339eccf588efa91dbfb2b47d6a"}, "57201e05-589e-46d0-b4bb-a6c0a7ef6427": {"doc_hash": "a967b8adee1222adf46bbad38ffd6e718b6f26936538627c84277c7f3212060e"}, "9ae2302c-4b63-44dd-af6d-8ef751b710c8": {"doc_hash": "a5bcc7b7a312570148007458e296174179a1608df35d68c92e4819b31c40288c"}, "981f28d3-ec7c-414f-98d0-54474de618d1": {"doc_hash": "05f047bf335098913cfd1d4aa8557d3f2d2e90c88ffc1a78290df6bc36872b3f"}, "56daa91c-dac8-414b-8906-123be8275ce4": {"doc_hash": "917d2c0278b020bdff8b139f730d83ebd7a20ae1640e8159f1ec1784f3965d5c"}, "e35e66e4-8188-40cd-98a1-63f018ebdd8e": {"doc_hash": "88a04ba1641940186c42bfb3adc8820ea8d752594edf50dc7cb25b8b147082ec"}, "61471dc9-906e-49a9-a1fc-43540c3c2cfd": {"doc_hash": "a4b633117d8230744a674fe97262690b596a8b999b70e96255240b80686421c8"}, "d258ce96-c3af-45f2-bb04-7f477a1d8a2b": {"doc_hash": "15f19d0e65dd604918e3aa9d73d51704cbc8b647ac4fd8b75ba7c12e2f45aa75"}, "3ecdd703-75ad-4316-91cf-8c559261756d": {"doc_hash": "ec27ce95789775b977944c52ec9bd8dfeff9d813e4ac44f4a37199165d6a82a7"}, "f512b869-bf03-4263-9f54-b16539c58706": {"doc_hash": "c7fdb9f4fc9f99741b703c2edef6c5c59c4c258b5f81fb6303a1f6bb54eec2a7"}, "d52a3df3-cb27-4a26-bc9a-534ab5fb2ed1": {"doc_hash": "5c28af3bfb93feccb26abdbbf181915de0443a9b2e31bb34fc6b60f181aaa372"}, "16a0db4a-3df9-4260-a93f-dc4022691c53": {"doc_hash": "38bb733665d51161e43c13f11f1e56b4769a0a8da41b021c1e31014b23ae029d"}, "48a76c2b-1991-409d-9f66-3a43ea332a37": {"doc_hash": "423664a2fd36771698db83074d84136a50e204cf9f7f6922e214f669ccd8db0b"}, "d7db6e3a-3fee-4d15-87c7-da8be83afd01": {"doc_hash": "fc173ee2035fb9aaaf6555f3ab160fd29ea52ba9454560dfaf69ac310024a892"}, "11dccb50-ea5b-447f-b4ad-c8fbd5501119": {"doc_hash": "a63454531a9c3cc930f5fcbfc987c10d040fec86c6910cf057872e8a0dd3a254"}, "54eecb9d-e31c-4c3a-899d-3e1b036f0f60": {"doc_hash": "c0ece57ab6c13d5de97f5167e838e9e33b52fb1b618a0ea73d0c289929694c05"}, "5e46ff26-85ec-4467-a6a3-0e524961d68b": {"doc_hash": "82d1be20bf52f1381f940d53b81a355137936de72154866860bb4f0bf958914c", "ref_doc_id": "d8047731-f86b-47f5-9208-b627a0d9a405"}, "f75aa2eb-3138-40ae-b6db-4bf7d2d01d41": {"doc_hash": "7d7d93c7a67ed90549ce265655b6922107a0729dc5c95f4eee9fda59629ca297", "ref_doc_id": "bf9bfd21-e034-4a71-9fd3-af6a09b58be6"}, "24033059-a7a5-42fe-8f17-2b4c3643fd32": {"doc_hash": "359893b2ea11c3f5c170930359bf0e1e7e42079d1e709fb972ce347f12ee4b47", "ref_doc_id": "53750600-526d-4e85-a907-f0c5560deac2"}, "f20664ed-1090-4e57-a60d-58a42562dc62": {"doc_hash": "b1b592c1e8ee34c73f213d0fe681b76d06d91f70043d671dc7e6e1b2fb438c28", "ref_doc_id": "794e12a2-dc7f-490f-9b9c-e6c0c5b713d8"}, "33c3ec2a-3d9e-4b5f-99b2-0568da0d823a": {"doc_hash": "adf317892014c64c29ed1eda8a4c11c72ac3a6941c68ef235291cd46ef495258", "ref_doc_id": "dcd5cab4-4994-4e98-a677-a4004d4510dd"}, "4a09093c-28cf-49ed-a09c-078946014068": {"doc_hash": "00f8d8941887e1f611d580dfb7516f9e1429b5a23031aad88da2c879672f2857", "ref_doc_id": "12bb99a6-6534-4f08-8a28-adcc6179abd8"}, "e32c72ec-7ce8-479c-844b-1b127a29ba4e": {"doc_hash": "d338eb8e92551a80a84cc5ae050c15ed8850928e573fb4412a544efb36af2048", "ref_doc_id": "095af734-1710-45f9-b28f-5380e3e8c94b"}, "3b27b47c-9b6a-4583-9d07-75cc369c941f": {"doc_hash": "3b5ccf3c1efc3100dce9aeb3d3f96090dd40631d0f407ed8115d4a279ac0070d", "ref_doc_id": "41d73e69-bf24-436f-8a6a-f0d38a33ed24"}, "fc61189e-6a1c-4553-87cc-ddc0bb2c6be3": {"doc_hash": "8ba29d98690494e8c4b5d69a0cd26062062e19aeb1de39cabc2778eb015d46b6", "ref_doc_id": "000bae20-f570-4e2a-82eb-54c7762d1d14"}, "13376bb6-36e9-439c-9400-f86a73385eac": {"doc_hash": "72a0466c33e722e69ed4707fbe2d2ec3904d3953c1020c19f3063e9b215338e5", "ref_doc_id": "5c974810-a925-4419-b2dc-2920e9794f5a"}, "b662a6f4-883e-45fe-9963-dafab7e45245": {"doc_hash": "338d3f10528f06b34255e65271a275307bea7f09d77ff75ceec13983cb7d9459", "ref_doc_id": "d76d575c-27ca-400b-aa40-53acd3d5fa2c"}, "bd92f662-d22f-4c45-937a-48a7eda9ab3c": {"doc_hash": "925dd393b7458574e39cc48b1c50dd5c54c6bebd8d3bcc4546987311d8b13716", "ref_doc_id": "ccc2ca28-a567-4613-bada-74579d4a3f41"}, "899a7c44-ef3a-4270-9a94-df4c7a9b54e0": {"doc_hash": "e6e5df5f464c898718fb23fae5b1920bba6e980c26a8e76aa65b6da0d8b060e4", "ref_doc_id": "a3c8ca37-a928-419f-9ec9-af78623d7387"}, "0ed21185-a1b0-4acc-b720-9dec8704115e": {"doc_hash": "f09ee596e588768d52e34574862792c4fb88c54c5a83dd2268f6413b96780fb4", "ref_doc_id": "91c71f01-86f7-463a-9535-51387506932a"}, "669bc79e-4f2f-4662-9c87-2027f0528f34": {"doc_hash": "ac01893648a4bc7d88195dfe6b6acd5b1fa25f4124d33319ae347f3b44b96a59", "ref_doc_id": "28094a3c-1118-4e0e-9f18-db3d88d6a805"}, "c03cb067-fb0f-465f-8654-e590f81083c5": {"doc_hash": "a99f676a59d43faea3a03f7f23b379a648edd303b7605fffd1db8a46728d5f52", "ref_doc_id": "13b91b49-bb8d-499f-aa64-8a4fe3899b20"}, "123d63f5-ae64-414b-96ae-c7a585830a4c": {"doc_hash": "8ee04fa7bbdf119295b825bb5c18a894a98aec9abf58976a42d8604e4e121c75", "ref_doc_id": "184a637a-3adb-4e11-9f39-d4e7b8e927cd"}, "10eb6275-7cbb-45d2-996f-f575626a9e86": {"doc_hash": "8c19efce3c71e513a24c3029ae14987547cf89b3633afd9209af83c31f7a815f", "ref_doc_id": "f27466b4-bc1b-45ce-95d0-10da581e0082"}, "4fc7bb18-d46a-4f34-94e6-1267be9924f6": {"doc_hash": "82583219d29209125fc4f2ccf3a6576fc2214c2e03ca2e23d9a73888132ea09c", "ref_doc_id": "c4b0ddfa-a7c2-47c1-ab9f-799a5cef3b70"}, "4c1e8140-0fce-43e4-b745-4876ac87ef31": {"doc_hash": "3b780c816036ac9053d3a55d04a4d1329346a29a7eedcd6f5de2721397b615d3", "ref_doc_id": "0d237f8a-d803-4e5c-a620-dd5c08937c88"}, "4b6861e3-a7c7-480e-948a-382e420bd6c2": {"doc_hash": "f59854ea5341b483010775620280fe2eec4d8aa7376781e822899328e58271ce", "ref_doc_id": "22e29a89-0ea5-435c-9231-ae85f0aa253f"}, "4b3d399e-3ef8-4a13-abf3-2dad1415d57c": {"doc_hash": "473e2c429d18d596c7e58481b26efcb4834aebece97a0af321501c734d49665d", "ref_doc_id": "369dfcf4-8fee-48ce-bab0-b705bacd1113"}, "939e9f09-e46c-4bf6-aff2-b0b657232eca": {"doc_hash": "75c0c0dd830084d78716be9297ebfdfdba1ea48972137b206af108342cfe0344", "ref_doc_id": "6b4a884d-dff3-4500-8381-4bf04c2ebcf9"}, "5ee25271-4507-4be4-97f0-fbe08eb57183": {"doc_hash": "63f1ae6e44e80380b13b5d8fe3d33eab56dea101dae4131c6d3b312a1bf0ea61", "ref_doc_id": "e878a92b-77bd-4f46-bd89-27e41eaf3d37"}, "a6846dc8-31c2-4aa6-96d0-e441d5830749": {"doc_hash": "1b31088e9a680037ca07891f940c17b96ec5b493fbe7266b8d654ac16d066041", "ref_doc_id": "ddc4ec1e-4c5e-4303-8ce5-d01893570a3a"}, "a863931e-78a9-4d07-aa8c-23a063c59631": {"doc_hash": "e2bb79a930a57b31d00088778fdc478a22d674f5cf3d3e3d9e78d68b13cc518d", "ref_doc_id": "84c1476f-04c0-4513-ac00-8d57c2608ec4"}, "fc464a98-8539-444e-a423-069f0226e54a": {"doc_hash": "5c13051ad503c379115d7b92fcf4775e56585d9fe5b76ce06f27976eacbeb67c", "ref_doc_id": "d4d848c0-2093-4308-aa3e-e837896edf23"}, "f4c8bb9e-30e0-40b2-8a7b-6e7747b77435": {"doc_hash": "13a6210e57c00ebfc9e8db93bff90012a866c86e358048051611207b4aa63f01", "ref_doc_id": "1d1e51dd-15f4-4ead-a918-a966a805368f"}, "62730dcb-d8be-4c10-8041-db51c4aee266": {"doc_hash": "55f509757441ddc0e4d3a6600e428a44cdd053344092f00189b3492545c47b49", "ref_doc_id": "7a543f68-cc1d-4963-ba9f-b1690bfe8d93"}, "05c682fb-ae90-4680-ac45-f5cc0e71a900": {"doc_hash": "9b2b8423c3f3d2c3e7e358ea33b0e661da83cb90a818a972f3170691ee864a53", "ref_doc_id": "651ec303-63ef-4d10-a912-8b4f3f215f6a"}, "6619335f-d4f2-4bcf-807d-0fb231faffa5": {"doc_hash": "455619a151da20160593940f140c918ef1026014e345b6eb46d21e32d854d5ea", "ref_doc_id": "531ba80c-edf1-41af-9db0-c50e5d1cb618"}, "a5a31ceb-ec2e-4978-ae8c-60bb13b108cd": {"doc_hash": "d5efca1d43e0311a45bbcb94187a0e7d5b0ed2194c22f730adce291937d1c53b", "ref_doc_id": "64000ff4-a3c7-48e5-912b-18ac77dcbfc4"}, "2d463acf-35ad-499e-bf22-9d29437813f2": {"doc_hash": "b8b6d1f325366e91e9c289d6e43a7fe2a8816d38cd2878a357e3685fd09b1538", "ref_doc_id": "0731307d-2717-4ad4-887a-d4eb1a598ddb"}, "dfe56a87-3d01-4fa3-8d63-0ed914975c5c": {"doc_hash": "4ae680798bf67a0091569d374859feceb34b6ba6995de4a38589389a951edbe4", "ref_doc_id": "4ddffa52-f628-44c6-9fec-9df76e7c6dec"}, "056caece-2f41-415a-847a-6b4619847017": {"doc_hash": "64d59e9dcb587b0b7d8d2c997df264653fa201c3e686466383118a1e99b687e0", "ref_doc_id": "ed16e6c6-3853-48b7-ab78-1d5917c4fb53"}, "08ba64e9-cf35-4f8a-b12e-85b524aa6696": {"doc_hash": "8d68015828496376c4618c56a64fa9712333e84339c3f9fe61f6c482d599fb40", "ref_doc_id": "6ee774ed-f554-44fd-9e7c-cbec957821bc"}, "1049d8ed-6225-48cd-858c-2cbd9c21efd7": {"doc_hash": "def856cd44e70a8d0884ea59a13feae32b7280c0d02d77b97bba962b3200190b", "ref_doc_id": "493874c9-fc7a-46a5-952d-4cc40d7b5f44"}, "5de92e4e-cee4-4c1f-9161-6f76ccdeb20e": {"doc_hash": "de2da7e97ce35558092162a0de41f2b3844fb79967b259c5a881557d78652294", "ref_doc_id": "282cd0a3-6fea-423a-aff0-07ce06bfc6c8"}, "197bd7b9-004e-416d-ae04-af04ac5e8a01": {"doc_hash": "4ce13e7978a41cc2372c71c1ffe285d95756b1d98d3f38084200cf3f4a9b104e", "ref_doc_id": "743ef640-2ca5-4ae4-b3f1-8389eb330a94"}, "00c30f45-4cb6-453f-8459-ace6ffc79dbc": {"doc_hash": "7ec52780ebc82bcda8d689f3b38fd014f32297e05ee544f9c264f2072bf437a1", "ref_doc_id": "144cf655-c064-4541-ba7a-f8fd1a26cd71"}, "186b9a0c-f180-481d-9c09-d25ba9dbe618": {"doc_hash": "968973fa723c0590ccbbed758935059f7b13b8c9a2713182739fe721c3a27959", "ref_doc_id": "83a88c77-8a6d-43d4-8ce1-bb6fa9f573ea"}, "7d950b9a-9753-405e-a32d-a0893977d55e": {"doc_hash": "73b4f3cd2f9898218d03ae796d5e9ce31e94d63fee2b2a5720f79960a3f2c166", "ref_doc_id": "4b5305d9-064c-4cf7-bc3f-a73299ea7ce9"}, "5f198dc6-20f7-49ba-acc6-0fa7b11c3604": {"doc_hash": "0b41ce6bdada338443e1c0d5552a8ba7e8226eb37dff9e7b5824937acb03a202", "ref_doc_id": "22a429bb-11a1-451a-85e1-6692cdb307f8"}, "85823400-3c0e-44b4-b2db-a4d392954840": {"doc_hash": "bec1f91806286dc6086af393a64f44f83c63e064a9dab91214c758cf4977cde6", "ref_doc_id": "9decd28f-b1e6-4d2c-ac0e-5a5ef3b79ad9"}, "71244b1f-26e3-48fa-8a73-98e33d8c93df": {"doc_hash": "ef69ee7bc35cddb31d1509549ec0cb1ebfad9866c878d2285d5506b6e1c85a69", "ref_doc_id": "2afae01f-4b93-4f6a-9ac4-5aa43801a725"}, "56a51535-bad6-4882-907b-22181ec3c54c": {"doc_hash": "8eb13942f0f968f2438c373854d621250a8d0c80e989c5a3cd65f497180863fe", "ref_doc_id": "35474a31-1650-473f-a6a2-7d226b67a4db"}, "c53e19c7-96ee-4980-8f73-a9c450614c75": {"doc_hash": "e46927411e02c0cd653d148fbbb3d6bd960ba6ec17eeced68ceb992c5bef8df9", "ref_doc_id": "cff52d8b-0a61-4d44-97b4-984ce9c21efa"}, "afbc7918-d391-44c2-bb5b-6ac0093aeb11": {"doc_hash": "7e6023ac6ebad0613fbf760246057028461352e5533ef19b967b72f3100ea93b", "ref_doc_id": "97e39be8-dab5-4c1d-b529-e582e146bdd1"}, "b4dbda92-63a4-4b5b-88c0-87f66074dc8a": {"doc_hash": "71d8a32165e86c5e52bbad6675a9558e60e29cea6e219cf2638233854d0a6481", "ref_doc_id": "50ff17f2-18c0-4b6c-ad3c-b5051c6d0348"}, "329efccc-2202-4f99-9ada-3b82280913d7": {"doc_hash": "a68a7d0842dfa20c58533ecd04e391037711d115510e138b80d9c561281dadf4", "ref_doc_id": "a7ea82c2-fb81-481c-a092-c8425d27edc4"}, "9b5fa268-c48d-4898-8f41-228a09085fd1": {"doc_hash": "94df491d4f9a858af23e22e0100f1f7292c0a5e4703276e07a531ea6bba24ba2", "ref_doc_id": "0b4184a0-8836-4710-8249-a1954c07279c"}, "203920f2-f7cf-4859-a903-61cf0367d0ca": {"doc_hash": "9dccc70f572fc388138fa55c937bbd984dcf884aaa45367cb3eb9551e78ee65d", "ref_doc_id": "0f2074fa-3df6-4cf8-a50f-bf32ab8ff17c"}, "dec05349-1670-4914-b209-87400343389a": {"doc_hash": "3e1cb35a02f4cbfa065fe1370174861216225c302c12d88f8d4200f265cd9d69", "ref_doc_id": "f51e46b4-d1c6-49f8-bf21-278040d32ff7"}, "e6a8c80a-957b-4ca4-b5cc-227563d63500": {"doc_hash": "57fe0a3947198bc2d124afe986616a22460ca69c5a4b58207ad31e8fddf36a19", "ref_doc_id": "d9ba5693-c090-498c-a2ff-3dccdd5e502c"}, "cfe13f5d-c6de-4f10-9cdc-0ca968cc3dbf": {"doc_hash": "f3ef8ab8b63fb765908d4236391d268b2b53ad0afcd8b1313b31fe58344bd93c", "ref_doc_id": "3b1ee236-61e5-46ef-af8f-632cd957e2ff"}, "c0e25f00-6d03-43c1-a1c6-96a02073b1b0": {"doc_hash": "f9c6b4b9a96b8bcb3c0afe586ec9c442899bde1ffffbcf85e6c70dc53363e331", "ref_doc_id": "598c72cb-9121-430a-abcb-869d79fff9fd"}, "ef77447e-ff08-4aad-802b-589c650fe907": {"doc_hash": "9083abb0b846d42772ac865168d207e15edf073b420358657f64bbc240289d91", "ref_doc_id": "d689d4bd-73e1-42bc-a78e-d3db93795f30"}, "9e67f71f-79db-4666-8c2f-ca65f75f71af": {"doc_hash": "297240c6572b4491056ff92f762c0e169baf8b22c7eedf7c293308eabfd9fe0e", "ref_doc_id": "b22ec1c1-c5d5-40d3-83ba-53760794e3da"}, "c93fa4d5-a8e6-41ff-9a26-cc767995488f": {"doc_hash": "b80579a746b49cd47d923ede3e971f0e2b2894923a8bab414d0dded6b2af99c6", "ref_doc_id": "e8c1ab1e-850a-49e4-9e1e-597421df5649"}, "aa397134-a271-4afd-8bd7-a511cd337927": {"doc_hash": "ffc953c960b593f55d14484626c35f9a9073e8b805acab7717e6183a716cc9d4", "ref_doc_id": "f10e9dc3-2a5a-468a-aaa1-b1d512035d41"}, "7a855481-04db-4793-ab18-ed7814a19e75": {"doc_hash": "f86ff1e003de361d960b4e089e14e47fc5dbc8e1ade6a6be10f487b4581f8ebf", "ref_doc_id": "4dcd26de-aad6-4043-8fb1-05429ef756b5"}, "506f08b2-4390-49c9-869a-13a1a8235bf2": {"doc_hash": "0e97ffe208b0ae200775043d1f134aaa403058dc7ae83cae10147c579db175c9", "ref_doc_id": "5342f389-52cc-4e81-a12e-9f1a299143a3"}, "9bc95951-d9f9-4190-90d1-c436f4b69747": {"doc_hash": "3c24ae62751c98fcdbbe7682b752059aea650dc9cb5a00f37ce20a5b26d9980a", "ref_doc_id": "9f1ade06-2dd1-48d0-b7a3-bc36a540f6b8"}, "0b2ea6b3-f1c9-4363-bcb3-c4ef06e7c97f": {"doc_hash": "03c67654c74f7b493529b9ed5f6801a2e03901f250c2c5b58b2061e2bcea8514", "ref_doc_id": "3d56ea21-939a-4134-95be-abb3686a9fa2"}, "6964ace7-3441-4e1b-89a4-ba86ad81954e": {"doc_hash": "f2a27d5e0a21cc10e4eb069cade6b5fa872be4f59262cc28db6bc15a09478136", "ref_doc_id": "3fb14a6b-6388-4877-b0bf-db13a8abf066"}, "404b88e0-0922-4a42-b267-bcf7f53fd1c4": {"doc_hash": "78544b2e469ab8788c09b370742b71e7d2fbc9b16595dffb74eea68a4ba4b833", "ref_doc_id": "bddd971a-2008-42ac-9747-8bc363c7aceb"}, "c9f23c79-02c1-4ad3-b39c-9c0477632566": {"doc_hash": "bf569de94c60a3d2a7c4300cb4587a8090ac80f1b3c943073c15b651c0a07593", "ref_doc_id": "06c61145-4bdd-4d04-8959-67ffc4fb5178"}, "9685144f-264c-4eae-8f2e-74ec1dc5533a": {"doc_hash": "7c4cc4af72eb3f06823aec06514050f52c26a2b9d1d668b0fc2f20c8aa87ff62", "ref_doc_id": "ecc9298c-fa21-413c-aca9-b403893a6a67"}, "fdb5ea14-5a73-4454-ae38-41dc3b924c8b": {"doc_hash": "5d89eaa72bfa4ced62e66a52cc1f20c5056a04edb9d1e0ef7ef0bef0607af1e8", "ref_doc_id": "f9aca6cf-b9c3-4718-a1ba-6f8422582f27"}, "c0c5fde0-62e5-436f-b6ae-4f94114210a4": {"doc_hash": "03f209e04017ffe2930b4b0e4c1cc5a0f391c8c9328b3c751f9186c50a64ed97", "ref_doc_id": "02197a79-7292-489c-a3e3-b093f58eb254"}, "9e56240f-eef4-4a53-977a-daf93f9d851c": {"doc_hash": "7cf11f80e711f29d999fc8672991c8f03a8b82b435a911a468e6ac9bcba13a34", "ref_doc_id": "f01006f3-e81c-448b-aaf2-ace749dcbc12"}, "25ad3d92-788b-4979-a823-e4ee9319131c": {"doc_hash": "d16be9f7460b442a63a180134a03238404d0da6d081b9681d49ccafcf26dbac2", "ref_doc_id": "b758595c-9fe9-4453-9f25-07e0f59cc78e"}, "0bc11349-1046-48de-a834-ed5a3d646e40": {"doc_hash": "2981dd1c7b975d70a83033653607a8763bcdc603105c01fb5c44b02115c2aa2d", "ref_doc_id": "a8c4e4cc-3960-41b9-a068-cd651a991e71"}, "36ac9d76-1c25-4d3a-aa74-63f1b6bccbce": {"doc_hash": "e037ca1cfdaaeb0129695748de3e5d09decee0211d35400b10ee7fc3cae695ac", "ref_doc_id": "a0bced67-df54-4844-8c99-32c94a26e079"}, "33e1c742-b479-4dd8-9490-d20d8eda4a82": {"doc_hash": "94fefdb4c1270f52495d8231fc446dfd40b212215b876980f11ef78f36e581e6", "ref_doc_id": "5591dfe6-336f-4aaf-9fbe-8803a4fdcc58"}, "3f0f5697-016f-49fa-966c-87cd7a8c4c8f": {"doc_hash": "4024feeb64832b2831c4c3d2df461641850331585fc6ae27eef05ebf2b729ad3", "ref_doc_id": "a31dff71-b5d6-4337-8b47-a4d535b4c5c5"}, "56a825aa-8929-49ae-868d-e3c83254d5c3": {"doc_hash": "7a3054c1e2574e7625ee83d67b875aea4c927f35b153ba747460496ee4a498aa", "ref_doc_id": "032c09ea-7615-4c94-9857-1a0f998a0d2f"}, "5f0e4379-7ed0-4d0c-b569-43d18a23e0a3": {"doc_hash": "b0de1c54deebfd3534023ce76ea1c01163808964988b2b978de45320c0766293", "ref_doc_id": "265dc7fd-e5e9-42a2-8d38-8ab7264c27db"}, "1b007f94-19cf-45ef-a92e-0ece64151a9a": {"doc_hash": "7b3906ff204e79f42bd08f194764cbd71112763e9745e43789e7b19b5c7b43a1", "ref_doc_id": "db6741ce-55ee-420d-bb37-8414013e6fc3"}, "6f67156c-43ad-4fb9-8824-b011de53a1b8": {"doc_hash": "963b1776677f024636bb5e257b70bc564e28356f92ad03ff082c1000732d7daf", "ref_doc_id": "907c1d0c-b994-4985-a2ec-fa6898e493e6"}, "1fc26ba3-d906-41d5-bfd8-dbe93f14b7d4": {"doc_hash": "252d69610bab69e1e40b28404064556c482b9192c8a0c4065b9a6a68493db5ec", "ref_doc_id": "3e8e16ce-9737-4e0d-ae5c-460e14a4b254"}, "4b4386a2-945e-4096-85ce-21d6cc3a574b": {"doc_hash": "a79917acc9da5e0c63cb9513899a9e849a7c1e4e393bd94add8a8b6b0dab4c16", "ref_doc_id": "cc0ace0f-4425-41aa-ad09-94b82b6b300c"}, "23c78c0b-cc84-4a69-ba09-e15c701078f8": {"doc_hash": "0cbbddfb04cd6ca5e3fa0f562002fc3357fa5eff1ce76929559d6e07c01d3fae", "ref_doc_id": "04f208d3-930b-4487-904f-35de70308c10"}, "d53a1acd-f317-425c-ad58-cfd97da8bbfe": {"doc_hash": "a13ed662f88ae06a86c7ae87514decd5be8e173ae090d4d2b6611d2bc82fb90e", "ref_doc_id": "d8fcb4c7-820c-4627-8a61-c90a3d2474c9"}, "482c9d24-58d7-4a79-99e6-fde5309da079": {"doc_hash": "0e2e9fa47b1c506824d544372c9c976025f21ca571fc86d13e47a8feb306934d", "ref_doc_id": "aeaba038-126b-481d-9977-6e49ca2805ab"}, "db7e80c8-4dcf-43e2-814a-4cd21a1db3bb": {"doc_hash": "6e2f6a250f54c8b6aa0125021a0c9732ab14ec802b3157e455ce96ba6c917287", "ref_doc_id": "3277a520-1d7c-49c3-bc2f-f04e78b01107"}, "90c0e58c-af8c-4c6b-9785-49f713fce8fd": {"doc_hash": "3a5afc3fb980b952271edb75d9ea9086c2fa002f38f2a41100f60047e0f9e054", "ref_doc_id": "a72a4de0-7137-4a59-beb1-74ae324df776"}, "e8bc582d-1990-4c2b-bd28-b6dd09e6f6c4": {"doc_hash": "744aff158314b3a8b72457b63544310cce6b85d0ad736480f22ce1dad155f056", "ref_doc_id": "f1ed563c-14c7-435b-aa8d-d9e2eaed4e62"}, "cea2e09b-76f2-4447-8218-23dcf2ed48f7": {"doc_hash": "f3f8fe7b10466c996732e2309204535fdfd09e613d68634cf75ca282d980fb1e", "ref_doc_id": "f1259d0c-71a6-434f-8da3-1904ce7961a6"}, "8b94d391-66e1-4448-8da9-2158ee8da54b": {"doc_hash": "745430f9e7136fca565c31dc4a313f0769468f9905cf818943abca5013bb8a66", "ref_doc_id": "bf6962ae-3762-4ff0-b100-7155941d0796"}, "cc84e1a2-e4e0-4b29-ac40-3b557def75f3": {"doc_hash": "b88f118e679578535a6745ea7692f07e901477758a7d5211c6415c32937be1c1", "ref_doc_id": "5b604ca9-42f9-4a4a-9d21-fb4b8440e738"}, "7d99d9c8-1639-44d5-8dcd-59587a13694e": {"doc_hash": "c284040afb624cbcaa306fc7cc0adb08fee0c50c6bae89b1aaf9b154ff118437", "ref_doc_id": "730842b9-95f3-4ef6-9aea-37f0bb205b1b"}, "76752ad2-6966-4150-a02b-b3bdff4bcec3": {"doc_hash": "f9dc85bd6ee993e09eeedbc2d487ff1329601915caa2dc4803a823c06adae4cc", "ref_doc_id": "12c875a7-0ba4-4910-b9a0-837d53d3bfb4"}, "e256b78a-4ac8-4d50-89ff-6638b6bddf53": {"doc_hash": "f1bd580de5e9c08909a73c5ebc966089719339782fcb0bbd81500445eeba3229", "ref_doc_id": "f3a9a72e-b45a-478a-9d4e-c38342d2494b"}, "f6f7a3b7-c754-44d4-80c2-10a6f6e5fd75": {"doc_hash": "3919d62650d2c142d4ef7522ef29a1000d08deff8cec343c694bebdf2a72b1a1", "ref_doc_id": "34c9675f-17b6-4882-a9b4-04e1d51c2265"}, "9b8a1845-6c9f-41f1-8e7a-e43583454239": {"doc_hash": "96d72d7bb677c80367fb31a19dc0ab79f2e4b14b3a9b724b319fdbe63c1e8a69", "ref_doc_id": "d6330fe0-a870-4567-abe7-ae77c30f0378"}, "e717cb25-d67f-4ffe-a5b7-616b2e4d642f": {"doc_hash": "873f84f292623e614ba857451498f585b7fd8927638cd0323528d6301e48151f", "ref_doc_id": "df6399d0-534d-4ba8-899d-20d0cc62bf77"}, "5f4688a7-7d86-44a7-b197-cb98a15f26ad": {"doc_hash": "356c34f6c8014e9d12948a30f54560b48ec19460f32d55d6fa5da55eb5b3092f", "ref_doc_id": "f5b2a9af-88bd-48b7-95e9-333bd1f1eb3b"}, "83dbbcd2-17bc-455e-93f5-3b6430631af6": {"doc_hash": "3afd49006aa930f6404527e5b3b10f1c7461f405b0c13444a4a390c39c648267", "ref_doc_id": "f98a2dfb-d725-424c-af17-2b263c963977"}, "ec0cba00-2cae-4c3c-85dd-6f105a9c1a38": {"doc_hash": "9944edd298b5f5e31cc1842cf3f5758ac4cca7b97d1279d6be29a038aeb42338", "ref_doc_id": "d6900a33-1c59-4ea3-bc89-37250cab5e54"}, "0df9465d-1fe9-4c11-8f70-0674b2aae9a8": {"doc_hash": "b127a0020beebaa0256866380d20b3da74bf67d3ea36e4924c16ff7b5e91a0c6", "ref_doc_id": "67a5f6f5-c69b-430d-8519-8688e51feadb"}, "36b9c8ef-5366-4079-9671-7aa7e644c6c5": {"doc_hash": "ed9de8ede003f06c06564cd6da708c6f57019a669145a85b23f9cff2671932b8", "ref_doc_id": "9db7cb74-8532-4825-bc1c-e46dc2cc5282"}, "12e606a0-13c0-4de7-91dc-1e4d4ae32e90": {"doc_hash": "8863ed520655a7ae9c68e087fa780eee242ab1d13ec283fd7dd4ad17d6803b6b", "ref_doc_id": "1b8c466c-57cd-4fc9-821b-088f59bcf067"}, "596b2e91-3ca4-4c9e-ba78-fe2bc7813b90": {"doc_hash": "c07a073f485211c6cfa48f33eeb283c4ca3b9fc60d53957b743de6bc7760bb89", "ref_doc_id": "7a115120-4afe-41cf-8de6-7f7b2b73df43"}, "acd4af57-f24e-448e-a07e-0b110d6390a9": {"doc_hash": "59ed451483ba5ca55720694ba3bdae860af0ff44ad5920a565a6f809445eec93", "ref_doc_id": "440a2d4c-60d8-471f-8199-ad6d5d5a7f35"}, "7167942a-073e-473c-80dd-3eff469a2da5": {"doc_hash": "4e831c6df625446fe726419c84e388d979c5a4c4d88107080737d021c7ff82cd", "ref_doc_id": "1817412a-eff8-4669-9db7-3a87f3eb975d"}, "a5b00580-cdcd-42f2-ab29-1f50f8029d67": {"doc_hash": "aa4f611a583258baa849046925a19fdffd7aba3f5b56ffa083bd2e06ed5fb33a", "ref_doc_id": "e8f8fa17-04e5-42f6-a76e-4da0c97ce9e0"}, "0a280776-0a65-4e70-9d85-7b261843da8f": {"doc_hash": "3fde8c5b30dc96221db39aef860ee1fdfc8f37e54e78010a02a982edaa9a6cd0", "ref_doc_id": "5061526f-eb15-4894-be93-fdc86eb9ef1f"}, "3bb6ad28-dc38-404e-b34f-fd29e85b3dab": {"doc_hash": "607a1d313db8059c6a871ff24cf63ea2ccaf612986da90ce3bb252b66db107cc", "ref_doc_id": "ea23fb20-3622-41be-9eeb-f7b1ffcd6f3e"}, "c9619577-c5ca-469a-a4ae-c4483deb66a9": {"doc_hash": "b79061f1b34721bf2500f62ad97eba66b37487b068e3a1046d71f0a5de200efb", "ref_doc_id": "7dbc7fdb-5578-43ad-98c6-24ba1c334a31"}, "063cbd6b-e0d5-434d-bdf4-768debc755bb": {"doc_hash": "57b7eb1ff5dfcfc401bfd42e593f68c1d7501afcedbe4b4eb64fe8fd3af76c06", "ref_doc_id": "f9073010-c58e-4f72-aaf8-3fc9fbdd9be8"}, "8d0eef9f-7cce-475d-9571-5aa13c679949": {"doc_hash": "df892c46c2130b7f2c0e0cfb7646ff9a4a01a657bb5231283c1607ca2c706de3", "ref_doc_id": "7c947301-b1d4-4f9f-b0da-9d0638dd78ca"}, "3263f8d3-7032-4b3a-b952-66524a167bb5": {"doc_hash": "53cddb08615cc58ffacab7b234252dcdf7351fb648312aa8c11fdb5600cc6e10", "ref_doc_id": "c1f50016-dded-490c-b586-327d60cd24fd"}, "5810a16d-6893-472f-8b5d-85e8e18912c6": {"doc_hash": "a5e5093491906f3873da4a13ae3b522a06bef3159cbb3d3bd090e23cde0370a7", "ref_doc_id": "00a2b1cd-e5ec-4a1f-ba02-4d0937b9856f"}, "a5df5063-dc7e-4e68-96ee-f511149643f5": {"doc_hash": "6fd07e337191932859375d0359cc57ea90c877bdae6dd8b93c601dfd4955d400", "ref_doc_id": "4fa9d533-1a89-41bc-82b0-df65b93469b7"}, "e88f3eb2-d0d5-4696-aefd-4befcf947bdf": {"doc_hash": "df8e43565d4e5e73b7430f28749a784414e11a66f2ad99e4ca05774adb497acd", "ref_doc_id": "eb05c358-6d20-4f48-976e-fb9949fe9f1e"}, "c3e26604-c7c7-48ca-95c2-ee511b882588": {"doc_hash": "9ba4ea655bd40ab5c260265352562781cb0c3465303353a88ecd5257fca566dc", "ref_doc_id": "25b4ba66-98af-45a2-a5d0-24ac58e101ae"}, "59f7f8c5-d043-485e-8a24-23ef521783d7": {"doc_hash": "b99183ca6c07ac712ef70db5e5125be17a73310378b065598679fa57da01aa85", "ref_doc_id": "af6edcfa-ca2e-4446-a1bc-1ee64199c805"}, "dad6c292-4c19-49f9-993d-06899ac0c8e8": {"doc_hash": "792bcbb1ae5ab7da2743cbcb76711b7df94c45c4cf7f79440657a2b5f768c266", "ref_doc_id": "e45c5900-a7b3-4807-afee-8d21a304a178"}, "216d1c06-32a8-48c2-99cf-a3b534695105": {"doc_hash": "68bb1a424bc045af75cbaa8ae80bd79d937a9272ce70caced2fbf8a09ddcc289", "ref_doc_id": "58af4e95-83ec-465c-b917-532eaf22841a"}, "4877dcde-10bc-4f12-88f4-3dab34f84ffa": {"doc_hash": "9f8fd9374183e71e41c4805b71dad1f83abf2141cb5b2e87bccdbf89e5815c8f", "ref_doc_id": "43dd8565-3892-47ea-a3ce-1fe64ef5e9fd"}, "bf98c7ef-0995-4b43-bec1-89bf43575f1c": {"doc_hash": "5e0147a22191da227ea4172ad731e6c089bad4052e05ec4012b90f446b42c25e", "ref_doc_id": "8cf45b88-1dfb-4dc5-83ce-3228c341e2b0"}, "34a13c87-334b-492b-b2a7-aa445877b052": {"doc_hash": "2df1e8bb05067673d386ac5317173be427cd878fd854ad7fa52a832a88a0cc2f", "ref_doc_id": "188e509c-5349-406e-981a-ddb9c0deaf80"}, "c4148335-4999-4e3c-91ff-f92df512baaa": {"doc_hash": "c4283780f5974fafc6e680450267a277926c648893ebd91db231d222cf33b838", "ref_doc_id": "9fb8712d-f8a5-4e20-b1d1-a0889a32e6be"}, "22791e37-5bdb-487e-a96f-c328bdaa02bf": {"doc_hash": "dbd9c547a6c9c8dfd6860db554ae0afbcca6741387709df926f8aa1bebc02013", "ref_doc_id": "e30d9eea-520f-402a-8e91-da6d4cd8e348"}, "f35b98a3-2ea7-4cab-922c-739dca9256ef": {"doc_hash": "308f557522e89f18a9a01ca964dc647d3718e82c8cabb83960c44c99b1e70bab", "ref_doc_id": "5e2c8dd6-9a3c-4920-b630-4ad43af77f2f"}, "fd997ac1-cd96-4a8a-840e-843a500dc6db": {"doc_hash": "7ad8dba3c84b13283b532d8103bcdb7302dede5c84ca630d717c6381a6fc8b8f", "ref_doc_id": "73362b9f-f597-4ee5-862f-802e822f499f"}, "7678093a-6c1b-43ed-b2f2-1ed5cdd6501f": {"doc_hash": "ca6c046565e4c793d504293d666107aff48161f73bb03b4eae336a73bb0082d9", "ref_doc_id": "14e656fc-262c-4b05-be19-3d8ae7ebaf0c"}, "dacff4fc-3f4d-44c5-adb5-44bb6537064e": {"doc_hash": "af9ba61d39ece2255af2e7cf26de649c51a5f3dad9c8a2f0367ed2e8ee9804c2", "ref_doc_id": "c967edc4-6a3a-43bc-bb8f-0f443dc3ad91"}, "3294d776-e4ba-4bb6-8cad-113fa51f42ae": {"doc_hash": "c2acd05bc32cf611c8aa04513ef69ea51e22a57cf7a5a6cb45300174b59f4db7", "ref_doc_id": "b78c2017-73bb-4dbb-944a-cc3545317c0f"}, "1b59f1de-1355-4908-9904-f96b8849f0eb": {"doc_hash": "b322a4d17bf9d7895f70c0a3a7c8cda693f99157ccf2289573dfbb862fd1b21e", "ref_doc_id": "e79a7249-4bcc-4c2f-a34b-411c57c0e680"}, "31586065-9298-446f-b62a-d80751687c55": {"doc_hash": "72e92c9c00bdf56d895c7ce7f745256e84bc02853d8e618fcc857202231f1fb0", "ref_doc_id": "b7bd7f6f-6002-4892-af39-65dd40f50eb0"}, "bbd06ac2-fe81-4d34-bb23-7ca7ac23f581": {"doc_hash": "2071dd75c2cbd3abb4e870f09d73a80622e135bc2a4d9a5fee4480fa9420911d", "ref_doc_id": "64d808ba-1df0-4e93-a70a-6046f91fc9b6"}, "b8d90375-da1d-4489-a9fd-c1a34cad52bf": {"doc_hash": "0af3b477f4d800dc08d7b076cac152af4038d70c5aabc3c8c0ff6a052fbb1e43", "ref_doc_id": "1e29db17-a67a-45bb-b96f-5126cca22574"}, "50bc80b1-5974-4341-990b-406f24607fee": {"doc_hash": "66c76c34389d10764af49199e2352481be47cc1b24bf9a2ba80de988662ab87b", "ref_doc_id": "54bd53fb-b3c9-43b0-92d1-cbc7984ce1b4"}, "6ba86243-8cac-41ee-b74b-c7746d4638e1": {"doc_hash": "997f264aeccbc2ad43c0fc2f59d14d2c86caabf0a93a26e45dcff53dda80527a", "ref_doc_id": "90448a71-83ab-49dd-b4e8-c7bb32a0bb19"}, "e09c3867-c431-494b-87db-209906c84a9e": {"doc_hash": "39c77ebd3e109fb43f31351e47f4356a45dad9b7b37b2d0bbcc811f192aaccfc", "ref_doc_id": "7583019a-8eba-4fbf-b74a-3f19ff212db8"}, "86c0d7f7-e884-45be-954d-88d2116193d4": {"doc_hash": "ab6d3eae40661024b9da90012c3f4bc223807319de78893fc601cb0ae49ff958", "ref_doc_id": "6b9ed295-4a6f-405f-8676-4e3c226d1d32"}, "81bb5a0f-67fe-4ae1-8f68-17342acc3df2": {"doc_hash": "5cc8bc3cf650c5853f840e6867368282af72c39c5eb883da04097c156bb32624", "ref_doc_id": "cf7d5fdc-0c39-4329-bc87-1e908af0b536"}, "576752bc-8452-4af8-b894-a2559fb1829b": {"doc_hash": "30a8cb0bcf7ca8b3e61a3a916b5ea9a27f9be0c609b84d4e8581e279985ab406", "ref_doc_id": "fa79d2c3-2afc-4ade-8b45-f59d50e199ad"}, "98177f92-4733-42a7-bfd3-3f25b500ba46": {"doc_hash": "536e65b19c9ff869a20f61a0cca7af022d8f60ee19c622365b7b293ddeaef726", "ref_doc_id": "412db2f0-0a38-4e23-99fd-1ebf965970b0"}, "68021843-5627-4056-8fb4-c2e5aa00f90a": {"doc_hash": "4899d9ab5a65526d4450a16937cde86ea8dbd898a48208d224a4957bed3f123d", "ref_doc_id": "0b458b23-f178-4f3c-98b8-afde6ae5afff"}, "67b5cc36-765c-4750-a1f6-42e799fbd1b0": {"doc_hash": "fd3bbc18612872dc0dbc4bb344534584cb2851da1cbf0eca4ad5987cc86e6cc8", "ref_doc_id": "83bb0a77-8bbd-45d6-8d8b-442686baa93f"}, "6ffc0892-86ea-4d16-bc1c-fe67fc19732c": {"doc_hash": "a1022103dd518635fe255eb1e88cd1ca90eae3872a44f8311c3fdd3066e6ea40", "ref_doc_id": "aa89f926-9c71-4505-9d25-560d16ed12b8"}, "609f97b6-26cb-4808-a87f-5ce928d6c058": {"doc_hash": "44f93e63777ba1fab160287054909db2e5c3ca3d5da20aa32484383c1dd8a73c", "ref_doc_id": "da579029-b1e5-494e-94da-2b4308c358fd"}, "13caefff-6538-41c8-8fe3-7016614e0cbb": {"doc_hash": "b040d38f0163e2709dadc2657547a362c10863be58f987c358a34066d40cba4e", "ref_doc_id": "68b9ca2b-3cdc-4e47-ab12-b55a24d96994"}, "25cc1f12-e825-4d6d-8bad-3e6f99b6b7d6": {"doc_hash": "1ad73b5a42b2a236946d0bf4923a35c0ff415da7d32fbdab74d17f59aee8099a", "ref_doc_id": "faa98306-561d-4487-bb4c-3457f11054e8"}, "0d3a096a-7a30-44ed-b67b-e302b888be7b": {"doc_hash": "0c802f136ac5884fd7d59a7e98c94497beb6ec3b2babdb204dd944ef2e767b50", "ref_doc_id": "3a4c0e3a-68f2-416b-b32e-a35b32c28d1e"}, "50aeefcf-09e0-48c3-9dd0-848cc2f952d5": {"doc_hash": "981d79f425a2ea690fecaa60482c053d9f487eb36e05803785d4b2b976f0cb44", "ref_doc_id": "120c6e9f-a10c-4bb1-b4a2-5d432821ef3c"}, "b06129b8-7300-46e0-ad67-5c57caf159a7": {"doc_hash": "2b0952b1fcaf8626c531a7a0433be79746ff2df8015f6a4beadab8544f4eb144", "ref_doc_id": "d53bbfd3-1a0a-4b43-ba15-e59e94eb55f6"}, "116bfcf3-1466-4eeb-863e-e52124d07c4c": {"doc_hash": "da3a77dbdb52a5cd05b53517d93e8d9bb13ab92f8505e3a32dbc578a94c85792", "ref_doc_id": "482ed358-fee5-48d6-b8ae-28a68d81bd88"}, "e6274619-0c2d-4350-9efe-7d1d7a7f14e3": {"doc_hash": "212e8b3fc1f6af9132bda7ad6e1ffb809a84109db72605fe7314cfd33374dfdf", "ref_doc_id": "1720d139-5059-4eef-bc86-48cf875a2f3d"}, "e2c2892e-c563-44f4-a6c9-dbc56809415d": {"doc_hash": "01e09a88c5f6196ab230c3e48640a913ea87a69c2dc0306335266de0da54394a", "ref_doc_id": "79e0157c-ec8d-45ed-b9ae-b05e97cf40d0"}, "cbad8c5d-9e3a-4359-a41f-ee27b76f5860": {"doc_hash": "17680ec62fced9e6d82f8a6a08dd048494e26bf840ae49d50f80093adb9e5a5e", "ref_doc_id": "2cc0ac07-6850-4445-af05-252842503dba"}, "791b711f-69de-4a21-b01c-4283251863ba": {"doc_hash": "82d10bf2f554a83a49a5f15d25da33a5a99d25985e35d9a210a0d34d2bd2bd36", "ref_doc_id": "bf7665bc-412f-41de-ae27-21b1f6e753eb"}, "c4c48bf6-02d2-4a2e-bb1b-8c17f995d0ba": {"doc_hash": "43dde5dfeb6d3fc03c9209284e1851bd27508c15bb7813c1b2b6cb4ed6fcd8c8", "ref_doc_id": "88809d3f-2ef8-45ae-b4c1-b65680c677a2"}, "9eaf72d8-ccab-4694-9f02-b63a9f53941b": {"doc_hash": "c3dbe85069b721eccefaad8933f89d1650a9cf723f903882639f60cf31a7ea25", "ref_doc_id": "db2aad2b-71ab-4233-9288-efe47b532330"}, "2ad807a7-035a-481b-958d-aa719aa1bd62": {"doc_hash": "b459ba45c0af58912a45c251794a72511a5cfa5d78dc703efd2b81305c29feff", "ref_doc_id": "322b07fd-e851-4ecc-a25a-158ef097b19d"}, "3ce02b55-97ba-45ce-be17-2c560490c541": {"doc_hash": "1955985d197331821cbe1b00bb6ac43e0c325a441ef34c45c72918a4a365f7d4", "ref_doc_id": "ff0457e6-8a07-421f-a198-602ffd46e3ca"}, "d9fd9a61-a918-4f5a-b19a-b808c88eb9b6": {"doc_hash": "6af5c1d3e814136c70422f089e9713941a2dda6e57baecf6c805dcfd37f64417", "ref_doc_id": "867646ba-cf8a-4481-91ea-e3d92801ed56"}, "0e7699e9-e098-4ccd-af7d-fea0a5aceb08": {"doc_hash": "7a6b9a27a5f31ccf606b3d525910ad20f0732415e820fe4ec9c585565547383d", "ref_doc_id": "8f9c8a35-dcf5-4e3e-8dde-bb4225b64a02"}, "ac93db33-e2f2-4774-b83c-16306a01caa8": {"doc_hash": "2bf0df6381c0712c21a1dc2655af17143df66d3c37a1d87d05df0d3ed78113f6", "ref_doc_id": "3587c476-0adc-4da5-ac3d-a134a161679a"}, "50cbe6a5-d26f-472c-9adc-39be7884befb": {"doc_hash": "c3f6bb709b06d84295cadec9df0524089c5549da261c6613fd874449251e420b", "ref_doc_id": "9d0626c5-a740-49b2-9fa3-81bec6a92524"}, "ea3b924f-432d-4dd1-aa1d-4504faffe5e5": {"doc_hash": "97db4beb5789bd8578d6a558a8032ebc49ac03ebde4fa24b668b17b339ecb495", "ref_doc_id": "87da16f3-4133-4ca6-8b5b-86bed7e5d269"}, "b7ae68f5-9604-4a49-a76a-a3789a2972d7": {"doc_hash": "50ab3ef92dbd516c8d2d10161b48fa42a1b395f56be040e88d2ec0edd94ceeb2", "ref_doc_id": "6a077399-770d-4ab8-b0fa-b7f1eee32c1b"}, "19cd4ef5-b878-46db-92d4-ccf3ac0ddbca": {"doc_hash": "03f1d2d84590535964188168a842235a10558a914f107dcea3864b4589fad5ea", "ref_doc_id": "d04fe36c-7c1e-4c5e-bbac-4b09e88491ff"}, "384a1e60-6729-453f-9033-ea2e643bd01a": {"doc_hash": "3c92caa935ee11d3f7719c68e824b8d6f3715fd09800125b6a12cbe68157bef4", "ref_doc_id": "c2a834bf-6d74-40b4-816e-d8c54cf34a3e"}, "00e7d5c2-a11b-4814-961e-23d8796fe9a2": {"doc_hash": "aed01d944d1aa709fdbd5535ebadead687ae6659762250092fc2dfa7e17251ab", "ref_doc_id": "09db70e1-3b7d-4701-b128-4698645f97dc"}, "0fd2499f-16f5-4a2b-90cf-fcf54bed62ec": {"doc_hash": "6f0c319eab6601bb92bdcab0500886e892f310d29bde53708aaf57ce43fa6aac", "ref_doc_id": "fd90ccbb-7c15-42c9-86be-319284631003"}, "5d6b2daa-3afd-4c53-bc88-b8f47961f321": {"doc_hash": "127e8b16a1b0d7764e6eaad512bf70d4d1d039ddc405f8987ba5d91cb8c2601c", "ref_doc_id": "23f48073-953d-4899-93ef-0651343e3e8f"}, "7f16c85c-87d3-4267-b5d2-a49f008bc64e": {"doc_hash": "bad0040be157c453e6a7fce53f170692e21dbd716a3995bce02fb0505af46b89", "ref_doc_id": "d1dd4c53-d2a6-4606-8bd9-bc5be7f62886"}, "8437303b-3315-4973-bc25-743ae12768c6": {"doc_hash": "35fe982843a79cffcd9c7d8e7266fadc625030819fb3abbb870da1a2b9586782", "ref_doc_id": "fa3dd2b3-eaed-4004-8670-47e0622c88d4"}, "5ada16fa-0a5b-4fcf-bc85-b700eab5fd8a": {"doc_hash": "d58582f54c2c9dedee45ff11569170a22f78472657be9784629ffc822bf955ae", "ref_doc_id": "9bf1ed3f-0350-4171-b261-32fc86adf950"}, "fd45c0c4-8a82-417a-9488-311ee81969b9": {"doc_hash": "871a61829e792d20472fa48da7c8a04c6e33b5d64a8eae508692294c6e05bd8e", "ref_doc_id": "9c54a1e3-b4a3-4db2-a522-983b4aee673a"}, "68e96cb7-0fee-48c7-a55d-f994f175fa50": {"doc_hash": "1b2d91907a43b4fe87bacc769e1391b3034b5acb888249b33645cfed1a6226f0", "ref_doc_id": "1ef3dec6-3329-47a8-8a9f-d19f50b2eac4"}, "4a329bb8-9774-47b7-a955-61ac9d2c941f": {"doc_hash": "419ab9f98996c5c66de08e0fd2a9fdf8a92f8ac601dee9f0fae008ba59063a9a", "ref_doc_id": "d35f839f-bda0-44e3-b13a-16d7442529b8"}, "e95c6fba-5404-4a4b-ba3c-816248317c40": {"doc_hash": "24743ca6494ffda85963d4fdaad4a8ec05e9cdebd190fcb2758de1258632b896", "ref_doc_id": "2a2a826c-0d5d-4894-aca3-36f6217171e2"}, "728151aa-b55b-4999-bef5-fb14879177ed": {"doc_hash": "1031d8e7d5cb2d32037dc3dd353a1020e521e88ed7d4976958f51187d954a0cc", "ref_doc_id": "6d374a79-2716-4c7f-a9eb-4cc498671e55"}, "f5892d2a-167a-49c7-914f-956c062c0627": {"doc_hash": "9364212f23db948fdb0b798971380bd7ad3461fdd64153798799330b6fca7b11", "ref_doc_id": "72a7ede3-dc86-4a0d-b5be-09f55f33385c"}, "8efcdd85-eb0c-416e-a49f-b8d9ab608a05": {"doc_hash": "79971ec0e75e78086fda6e139e6fcaa3b36c574b51f9fc14de1a0324dc9d8c07", "ref_doc_id": "560c1cab-acdd-48b5-acb2-adb67a47578f"}, "decd53f1-5f7a-4afe-b645-707889a1216c": {"doc_hash": "5137eaf114cbf75230d6b4c0c9c8a79582a638b06861c37c33b4f24231ffd4ae", "ref_doc_id": "515f0944-1f53-4423-8c6d-a87eab0f7f95"}, "e3a11132-ca32-40b9-8213-790109da8f69": {"doc_hash": "9bf311ccf3edf21bc16fd51a96e6efe9366c7d9e3f753eab36545bdacc929e6f", "ref_doc_id": "73adafef-25da-451f-b493-566bd11d5c40"}, "6de09143-cde8-4e52-b50f-029d6a6a5eac": {"doc_hash": "143c08b7289764a0cedb3c1f9ffd0a5c1dfc89b296a1006791594b943b967397", "ref_doc_id": "a4d672b8-ec88-44f3-bc90-9d86c5d48109"}, "76348446-a0bb-448f-8a1c-fcccb1a16999": {"doc_hash": "fd3db9141e5cebfdc27621d57262a458f07b8f8ec63ab322c65e233d57b79a43", "ref_doc_id": "13970d91-016a-481f-b307-33161b97b8eb"}, "94c532ce-919b-4b9b-958c-361d74d37de7": {"doc_hash": "824bcbd3969413cbe4135bb6d0b99ceda14e2492dcba117b5ddbf34a351493de", "ref_doc_id": "aac50e13-4da0-4918-a806-b3ba375fdb04"}, "ae5712b0-124d-42cc-a99c-14c7d65c700a": {"doc_hash": "ca4eb178301ee7975b6db67d12e2d87ad4f4101fb1811146419e69b9655fe3f1", "ref_doc_id": "c7789da2-8273-4181-b40c-9154cd41895c"}, "40979d75-6731-4286-9dc2-567a27b3fb29": {"doc_hash": "36eba286cab2c82df9ae2fcaec55b3691c65a19e18b692b30a5ebe07056a3316", "ref_doc_id": "651c4294-609b-410a-8e65-22a4caad1033"}, "b89a4454-7b2d-4728-8e37-770b0d0a1b46": {"doc_hash": "e82d5c4dc033447fed77e6d1c8768769fa3ccd465b20afde35022cf6238b04cd", "ref_doc_id": "fd23dcdb-4230-496a-a80f-e38106f2907a"}, "1a8ca033-bdc0-4ebc-ad68-ee5a372aabb2": {"doc_hash": "e3937c377555e4d19308166d85f9fd0ac660feeff78a0124eba72cb3e24f7692", "ref_doc_id": "e8f8372e-28d4-459c-a3c4-acc0d9e92a7d"}, "1585ce30-9668-4d0f-ac1c-74624f36322c": {"doc_hash": "c669cc7db7dfc9e427b05bf8332507f52f87c2a6cb4a5bbecaf97b155a7f0398", "ref_doc_id": "d8382130-67d3-4698-8a03-9f5e6787daec"}, "79e94695-9480-4661-8700-a24c9252926d": {"doc_hash": "efea18c252157631d3d260b49c6aa87b1e4324d539c415f0e14954470c924b48", "ref_doc_id": "f1999008-d6b1-4f19-a3d0-998631528ad6"}, "bd6ef9ec-3555-4604-b7fa-25d38b984ef5": {"doc_hash": "d7b90b81b178e288545eaaeb1ce4b21766b18727b00213ce2a7bb4d937de0422", "ref_doc_id": "852ba98d-947e-4f3e-bd60-85b47344026d"}, "c7a629a0-636f-4d05-a893-46ec916239ec": {"doc_hash": "ea55b8a9a0d7bb1722f88cbf5e2722fda1cb30000b87d51d80d650fef3717cb6", "ref_doc_id": "803f1263-84ad-4d19-9e7e-9ee38a82ab5c"}, "8916d278-7451-4e51-8c4c-844938a342d7": {"doc_hash": "7d57abba2f574601c3fb13b8ecee08745c9b9b930a1b70d94c6c570e249a5ccd", "ref_doc_id": "1af94268-07e7-437d-a480-295645bf151c"}, "5672abde-9b9a-4fdb-8e8e-69c76099376f": {"doc_hash": "7ef5bbbbec048bcd4c6365e326e1d4e69dcd9e0df18743961fd02ffdf02ce54a", "ref_doc_id": "3edb30d8-c393-4de2-b658-be1a77b4a650"}, "43d021b0-e35e-4997-8cd0-44001ed9d7d3": {"doc_hash": "b30fb5ec20a657e268f4af480ec9ea7f58db90cf407a7de71e471c94bb50bada", "ref_doc_id": "9acc31be-fb6d-4ad3-9653-3b8567a06c7f"}, "720d0810-6731-4588-af7e-e73cff16cda9": {"doc_hash": "faca7fe007b1d1f1e834d6d657cfc227d9c05645a7029ed475d6b2e4580f93d4", "ref_doc_id": "acb8b2fe-19dd-4459-8e9f-e244c68af468"}, "7838610a-58a8-4e0b-9605-7ed75c3f322e": {"doc_hash": "cd19ef37efa9514caf323abf71e5ccfd0d722d9954947708b64779a4d392b9e0", "ref_doc_id": "aa2afdb0-3728-4031-a02a-204891e8c9b3"}, "2468823a-ea15-47dd-a3e9-1e7069d87574": {"doc_hash": "f22e28c2d83948ca1373f6913759a2e6c0fe6d2584a297430de97a20c7b407d3", "ref_doc_id": "9a4d7b75-e30a-4b37-9bce-dcbbbae793de"}, "1daeba27-ba33-4235-8409-54ce054bd6ed": {"doc_hash": "5c5fb67e484cf72af868dd7a97561cf490360efc94f796ab9d9f74763c7980a6", "ref_doc_id": "f568230d-3a92-4131-a13c-e1a0b50a617e"}, "ee66eb48-2e1f-42ac-927e-8f38364ca8f8": {"doc_hash": "64465723cd7ad9da1ec5a0f5bb123bba7afd83bfb75cf1e727fed42101141a94", "ref_doc_id": "445396a4-bf24-41df-bb07-c1006109eca4"}, "9640be44-c19f-45f1-876b-ec09e99ee6eb": {"doc_hash": "cc1f129cce7e890c9c93a9c50ec9bb274852207e8a956ceede9f309963c9d4ec", "ref_doc_id": "0410652a-7d46-411e-bd7e-f8b3011562dd"}, "84d4066e-d111-433c-b1ea-ce9fa6c0078f": {"doc_hash": "1e656224f6cec3de6a0dbf0ed874da7ca9ed4d45c8cddff3f26ef30ee9a2afe8", "ref_doc_id": "160275d7-e07b-4540-b4fb-84e02885897b"}, "895f0c71-f883-44c9-a79a-91536a2e3ae0": {"doc_hash": "722e9e5b94938ee735382ebb560841d19120d7fbc3731f476e6a6ce1a9c241ae", "ref_doc_id": "eccbc706-8b36-4643-ac26-35e67e98a2ee"}, "05fa6c71-e460-49fc-98af-052af603b48a": {"doc_hash": "1fd2b9554cb9085688337f87b93a780c579576cf50d746b7eccf4953c1178b35", "ref_doc_id": "6076a4a4-5a25-4e15-a6e8-3eca1aa2cf6c"}, "acf1d12f-f167-4d6c-9712-41b183869668": {"doc_hash": "2c6cfbf78d9aa1fd5a16b7c52da4361caec57fdcb35037f09948ce7ef63ecb17", "ref_doc_id": "eb977b19-ef57-4f49-bb58-c50a9eb3bef1"}, "654f7ac5-3dde-4845-9a81-6f6e3895bb89": {"doc_hash": "26e51e0f78ee51d7f3c67b172167c9565d07a434fcc76593edf314b3bfb183c0", "ref_doc_id": "8e5efcaa-2557-437b-98d7-f2856ebad259"}, "41a99df0-b334-477b-8459-7ed49fc26891": {"doc_hash": "73dfa90a882a4cd18c7b5ca46b3f98523a81c500d2cf193e0fad91ff151cd0c4", "ref_doc_id": "153ac4c6-6ee3-4845-a037-aa7f54d2c5f2"}, "e0d1b15d-4966-4b13-a5ca-c17dbb3d1c0a": {"doc_hash": "4f2928333d70ae12057eb9a4b205d7c0b4962d01d25c827dd58661852e10bac4", "ref_doc_id": "5d2fade2-e276-42e7-a997-cc4ae8877b43"}, "8971917a-81c5-4bf1-89ee-ffc8939b3256": {"doc_hash": "2b781d246536eed9a57210d6343f82d5054b1c9b3e6fa0e9ee4ac5560a2628c7", "ref_doc_id": "e77a9e55-53ef-4dba-9d06-1b7d6f0f255c"}, "28c31606-5e86-4826-884e-5313d1597091": {"doc_hash": "08d278e9de7f8f7b0e3629db0a467e9b57c3f0fe9a9d5c739e244a6aaaa2cd90", "ref_doc_id": "9c491f2d-3df4-4af7-951f-5228dfc4fa95"}, "66bc73e3-b98a-4189-ab07-d00ec4fa5ab6": {"doc_hash": "26e814b53b9e2b2558c6704b34400893bf17769e36e1d9a12bd10b24299d9de8", "ref_doc_id": "4750bf8b-3a68-4c1e-9779-bf8d701a9b22"}, "635e9b56-def0-4ecf-98fa-ea006743d31e": {"doc_hash": "d4b9166b186da99f9df979b6fd10e683d90289f2135489a2f49571189e67529c", "ref_doc_id": "1c6439f7-2cb9-47db-b37e-559a7b28adc2"}, "3395945f-3e01-4358-b8f0-cf17d89f5cb4": {"doc_hash": "1fbc359c5a525853be3ed450a3bf6a931ea61951bce6b5eadf24ab1e3b96c940", "ref_doc_id": "4e61c6e8-2b9a-4906-b237-821659570f63"}, "935a4940-81f8-4af2-9295-c1becf22dbc5": {"doc_hash": "ef6cd1bcf4b470043255e88175394d25fb2c992cfbd9dcc7e13a51dc9c645d93", "ref_doc_id": "d4fe3759-8fde-47ee-a62c-293b66b8da52"}, "be4ebb7a-4e2e-416c-a43b-a76a6578d47f": {"doc_hash": "655deb092aed692995d023f0e87e9c1c08bc6c4d77c2de0c7d65444a176e7516", "ref_doc_id": "e3e5aa5a-c374-4387-99a4-a9cc19d555a5"}, "9d74a19c-8be9-4bb1-9b9a-15116c23594f": {"doc_hash": "7c9be5f1fd8e02129f3fc7d92a8cb31d8b79bfb6929e4c93b7873ee5458d9f55", "ref_doc_id": "1ce3c3fe-f692-44a2-9dde-97a2ae12cd75"}, "4b0147a0-ea81-4048-8867-695fce1b4572": {"doc_hash": "7ea60710a7dd1ad6667d6fd8b036f8d8c7e85343722b0df3d0939651336fb042", "ref_doc_id": "c9fafd24-eee8-49a4-8eb2-ce712306c18f"}, "f7b28bef-65d3-4474-b1c0-67f52e416957": {"doc_hash": "4f003ae9c086a0fffdb1899829ad478635146abca9ae25608a21073f5ecc194f", "ref_doc_id": "3aa269f0-73fa-4dc0-8c3c-f3266ad8ba61"}, "5a2feebb-3390-43eb-9f3e-c75e45cf1df2": {"doc_hash": "e0557ffea9a4cfd7edd8099e9d49af1ad69ef3eb2754c0edd5079764c59a44e8", "ref_doc_id": "f20fd27b-836f-4a34-8f7f-45cb35b82c5d"}, "b3fb5069-dbe1-402e-8c45-a02b37f7a667": {"doc_hash": "d347f2fb94bc56856d6e302b0ac78ffcd49db018d5b92b53095ebfe0676379a0", "ref_doc_id": "65cb6302-11ec-410a-a09d-d363e6681b30"}, "7c78b4d1-4479-4180-84e6-896b52a49b59": {"doc_hash": "7b47204aabf8498e2546aaecf27e1fb0f58146d61792d843a62d5ab0170d7bec", "ref_doc_id": "60c59991-ce38-4238-8ec0-9c246db55037"}, "c9825965-5697-4662-8c4a-4c232afbdbb6": {"doc_hash": "eec2254603325da65b0b4a162deea0426c365977d2249a254ea4dba9ed1a70aa", "ref_doc_id": "fe439298-cfe0-4463-97ab-99bc311cc931"}, "3f554e09-0eb2-456d-9f3d-fe63303ab00e": {"doc_hash": "a0048d22c8e1fe126814417ab4c0861c4bb8021fe36aba55d1adf0540514db53", "ref_doc_id": "d7821250-0b01-4a2b-b2bf-6dd63571a49c"}, "0c6e2764-d9a1-430b-a1ca-7d80ffa8b4c2": {"doc_hash": "a8d25575b1bad736e3729dc67c99b2973465f6e8cfeda65f9ceb929119f1f3ae", "ref_doc_id": "72de8e2d-7d97-49fb-b8b7-f70a531dc765"}, "d93531ab-9003-4784-84ba-6c74b997a25e": {"doc_hash": "e7aa39a244b9c6e91cd2a5661f901920f5e14dd0ff7476f4d908d6109a8c01d8", "ref_doc_id": "c61a6dc0-556d-4c76-8a8f-55b540a044a5"}, "c68d6c15-2f0f-462b-8ae5-49292eac689f": {"doc_hash": "972a6c1c79f65e078e7538979c3b1138cbf513ae0f85042d10ceef549c413d49", "ref_doc_id": "a8b5e6cb-2084-4194-b3f9-2e0d28a2ace9"}, "68523497-0887-4eee-aca8-9de6ee6bf0f9": {"doc_hash": "b880cb4f1e063e9015721943b0c928499cee367e2fb1cbdc5e0de277febeb3c9", "ref_doc_id": "b972f8f1-81bc-4f21-8e89-af214d24de53"}, "b66df359-dde9-4911-aabe-45a689637520": {"doc_hash": "156da341d733cf99921a04d119c6cc4a4dfbcb71434604d8d9e0c32c1e823d98", "ref_doc_id": "387d9dbc-ed88-4627-8c63-ef80b0c85ca6"}, "a2d474bb-9bba-421f-a30f-0767ea823b31": {"doc_hash": "91af6bbfd314275e8d18e7325b537885c78e8dc06f049da653fb79086f492687", "ref_doc_id": "53e143b1-a573-4c0c-8bda-5a12b768494f"}, "6132f262-57ae-4b6f-bc6f-f86cc7599bec": {"doc_hash": "4af8e0f8d1d0ec0b35e091b3d5ba68fec75b6346e3f0d5aeeb27a00c3a3af7d1", "ref_doc_id": "9ebb6fa7-18d2-45c8-9731-195261311d58"}, "083b6a6c-d08f-48b0-a1a2-fe471269d76d": {"doc_hash": "e06139659923713b4972dd6fa6955564cc96969da83da99de3c69b4ea312b615", "ref_doc_id": "f9a8ade6-6a44-4351-b38e-5d7af3a0d849"}, "0deb6b37-d365-4fdb-93c6-5fe75d044021": {"doc_hash": "df002405daab63f402f7aa0d47bf24d409234c93fd8af1105307dcb456149b00", "ref_doc_id": "c04d2823-7a3b-4c65-8ac6-b5c059b0d586"}, "f6cac2cc-b0c5-4f03-8137-bd876edbb7e2": {"doc_hash": "a283cc9fb847bbe3d3f7d624957dc0d5e96d087b044d08ae0b715d42c9f3ffd3", "ref_doc_id": "19e899f6-58ed-4912-b450-067a8dc6ec76"}, "d34280c3-02fc-46d0-9c2c-11a8f3309459": {"doc_hash": "77a58a5ecefdcaf47e843eecba1593eb3f826611fa1b1bbb02e9ff51885ffe60", "ref_doc_id": "990864f0-e4a3-491a-80d8-ef1f888d2243"}, "c87c573f-5609-4301-9efe-b9ff27db84d7": {"doc_hash": "a5f09bf61a22a728adc9391603c6ad1237d3efb102cdf22ba0d6545723fb322c", "ref_doc_id": "b7a5fa17-2272-4e4f-b9ff-42eb33351f00"}, "fc4f1bb8-5f66-492d-8840-0903fe9f5bcd": {"doc_hash": "aefa3a87e92f7fc26238cb7d9c1f09e57a754b15e779450ae3c67ef88e32354f", "ref_doc_id": "6bee99d0-8710-4e05-b350-1a7b57085706"}, "aca4ee3b-e73c-48ef-9820-c69dfa3f7e37": {"doc_hash": "fa49c40408a158d7d3f1b10e38a0ca590976e6e24986fee694add3339b6102e1", "ref_doc_id": "e9a340b0-98de-4ddb-912b-980b3e4c3e0b"}, "c55e5a51-0bd0-4cd9-bd65-1b8f79485b83": {"doc_hash": "f235ab0fcf6c8e85a282819a4f9196e3d13c556d2538cc5a793ba2622075e598", "ref_doc_id": "156107b4-353f-4f28-918a-3c38c94f1ef9"}, "e29ca47f-1b41-4bc3-b494-ecc9c043f8c8": {"doc_hash": "eccc2f89018d0a03a77b5a98df60fc51026d0b198b0cf46a4fb9be138a69d227", "ref_doc_id": "c0c0822f-7956-4b29-83a5-dbbd48fa902f"}, "b17a7dfc-24b0-48d5-802b-49a2769446a9": {"doc_hash": "cdbf87f2973bb0ed7665ca1f22cab3ce6afd6381a6c11a593d17b74140f30af0", "ref_doc_id": "d655ab0b-800d-40ff-9906-a46518fc0a1a"}, "f9421257-8b15-4f71-bbe7-03f0cba9668c": {"doc_hash": "c048567bdae7d59aa2f18a36c930c67458015ff0da02dee56768be4baa531678", "ref_doc_id": "bd1a7fd4-54d6-49af-851d-bbfa6d40550a"}, "8eeb7705-a472-4312-9022-1ac3f11d3165": {"doc_hash": "cbc3736e17bc43076f2b9a5eddb7e47916ec7ebabe2c7a4f3ece48a11aa66593", "ref_doc_id": "94b9351a-4afa-4bd5-adf6-1cf345338c7f"}, "7b5dbb7f-c518-4603-8b8b-537c65ff3972": {"doc_hash": "28036a72e5dea4671f8bacd8a786f41d448a50b50037bafb2bab80535b3774b0", "ref_doc_id": "178aaf7f-bf08-461a-a57f-c12c4f9ce958"}, "abe32742-e3bc-4286-ab77-ea0bbed371fe": {"doc_hash": "526cf748db65f2ad5618fd429341c5543d21c766085d6cd2f858c1238aa8e671", "ref_doc_id": "80a5ac38-0c86-4869-b25a-e8d6092cbb24"}, "e89e8891-2c04-4b4d-853d-4f2f4afa0282": {"doc_hash": "0ace241b53228d4b66359318c6cfc09cefb73d97014e4c456f20b7138f313786", "ref_doc_id": "2e961a04-5386-45cd-81a9-071811a90251"}, "e02884a7-ef3a-4e64-b487-e10ae0cc87d2": {"doc_hash": "0e3aaf8192ad04bb7843ae9010c9bac25f87195086bee2bd8794692df0856574", "ref_doc_id": "da4c5e64-8583-4745-b487-525f62c94d4a"}, "d20cc65f-4a99-4f59-a936-4a56d2f5192b": {"doc_hash": "27402312a928621c660976438ff31ae668d16c871f6b7c82d3c72ea8476fa9f0", "ref_doc_id": "f17a5f6e-aba6-43dd-b16c-ce2f8f0ccda5"}, "dc0a468e-9a50-4ff1-a0e4-44cd6ca95946": {"doc_hash": "2fe205e4178cb6c320375352e91406b6c54b50cc1e527eae2762dbdf166ec8fc", "ref_doc_id": "227d6bfd-12ef-4b74-976a-7e9a1172a4c0"}, "729f5c96-5038-41f1-8c24-27e01b876eb5": {"doc_hash": "65ab312faea935c83eb6f0a4e22bcab4fe3933e591b66141bf2324551aa6f591", "ref_doc_id": "baa93e5e-7e1b-4cfe-b6ec-84a834f32975"}, "9acb9d48-f704-477a-8820-8620bee4aca0": {"doc_hash": "78a54c0a96ea72b9afb24e05fb00261b4f7414a1a7febe859feabb4b24ec1e45", "ref_doc_id": "a4c97274-2275-4e9b-bc89-c25afe47e7bf"}, "bc690db6-089f-44ef-98d3-afbbc92ef03d": {"doc_hash": "cf8bcf1e076066e768c3377d43771449a8899f09cedcf81583e666bd94012ea9", "ref_doc_id": "8cb2d50a-b403-4f28-a5df-fa8fb3e5b5fa"}, "d27bbdfe-d8f7-4e2f-805d-8c4f68e43214": {"doc_hash": "fb2dca05c72a5c19b68ec22152ac8409132f62790021f4e42c3afd11ad40ffbd", "ref_doc_id": "8d7c013c-ce75-407d-b7e4-61a1f819cad7"}, "823c97ec-8e0e-43b7-91c7-5361c3f40d2d": {"doc_hash": "15feed0d1703338f26ef87c8759c4d29aeb01e22e5c9fcddff82b706eec504b9", "ref_doc_id": "9360c30c-183b-45b2-ad4e-c6e285753388"}, "56fe13e5-7223-4838-bdc5-4cac69a914f8": {"doc_hash": "70c8f643d1bfaf0ba4f0ddf671d2225215c245a0f181fbe1996493eb5eac7bd2", "ref_doc_id": "25de1824-7c41-471e-a874-be67ccfcd70e"}, "16f87239-f9ab-4251-bf25-05d146002323": {"doc_hash": "a967b8adee1222adf46bbad38ffd6e718b6f26936538627c84277c7f3212060e", "ref_doc_id": "57201e05-589e-46d0-b4bb-a6c0a7ef6427"}, "50d00309-9cf0-494a-aa60-7632293cc74c": {"doc_hash": "a5bcc7b7a312570148007458e296174179a1608df35d68c92e4819b31c40288c", "ref_doc_id": "9ae2302c-4b63-44dd-af6d-8ef751b710c8"}, "40531ee5-37a3-4f9a-a585-e533b8c37ccd": {"doc_hash": "05f047bf335098913cfd1d4aa8557d3f2d2e90c88ffc1a78290df6bc36872b3f", "ref_doc_id": "981f28d3-ec7c-414f-98d0-54474de618d1"}, "c4fcf5e4-1486-4698-9e27-c7cc51fc30f9": {"doc_hash": "917d2c0278b020bdff8b139f730d83ebd7a20ae1640e8159f1ec1784f3965d5c", "ref_doc_id": "56daa91c-dac8-414b-8906-123be8275ce4"}, "0be00f96-1ca3-4829-a8b6-c8d801f47d78": {"doc_hash": "88a04ba1641940186c42bfb3adc8820ea8d752594edf50dc7cb25b8b147082ec", "ref_doc_id": "e35e66e4-8188-40cd-98a1-63f018ebdd8e"}, "a1bd49f8-d525-4600-bfd7-94b58caf6198": {"doc_hash": "a4b633117d8230744a674fe97262690b596a8b999b70e96255240b80686421c8", "ref_doc_id": "61471dc9-906e-49a9-a1fc-43540c3c2cfd"}, "c394b4cd-ba3e-4d5e-b491-fef5907482df": {"doc_hash": "15f19d0e65dd604918e3aa9d73d51704cbc8b647ac4fd8b75ba7c12e2f45aa75", "ref_doc_id": "d258ce96-c3af-45f2-bb04-7f477a1d8a2b"}, "8aee6c0f-75e9-42fc-9f13-9de06dcfb740": {"doc_hash": "ec27ce95789775b977944c52ec9bd8dfeff9d813e4ac44f4a37199165d6a82a7", "ref_doc_id": "3ecdd703-75ad-4316-91cf-8c559261756d"}, "e9e39198-5f3b-4476-bb38-91eba4407fb4": {"doc_hash": "c7fdb9f4fc9f99741b703c2edef6c5c59c4c258b5f81fb6303a1f6bb54eec2a7", "ref_doc_id": "f512b869-bf03-4263-9f54-b16539c58706"}, "7c159c44-2e7d-4745-b8ff-baa5598ffe1b": {"doc_hash": "5c28af3bfb93feccb26abdbbf181915de0443a9b2e31bb34fc6b60f181aaa372", "ref_doc_id": "d52a3df3-cb27-4a26-bc9a-534ab5fb2ed1"}, "ea078fda-849d-4a36-ba6d-d71dc781a018": {"doc_hash": "38bb733665d51161e43c13f11f1e56b4769a0a8da41b021c1e31014b23ae029d", "ref_doc_id": "16a0db4a-3df9-4260-a93f-dc4022691c53"}, "8d02b82e-5585-4893-b764-292f4333a157": {"doc_hash": "423664a2fd36771698db83074d84136a50e204cf9f7f6922e214f669ccd8db0b", "ref_doc_id": "48a76c2b-1991-409d-9f66-3a43ea332a37"}, "30e07b5d-b825-4220-9ead-f879a148724b": {"doc_hash": "fc173ee2035fb9aaaf6555f3ab160fd29ea52ba9454560dfaf69ac310024a892", "ref_doc_id": "d7db6e3a-3fee-4d15-87c7-da8be83afd01"}, "ccfa0a3f-b14d-4150-a6fc-dad8bb6c193f": {"doc_hash": "a63454531a9c3cc930f5fcbfc987c10d040fec86c6910cf057872e8a0dd3a254", "ref_doc_id": "11dccb50-ea5b-447f-b4ad-c8fbd5501119"}, "44f83b38-7e01-4f99-8364-b8afe8ca798b": {"doc_hash": "c0ece57ab6c13d5de97f5167e838e9e33b52fb1b618a0ea73d0c289929694c05", "ref_doc_id": "54eecb9d-e31c-4c3a-899d-3e1b036f0f60"}}, "docstore/data": {"5e46ff26-85ec-4467-a6a3-0e524961d68b": {"__data__": {"id_": "5e46ff26-85ec-4467-a6a3-0e524961d68b", "embedding": null, "metadata": {"page_label": "Cover", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8047731-f86b-47f5-9208-b627a0d9a405", "node_type": "4", "metadata": {"page_label": "Cover", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "4656ec06520331e6bc967dc9532ac72e1c1a7873c2a6ae627c015bc7ec2ee74c", "class_name": "RelatedNodeInfo"}}, "text": "www.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 19, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f75aa2eb-3138-40ae-b6db-4bf7d2d01d41": {"__data__": {"id_": "f75aa2eb-3138-40ae-b6db-4bf7d2d01d41", "embedding": null, "metadata": {"page_label": "FM1", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf9bfd21-e034-4a71-9fd3-af6a09b58be6", "node_type": "4", "metadata": {"page_label": "FM1", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "24b6c176e23e1f18f547fae5130ac691da04fdd9e8b89ef02c699711be0c4684", "class_name": "RelatedNodeInfo"}}, "text": "Learning Scrapy\nLearn the art of efficient web scraping and crawling  \nwith Python\nDimitrios Kouzis-Loukas\nBIRMINGHAM - MUMBAI\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24033059-a7a5-42fe-8f17-2b4c3643fd32": {"__data__": {"id_": "24033059-a7a5-42fe-8f17-2b4c3643fd32", "embedding": null, "metadata": {"page_label": "FM2", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53750600-526d-4e85-a907-f0c5560deac2", "node_type": "4", "metadata": {"page_label": "FM2", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "15be7c4d0f8268163f43fb06dcc95d3e8d6328591162422ab7e3102ffbdddbc5", "class_name": "RelatedNodeInfo"}}, "text": "Learning Scrapy\nCopyright \u00a9 2016 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval \nsystem, or transmitted in any form or by any means, without the prior written \npermission of the publisher, except in the case of brief quotations embedded in \ncritical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the accuracy \nof the information presented. However, the information contained in this book is \nsold without warranty, either express or implied. Neither the author, nor Packt \nPublishing, and its dealers and distributors will be held liable for any damages \ncaused or alleged to be caused directly or indirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all of the \ncompanies and products mentioned in this book by the appropriate use of capitals. \nHowever, Packt Publishing cannot guarantee the accuracy of this information.\nFirst published: January 2016\nProduction reference: 1220116\nPublished by Packt Publishing Ltd.\nLivery Place\n35 Livery Street\nBirmingham B3 2PB, UK.\nISBN 978-1-78439-978-8\nwww.packtpub.com\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f20664ed-1090-4e57-a60d-58a42562dc62": {"__data__": {"id_": "f20664ed-1090-4e57-a60d-58a42562dc62", "embedding": null, "metadata": {"page_label": "FM3", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "794e12a2-dc7f-490f-9b9c-e6c0c5b713d8", "node_type": "4", "metadata": {"page_label": "FM3", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "53ed719cbfa21670cd8a7a1def4b5a05a31aa32a2c5355090c685c58d4331ff6", "class_name": "RelatedNodeInfo"}}, "text": "Credits\nAuthor\nDimitrios Kouzis-Loukas\nReviewer\nLazar Telebak\nCommissioning Editor\nAkram Hussain\nAcquisition Editor\nSubho Gupta\nContent Development Editor\nKirti Patil\nTechnical Editor\nSiddhesh Ghadi\nCopy Editor\nPriyanka RaviProject Coordinator\nNidhi Joshi\nProofreader\nSafis Editing\nIndexer\nMonica Ajmera Mehta\nGraphics\nDisha Haria\nProduction Coordinator\nNilesh R. Mohite\nCover Work\nNilesh R. Mohite\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33c3ec2a-3d9e-4b5f-99b2-0568da0d823a": {"__data__": {"id_": "33c3ec2a-3d9e-4b5f-99b2-0568da0d823a", "embedding": null, "metadata": {"page_label": "FM4", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcd5cab4-4994-4e98-a677-a4004d4510dd", "node_type": "4", "metadata": {"page_label": "FM4", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c0e26fa4f241a7bf9df43af1675e6a343f7d15f7244ac51c169d22051826698b", "class_name": "RelatedNodeInfo"}}, "text": "About the Author\nDimitrios Kouzis-Loukas  has over fifteen years experience as a topnotch \nsoftware developer. He uses his acquired knowledge and expertise to teach a wide \nrange of audiences how to write great software, as well.\nHe studied and mastered several disciplines, including mathematics, physics, and \nmicroelectronics. His thorough understanding of these subjects helped him raise his \nstandards beyond the scope of \"pragmatic solutions.\" He knows that true solutions \nshould be as certain as the laws of physics, as robust as ECC memories, and as \nuniversal as mathematics.\nDimitrios now develops distributed, low-latency, highly-availability systems using \nthe latest datacenter technologies. He is language agnostic, yet has a slight preference \nfor Python, C++, and Java. A firm believer in open source software and hardware,  \nhe hopes that his contributions will benefit individual communities as well as all  \nof humanity.\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a09093c-28cf-49ed-a09c-078946014068": {"__data__": {"id_": "4a09093c-28cf-49ed-a09c-078946014068", "embedding": null, "metadata": {"page_label": "FM5", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12bb99a6-6534-4f08-8a28-adcc6179abd8", "node_type": "4", "metadata": {"page_label": "FM5", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "2f774c8308b25823e7491d0d3a6cc73c8863f39fe98d501d0c37af4a0c2c0364", "class_name": "RelatedNodeInfo"}}, "text": "About the Reviewer\nLazar Telebak  is a freelance web developer specializing in web scraping, crawling, \nand indexing web pages using Python libraries/frameworks.\nHe has worked mostly on projects that deal with automation and website scraping, \ncrawling, and exporting data to various formats, including CSV, JSON, XML, and \nTXT, and databases such as MongoDB, SQLAlchemy, and Postgres.\nHe also has experience in frontend technologies and the languages: HTML, CSS, JS, \nand jQuery.\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e32c72ec-7ce8-479c-844b-1b127a29ba4e": {"__data__": {"id_": "e32c72ec-7ce8-479c-844b-1b127a29ba4e", "embedding": null, "metadata": {"page_label": "FM6", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "095af734-1710-45f9-b28f-5380e3e8c94b", "node_type": "4", "metadata": {"page_label": "FM6", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e248a387075b53075732ec488a5a0b4732fca3f5d002b1015110ef9d1e6e91fb", "class_name": "RelatedNodeInfo"}}, "text": "www.PacktPub.com\nSupport files, eBooks, discount offers, and more\nFor support files and downloads related to your book, please visit www.PacktPub.com .\nDid you know that Packt offers eBook versions of every book published, with PDF \nand ePub files available? You can upgrade to the eBook version at www.PacktPub.\ncom and as a print book customer, you are entitled to a discount on the eBook copy. \nGet in touch with us at service@packtpub.com  for more details.\nAt www.PacktPub.com , you can also read a collection of free technical articles, sign \nup for a range of free newsletters and receive exclusive discounts and offers on Packt \nbooks and eBooks.\nTM\nhttps://www2.packtpub.com/books/subscription/packtlib\nDo you need instant solutions to your IT questions? PacktLib is Packt's online digital \nbook library. Here, you can search, access, and read Packt's entire library of books.\nWhy subscribe?\n\u2022 Fully searchable across every book published by Packt\n\u2022 Copy and paste, print, and bookmark content\n\u2022 On demand and accessible via a web browser\nFree access for Packt account holders\nIf you have an account with Packt at www.PacktPub.com , you can use this to access \nPacktLib today and view 9 entirely free books. Simply use your login credentials for \nimmediate access.\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b27b47c-9b6a-4583-9d07-75cc369c941f": {"__data__": {"id_": "3b27b47c-9b6a-4583-9d07-75cc369c941f", "embedding": null, "metadata": {"page_label": "i", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "41d73e69-bf24-436f-8a6a-f0d38a33ed24", "node_type": "4", "metadata": {"page_label": "i", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "edd7985b8fbced0e44da583050270469df8531a9851e5b61c8d96b672d7c2b4d", "class_name": "RelatedNodeInfo"}}, "text": "[ i ]Table of Contents\nPreface vii\nChapter 1: Introducing Scrapy 1\nHello Scrapy 1\nMore reasons to love Scrapy 2\nAbout this book: aim and usage 3\nThe importance of mastering automated data scraping 4\nDeveloping robust, quality applications, and providing realistic schedules 5\nDeveloping quality minimum viable products quickly 5\nScraping gives you scale; Google couldn't use forms 6\nDiscovering and integrating into your ecosystem 7\nBeing a good citizen in a world full of spiders 8\nWhat Scrapy is not 8\nSummary 9\nChapter 2: Understanding HTML and XPath 11\nHTML, the DOM tree representation, and the XPath 11\nThe URL  12\nThe HTML document 12\nThe tree representation 14\nWhat you see on the screen 15\nSelecting HTML elements with XPath 16\nUseful XPath expressions 17\nUsing Chrome to get XPath expressions 20\nExamples of common tasks 21\nAnticipating changes 22\nSummary 23\nChapter 3: Basic Crawling 25\nInstalling Scrapy 26\nMacOS 26\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc61189e-6a1c-4553-87cc-ddc0bb2c6be3": {"__data__": {"id_": "fc61189e-6a1c-4553-87cc-ddc0bb2c6be3", "embedding": null, "metadata": {"page_label": "ii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "000bae20-f570-4e2a-82eb-54c7762d1d14", "node_type": "4", "metadata": {"page_label": "ii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "bd4d7b653f84e0ff4c9dd937dc6d6b4d5abe2411a648079e7f72fd89e963305f", "class_name": "RelatedNodeInfo"}}, "text": "Table of Contents[ ii ]Windows 27\nLinux 27\nUbuntu or Debian Linux 28\nRed Hat or CentOS Linux 28\nFrom the latest source 28\nUpgrading Scrapy 29\nVagrant: this book's official way to run examples 29\nUR2IM \u2013 the fundamental scraping process 31\nThe URL  32\nThe request and the response 33\nThe Items 34\nA Scrapy project 40\nDefining items 41\nWriting spiders 42\nPopulating an item 46\nSaving to files 47\nCleaning up \u2013 item loaders and housekeeping fields 49\nCreating contracts 53\nExtracting more URLs 55\nTwo-direction crawling with a spider 58\nTwo-direction crawling with a CrawlSpider 61\nSummary 62\nChapter 4: From Scrapy to a Mobile App 63\nChoosing a mobile application framework 63\nCreating a database and a collection 64\nPopulating the database with Scrapy 66\nCreating a mobile application 69\nCreating a database access service 70\nSetting up the user interface 70\nMapping data to the User Interface 72\nMappings between database fields and User Interface controls 73\nTesting, sharing, and exporting your mobile app 74\nSummary 75\nChapter 5: Quick Spider Recipes 77\nA spider that logs in 78\nA spider that uses JSON APIs and AJAX pages 84\nPassing arguments between responses 87\nA 30-times faster property spider 88\nA spider that crawls based on an Excel file 92\nSummary 96\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13376bb6-36e9-439c-9400-f86a73385eac": {"__data__": {"id_": "13376bb6-36e9-439c-9400-f86a73385eac", "embedding": null, "metadata": {"page_label": "iii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c974810-a925-4419-b2dc-2920e9794f5a", "node_type": "4", "metadata": {"page_label": "iii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "d0937a6519962a01fee9af63ed15c64139649fe7339f1e6cf757dc429c8a2d43", "class_name": "RelatedNodeInfo"}}, "text": "Table of Contents[ iii ]Chapter 6: Deploying to Scrapinghub 97\nSigning up, signing in, and starting a project 98\nDeploying our spiders and scheduling runs 100\nAccessing our items 102\nScheduling recurring crawls 104\nSummary 104\nChapter 7: Configuration and Management 105\nUsing Scrapy settings 106\nEssential settings 107\nAnalysis 107\nLogging 108\nStats 108\nTelnet 108\nPerformance 110\nStopping crawls early 111\nHTTP caching and working offline 111\nExample 2 \u2013 working offline by using the cache 111\nCrawling style 112\nFeeds 113\nDownloading media 114\nOther media 114\nAmazon Web Services 115\nUsing proxies and crawlers  116\nExample 4 \u2013 using proxies and Crawlera's clever proxy 116\nFurther settings 117\nProject-related settings 118\nExtending Scrapy settings 118\nFine-tuning downloading 119\nAutothrottle extension settings 119\nMemory UsageExtension settings 119\nLogging and debugging 120\nSummary 120\nChapter 8: Programming Scrapy 121\nScrapy is a Twisted application 122\nDeferreds and deferred chains 124\nUnderstanding Twisted and nonblocking I/O \u2013 a Python tale 127\nOverview of Scrapy architecture 134\nExample 1 - a very simple pipeline 137\nSignals 138\nExample 2 - an extension that measures throughput and latencies 140\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b662a6f4-883e-45fe-9963-dafab7e45245": {"__data__": {"id_": "b662a6f4-883e-45fe-9963-dafab7e45245", "embedding": null, "metadata": {"page_label": "iv", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d76d575c-27ca-400b-aa40-53acd3d5fa2c", "node_type": "4", "metadata": {"page_label": "iv", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "338d3f10528f06b34255e65271a275307bea7f09d77ff75ceec13983cb7d9459", "class_name": "RelatedNodeInfo"}}, "text": "Table of Contents[ iv ]Extending beyond middlewares 144\nSummary 146\nChapter 9: Pipeline Recipes 147\nUsing REST APIs 148\nUsing treq 148\nA pipeline that writes to Elasticsearch 148\nA pipeline that geocodes using the Google Geocoding API 151\nEnabling geoindexing on Elasticsearch 158\nInterfacing databases with standard Python clients 159\nA pipeline that writes to MySQL  159\nInterfacing services using Twisted-specific clients 163\nA pipeline that reads/writes to Redis 163\nInterfacing CPU-intensive, blocking, or legacy functionality 167\nA pipeline that performs CPU-intensive or blocking operations 167\nA pipeline that uses binaries or scripts 170\nSummary 173\nChapter 10: Understanding Scrapy's Performance 175\nScrapy's engine \u2013 an intuitive approach 176\nCascading queuing systems 177\nIdentifying the bottleneck 178\nScrapy's performance model 179\nGetting component utilization using telnet 180\nOur benchmark system 182\nThe standard performance model 185\nSolving performance problems 187\nCase #1 \u2013 saturated CPU 188\nCase #2 \u2013 blocking code 189\nCase #3 \u2013 \"garbage\" on the downloader 191\nCase #4 \u2013 overflow due to many or large responses 194\nCase #5 \u2013 overflow due to limited/excessive item concurrency 195\nCase #6 \u2013 the downloader doesn't have enough to do 197\nTroubleshooting flow 199\nSummary 200\nChapter 11: Distributed Crawling with Scrapyd and Real-Time \nAnalytics 201\nHow does the title of a property affect the price? 202\nScrapyd 202\nOverview of our distributed system 205\nChanges to our spider and middleware 207\nSharded-index crawling 207", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd92f662-d22f-4c45-937a-48a7eda9ab3c": {"__data__": {"id_": "bd92f662-d22f-4c45-937a-48a7eda9ab3c", "embedding": null, "metadata": {"page_label": "v", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ccc2ca28-a567-4613-bada-74579d4a3f41", "node_type": "4", "metadata": {"page_label": "v", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "925dd393b7458574e39cc48b1c50dd5c54c6bebd8d3bcc4546987311d8b13716", "class_name": "RelatedNodeInfo"}}, "text": "Table of Contents[ v ]Batching crawl URLs 209\nGetting start URLs from settings 214\nDeploy your project to scrapyd servers 216\nCreating our custom monitoring command 217\nCalculating the shift with Apache Spark streaming 218\nRunning a distributed crawl 220\nSystem performance 223\nThe key take-away 223\nSummary 224\nAppendix: Installing and troubleshooting prerequisite software 225\nInstalling prerequisites 225\nThe system 226\nInstallation in a nutshell 228\nInstalling on Linux 228\nInstalling on Windows or Mac 230\nInstall Vagrant 230\nHow to access the terminal 231\nInstall VirtualBox and Git 232\nEnsure that VirtualBox supports 64-bit images 232\nEnable ssh client for Windows 234\nDownload this book's code and set up the system 235\nSystem setup and operations FAQ 235\nWhat do I download and how much time does it take? 236\nWhat should I do if Vagrant freezes? 237\nHow do I shut down/resume the VM quickly? 238\nHow do I fully reset the VM? 238\nHow do I resize the virtual machine? 239\nHow do I resolve any port conflicts? 239\nOn Linux using Docker natively 239\nOn Windows or Mac using a VM 239\nHow do I make it work behind a corporate proxy? 240\nHow do I connect with the Docker provider VM? 240\nHow much CPU/memory does each server use? 241\nHow can I see the size of Docker container images? 241\nHow can I reset the system if Vagrant doesn't respond? 242\nThere's a problem I can't work around, what can I do? 242\nIndex 243", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "899a7c44-ef3a-4270-9a94-df4c7a9b54e0": {"__data__": {"id_": "899a7c44-ef3a-4270-9a94-df4c7a9b54e0", "embedding": null, "metadata": {"page_label": "vi", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3c8ca37-a928-419f-9ec9-af78623d7387", "node_type": "4", "metadata": {"page_label": "vi", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e6e5df5f464c898718fb23fae5b1920bba6e980c26a8e76aa65b6da0d8b060e4", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ed21185-a1b0-4acc-b720-9dec8704115e": {"__data__": {"id_": "0ed21185-a1b0-4acc-b720-9dec8704115e", "embedding": null, "metadata": {"page_label": "vii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91c71f01-86f7-463a-9535-51387506932a", "node_type": "4", "metadata": {"page_label": "vii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f09ee596e588768d52e34574862792c4fb88c54c5a83dd2268f6413b96780fb4", "class_name": "RelatedNodeInfo"}}, "text": "[ vii ]Preface\nLet me take a wild guess. One of these two stories is curiously similar to yours:\nYour first encounter with Scrapy was while searching the net for something along \nthe lines of \"web scraping Python\". You had a quick look at it and thought, \"This is \ntoo complex...I just need something simple.\" You went on and developed a Python \nscript using requests, struggled a bit with beautiful soup, but finally made something \ncool. It was kind of slow, so you let it run overnight. You restarted it a few times, \nignored some semi-broken links and non-English characters, and in the morning, \nmost of the website was proudly on your hard disk. Sadly, for some unknown \nreason, you didn't want to see your code again. The next time you had to scrape \nsomething, you went directly to scrapy.org and this time the documentation made \nperfect sense. Scrapy now felt like it was elegantly and effortlessly solving all of the \nproblems that you faced, and it even took care of problems you hadn't thought of \nyet. You never looked back.\nAlternatively, your first encounter with Scrapy was while doing research for a web-\nscraping project. You needed something robust, fast, and enterprise-grade, so most \nof the fancy one-click web-scraping tools were out of question. You needed it to be \nsimple but at the same time flexible enough to allow you to customize its behavior \nfor different sources, provide different types of output feeds, and reliably run 24/7 \nin an automated manner. Companies that provided scraping as a service seemed too \nexpensive and you were more comfortable using open source solutions than feeling \nlocked on vendors. From the very beginning, Scrapy looked like a clear winner.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "669bc79e-4f2f-4662-9c87-2027f0528f34": {"__data__": {"id_": "669bc79e-4f2f-4662-9c87-2027f0528f34", "embedding": null, "metadata": {"page_label": "viii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28094a3c-1118-4e0e-9f18-db3d88d6a805", "node_type": "4", "metadata": {"page_label": "viii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ac01893648a4bc7d88195dfe6b6acd5b1fa25f4124d33319ae347f3b44b96a59", "class_name": "RelatedNodeInfo"}}, "text": "Preface[ viii ]No matter how you got here, I'm glad to meet you on a book that is entirely devoted \nto Scrapy. Scrapy is the secret of web-scraping experts throughout the world. They \nknow how to maneuver it to save them hours of work, deliver stellar performance, \nand keep their hosting bills to an absolute minimum. If you are less experienced and \nyou want to achieve their results, unfortunately, Google will do you a disservice. \nThe majority of Scrapy information on the Web is either simplistic and inefficient \nor complex. This book is an absolute necessity for everyone who wants accurate, \naccessible, and well-organized information on how to make the most out of Scrapy. It \nis my hope that it will help the Scrapy community grow even further and give it the \nwide adoption that it rightfully deserves.\nWhat this book covers\nChapter 1 , Introducing Scrapy , will introduce you to this book and Scrapy, and will \nallow you to set clear expectations for the framework and the rest of the book.\nChapter 2 , Understanding HTML and XPath , aims to bring web-crawling beginners  \nup to speed with the essential web-related technologies and techniques that we will \nuse thereafter.\nChapter 3 , Basic Crawling , is where we learn how to install Scrapy and crawl a \nwebsite. We develop this example step by step by showing you the methodology and \nthe way of thinking behind every action. After this chapter, you will be able to crawl \nthe majority of simple websites.\nChapter 4 , From Scrapy to a Mobile App , shows us how we can use our scraper to \npopulate a database and feed a mobile application. After this chapter, you will have \na clear appreciation of the benefits that web crawling brings in time to market terms.\nChapter 5 , Quick Spider Recipes , demonstrates more powerful spider features, allowing \nus to log in, scrape faster, consume APIs, and crawl lists of URLs.\nChapter 6 , Deploying to Scrapinghub , shows us how to deploy spiders to Scrapinghub's \ncloud servers and enjoy availability, easy deployment, and control.\nChapter 7 , Configuration and Management , is a well-organized presentation of the \nimpressive number of features that one can enable and fine-tune using Scrapy's \nconfiguration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c03cb067-fb0f-465f-8654-e590f81083c5": {"__data__": {"id_": "c03cb067-fb0f-465f-8654-e590f81083c5", "embedding": null, "metadata": {"page_label": "ix", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13b91b49-bb8d-499f-aa64-8a4fe3899b20", "node_type": "4", "metadata": {"page_label": "ix", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a99f676a59d43faea3a03f7f23b379a648edd303b7605fffd1db8a46728d5f52", "class_name": "RelatedNodeInfo"}}, "text": "Preface[ ix ]Chapter 8 , Programming Scrapy , takes our knowledge to a whole new level by showing \nus how to use the underlying Twisted engine and Scrapy's architecture to extend \nevery aspect of its functionality.\nChapter 9 , Pipeline Recipes , presents numerous examples where we alter Scrapy's \nfunctionality to insert into databases such as MySQL, Elasticsearch, and Redis, \ninterface APIs, and legacy applications with virtually no degradation of performance.\nChapter 10 , Understanding Scrapy's Performance , will help us understand how Scrapy \nspends its time, and what exactly we need to do to increase its performance.\nChapter 11 , Distributed Crawling with Scrapyd and Real-Time Analytics , is our final \nchapter showing how to use scrapyd in multiple servers to achieve horizontal \nscalability, and how to feed crawled data to an Apache Spark server that performs \nstream analytics on it.\nWhat you need for this book\nLots of effort was put into making this book's code and content available for as \nwide an audience as possible. We want to provide interesting examples that involve \nmultiple servers and databases, but we don't want you to have to know how to set \nall these up. We use a great technology called Vagrant to automatically download \nand set up a disposable multiserver environment inside your computer. Our Vagrant \nconfiguration uses a virtual machine on Mac OS X and Windows, and it can run \nnatively on Linux.\nFor Windows and OS X, you will need a 64-bit computer that supports either Intel \nor AMD virtualization technologies: VT-x or AMD-v. Most modern computers will \ndo fine. You will also need 1 GB of memory that is dedicated to the Virtual Machine \nfor most chapters with the exception of Chapter 9 , Pipeline Recipes , and Chapter 11 , \nDistributed Crawling with Scrapyd and Real-Time Analytics , which require 2 GB. Appendix \nA, Installing Prerequisites , has all the details of how to install the necessary software.\nScrapy itself has way more limited hardware and software requirements. If you \nare an experienced user and you don't want to use Vagrant, you will be able to \nset Scrapy up on any operating system even if it has limited memory using the \ninstructions that we provide in Chapter 3 , Basic Crawling .\nAfter you successfully set up your Vagrant environment, you will be able to run \nexamples from the entire book (with the obvious exceptions of Chapter 4 , From Scrapy \nto a Mobile App , and Chapter 6 , Deploying to Scrapinghub ) without the need for an \nInternet connection. Yes, you can enjoy this book on a flight.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "123d63f5-ae64-414b-96ae-c7a585830a4c": {"__data__": {"id_": "123d63f5-ae64-414b-96ae-c7a585830a4c", "embedding": null, "metadata": {"page_label": "x", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "184a637a-3adb-4e11-9f39-d4e7b8e927cd", "node_type": "4", "metadata": {"page_label": "x", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "8ee04fa7bbdf119295b825bb5c18a894a98aec9abf58976a42d8604e4e121c75", "class_name": "RelatedNodeInfo"}}, "text": "Preface[ x ]Who this book is for\nThis book tries to accommodate quite a wide audience. It should be useful to:\n\u2022 Web entrepreneurs who need source data to power their applications\n\u2022 Data scientists and Machine Learning practitioners who need to extract data \nfor analysis or to train their models\n\u2022 Software engineers who need to develop large-scale web-scraping \ninfrastructure\n\u2022 Hobbyists who want to run Scrapy on a Raspberry Pi for their next cool project\nIn terms of prerequisite knowledge, we tried to require a very small amount of \nit. This book presents the basics of web technologies and scraping in the earliest \nchapters for those who have very little web-scraping experience. Python is easily \nreadable and most of what we present in the spider chapters should be fine for \nanyone with basic experience of any programming language.\nFrankly, I strongly believe that if someone has a project in mind and wants to use \nScrapy, they will be able to hack the examples of this book and have something up and \nrunning within hours even with no previous scraping, Scrapy, or Python experience.\nAfter the first half of the book, we become more Python-heavy, and at this point, \nbeginners may want to allow themselves a few weeks of basic Scrapy experience \nbefore they delve deeper. At this point, more experienced Python/Scrapy developers \nwill enjoy learning event-driven Python development using Twisted and the very \ninteresting Scrapy internals. For the performance chapter, some mathematics intuition \nmay be beneficial, but even without it, most diagrams should make a clear impression.\nConventions\nIn this book, you will find a number of text styles that distinguish between different \nkinds of information. Here are some examples of these styles and an explanation of \ntheir meaning.\nCode words in text, database table names, folder names, filenames, file extensions, \npathnames, dummy URLs, user input, and Twitter handles are shown as follows: \"The \n<head>  part is important to indicate meta-information such as character encoding.\"", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10eb6275-7cbb-45d2-996f-f575626a9e86": {"__data__": {"id_": "10eb6275-7cbb-45d2-996f-f575626a9e86", "embedding": null, "metadata": {"page_label": "xi", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f27466b4-bc1b-45ce-95d0-10da581e0082", "node_type": "4", "metadata": {"page_label": "xi", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "8c19efce3c71e513a24c3029ae14987547cf89b3633afd9209af83c31f7a815f", "class_name": "RelatedNodeInfo"}}, "text": "Preface[ xi ]A block of command line is set as follows:\n$ python\n>>> from twisted.internet import defer\n>>> # Experiment 1\n>>> d = defer.Deferred()\n>>> d.called\nFalse\n>>> d.callback(3)\n>>> d.called\nTrue\n>>> d.result\n3\nNew terms  and important words  are shown in bold. Words that you see on the \nscreen, for example, in menus or dialog boxes, appear in the text like this: \"Clicking \nthe Next  button moves you to the next screen.\"\nWarnings or important notes appear in a box like this.\nTips and tricks appear like this.\nReader feedback\nFeedback from our readers is always welcome. Let us know what you think about \nthis book\u2014what you liked or disliked. Reader feedback is important for us as it helps \nus develop titles that you will really get the most out of.\nTo send us general feedback, simply e-mail feedback@packtpub.com , and mention \nthe book's title in the subject of your message.\nIf there is a topic that you have expertise in and you are interested in either writing \nor contributing to a book, see our author guide at www.packtpub.com/authors .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fc7bb18-d46a-4f34-94e6-1267be9924f6": {"__data__": {"id_": "4fc7bb18-d46a-4f34-94e6-1267be9924f6", "embedding": null, "metadata": {"page_label": "xii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4b0ddfa-a7c2-47c1-ab9f-799a5cef3b70", "node_type": "4", "metadata": {"page_label": "xii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "82583219d29209125fc4f2ccf3a6576fc2214c2e03ca2e23d9a73888132ea09c", "class_name": "RelatedNodeInfo"}}, "text": "Preface[ xii ]Customer support\nNow that you are the proud owner of a Packt book, we have a number of things to \nhelp you to get the most from your purchase.\nDownloading the example code\nYou can download the example code files from your account at http://www.\npacktpub.com  for all the Packt Publishing books you have purchased. If you \npurchased this book elsewhere, you can visit http://www.packtpub.com/support  \nand register to have the files e-mailed directly to you.\nErrata\nAlthough we have taken every care to ensure the accuracy of our content, mistakes \ndo happen. If you find a mistake in one of our books\u2014maybe a mistake in the text or \nthe code\u2014we would be grateful if you could report this to us. By doing so, you can \nsave other readers from frustration and help us improve subsequent versions of this \nbook. If you find any errata, please report them by visiting http://www.packtpub.\ncom/submit-errata , selecting your book, clicking on the Errata Submission Form  \nlink, and entering the details of your errata. Once your errata are verified, your \nsubmission will be accepted and the errata will be uploaded to our website or added \nto any list of existing errata under the Errata section of that title.\nTo view the previously submitted errata, go to https://www.packtpub.com/books/\ncontent/support  and enter the name of the book in the search field. The required \ninformation will appear under the Errata  section.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c1e8140-0fce-43e4-b745-4876ac87ef31": {"__data__": {"id_": "4c1e8140-0fce-43e4-b745-4876ac87ef31", "embedding": null, "metadata": {"page_label": "xiii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d237f8a-d803-4e5c-a620-dd5c08937c88", "node_type": "4", "metadata": {"page_label": "xiii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "3f2cfbf8f844fc8b0c2cee84e54d5c52b984c8ddea3846abce0974be417e89b7", "class_name": "RelatedNodeInfo"}}, "text": "Preface[ xiii ]Piracy\nPiracy of copyrighted material on the Internet is an ongoing problem across all \nmedia. At Packt, we take the protection of our copyright and licenses very seriously. \nIf you come across any illegal copies of our works in any form on the Internet, please \nprovide us with the location address or website name immediately so that we can \npursue a remedy.\nPlease contact us at copyright@packtpub.com  with a link to the suspected pirated \nmaterial.\nWe appreciate your help in protecting our authors and our ability to bring you \nvaluable content.\nQuestions\nIf you have a problem with any aspect of this book, you can contact us at \nquestions@packtpub.com , and we will do our best to address the problem.\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b6861e3-a7c7-480e-948a-382e420bd6c2": {"__data__": {"id_": "4b6861e3-a7c7-480e-948a-382e420bd6c2", "embedding": null, "metadata": {"page_label": "xiv", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22e29a89-0ea5-435c-9231-ae85f0aa253f", "node_type": "4", "metadata": {"page_label": "xiv", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f59854ea5341b483010775620280fe2eec4d8aa7376781e822899328e58271ce", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b3d399e-3ef8-4a13-abf3-2dad1415d57c": {"__data__": {"id_": "4b3d399e-3ef8-4a13-abf3-2dad1415d57c", "embedding": null, "metadata": {"page_label": "1", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "369dfcf4-8fee-48ce-bab0-b705bacd1113", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "473e2c429d18d596c7e58481b26efcb4834aebece97a0af321501c734d49665d", "class_name": "RelatedNodeInfo"}}, "text": "[ 1 ]Introducing Scrapy\nWelcome to your Scrapy journey. With this book, we aim to take you from a Scrapy \nbeginner\u2014someone who has little or no experience with Scrapy\u2014to a level where \nyou will be able to confidently use this powerful framework to scrape large datasets \nfrom the web or other sources. In this chapter, we will introduce you to Scrapy and \ntalk to you about some of the great things you can achieve with it.\nHello Scrapy\nScrapy is a  robust web framework for scraping data from various sources. As a \ncasual web user, you will often find yourself wishing to be able to get data from a \nwebsite that you're browsing on a spreadsheet program like Excel (see Chapter 3 , \nBasic Crawling ) in order to access it while you're offline or to perform calculations. As \na developer, you'll often wish to be able to combine data from various data sources, \nbut you are well aware of the complexities of retrieving or extracting them. Scrapy \ncan help you complete both easy and complex data extraction initiatives.\nScrapy is built upon years of experience in extracting massive amounts of data in a \nrobust and efficient manner. With Scrapy, you are able to do with a single setting \nwhat would take various classes, plug-ins, and configuration in most other scraping \nframeworks. A quick look at Chapter 7 , Configuration and Management  will make you \nappreciate how much you can achieve in Scrapy with a few lines of configuration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "939e9f09-e46c-4bf6-aff2-b0b657232eca": {"__data__": {"id_": "939e9f09-e46c-4bf6-aff2-b0b657232eca", "embedding": null, "metadata": {"page_label": "2", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b4a884d-dff3-4500-8381-4bf04c2ebcf9", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "75c0c0dd830084d78716be9297ebfdfdba1ea48972137b206af108342cfe0344", "class_name": "RelatedNodeInfo"}}, "text": "Introducing Scrapy[ 2 ]From a developer's perspective, you will also appreciate Scrapy's event-based \narchitecture (we will explore it in depth in Chapter 8 , Programming Scrapy  and  \nChapter 9 , Pipeline Recipes ). It allows us to cascade operations that clean, form, and \nenrich data, store them in databases, and so on, while enjoying very low degradation \nin performance\u2014if we do it in the right way, of course. In this book, you will learn \nexactly how to do so. Technically speaking, being event-based, Scrapy allows us to \ndisconnect latency from throughput by operating smoothly while having thousands \nof connections open. As an extreme example, imagine that you aim to extract listings \nfrom a website that has summary pages with a hundred listings per page. Scrapy \nwill effortlessly perform 16 requests on that site in parallel, and assuming that, on \nan average, a request takes a second to complete, you will be crawling at 16 pages \nper second. If you multiply that with the number of listings per page, you will be \ngenerating 1600 listings per second. Imagine now that for each of those listings \nyou have to do a write to a massively concurrent cloud storage, which takes 3 \nseconds (very bad idea) on an average. In order to support the throughput of 16 \nrequests per second, it turns out that we need to be running 1600 \u2219 3 = 4800 write \nrequests in parallel (you will see many such interesting calculations in Chapter 9 , \nPipeline Recipes ). For a traditional multithreaded application, this would translate \nto 4800 threads, which would be a very unpleasant experience for both you and the \noperating system. In Scrapy's world, 4800 concurrent requests is business as usual as \nlong as the operating system is okay with it. Furthermore, memory requirements of \nScrapy closely follow the amount of data that you need for your listings in contrast \nto a multithreaded application, where each thread adds a significant overhead as \ncompared to a listing's size.\nIn a nutshell, slow or unpredictable websites, databases, or remote APIs won't \nhave devastating consequences on your scraper's performance, since you can run \nmany requests concurrently, and manage everything from a single thread. This \ntranslates to lower hosting bills, opportunity for co-hosting scrapers with other \napplications, and simpler code (no synchronization necessary) as compared to \ntypical multithreaded applications.\nMore reasons to love Scrapy\nScrapy has been around for more than half a decade, and is mature and stable. \nBeyond the performance benefits  that we mentioned in the previous section, there \nare several other reasons to love Scrapy:\n\u2022 Scrapy understands broken HTML\nYou can use Beautiful Soup or lxml directly from Scrapy, but Scrapy provides \nselectors \u2014a higher level XPath (mainly) interface on top of lxml. It is able to \nefficiently handle broken HTML code and confusing encodings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ee25271-4507-4be4-97f0-fbe08eb57183": {"__data__": {"id_": "5ee25271-4507-4be4-97f0-fbe08eb57183", "embedding": null, "metadata": {"page_label": "3", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e878a92b-77bd-4f46-bd89-27e41eaf3d37", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "63f1ae6e44e80380b13b5d8fe3d33eab56dea101dae4131c6d3b312a1bf0ea61", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 1[ 3 ]\u2022 Community\nScrapy has  a vibrant community. Just have a look at the mailing list at \nhttps://groups.google.com/forum/#!forum/scrapy-users  and the \nthousands of questions in Stack Overflow at http://stackoverflow.\ncom/questions/tagged/scrapy . Most questions get answered within \nminutes. More community resources are  available at http://scrapy.org/\ncommunity/ .\n\u2022 Well-organized code that is maintained by the community\nScrapy requires a standard way of organizing your code. You write little \nPython modules called spiders and pipelines, and you automatically gain \nfrom any future improvements to the engine itself. If you search online, you \nwill find quite a few professionals who have Scrapy experience. This means \nthat it's quite easy to find a contractor who will help you maintain or extend \nyour code. Whoever joins your team won't have to go through the learning \ncurve of understanding the peculiarities of your own custom crawler.\n\u2022 Growing feature set but also quality focused\nIf you have a quick look at the Release Notes ( http://doc.scrapy.org/en/\nlatest/news.html ), you will notice that there is a growth, both in features \nand in stability/bug fixes.\nAbout this book: aim and usage\nWith this book, we aim to teach you Scrapy by using focused examples and realistic \ndatasets. Most chapters focus on crawling an example property rental website. We \nchose this, because it's representative of most of the web crawling projects, allows us \nto present interesting variations, and is at the same time simple. Having this example \nas the main theme helps us focus on Scrapy without distraction.\nWe start by running small crawls of a few hundred pages, and we scale it out to \nperforming distributed crawling of fifty thousand pages within minutes in  \nChapter 11 , Distributed Crawling with Scrapyd and Real-Time Analytics . In the process, \nwe will show you how to connect Scrapy with services like MySQL, Redis, and \nElasticsearch, use the Google geocoding API to find coordinates for the location of \nour example properties, and feed Apache Spark to predict the keywords which affect \nproperty prices the most.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6846dc8-31c2-4aa6-96d0-e441d5830749": {"__data__": {"id_": "a6846dc8-31c2-4aa6-96d0-e441d5830749", "embedding": null, "metadata": {"page_label": "4", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddc4ec1e-4c5e-4303-8ce5-d01893570a3a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "1b31088e9a680037ca07891f940c17b96ec5b493fbe7266b8d654ac16d066041", "class_name": "RelatedNodeInfo"}}, "text": "Introducing Scrapy[ 4 ]Be prepared to read this book several times. Maybe you can start by skimming \nthrough it to understand its structure. Then read a chapter or two, learn, experiment \nfor a while, and then move further. Don't be afraid to skip a chapter if you feel \nfamiliar with it. In particular, if you know HTML and XPath, there's no point \nspending much time on Chapter 2 , Understanding HTML and XPath . Don't worry; \nthis book still has plenty for you. Some chapters like Chapter 8 , Programming \nScrapy  combine the elements of a reference and a tutorial, and go in depth into \nprogramming concepts. That's an example of a chapter one might like to read a few \ntimes, while allowing a couple of weeks of Scrapy practice in between. You don't \nneed to perfectly master Chapter 8 , Programming Scrapy  before moving, for example, \nto Chapter 9 , Pipeline Recipes , which is full of applications. Reading the latter will help \nyou understand how to use the programming concepts, and if you wish, you can \nreiterate as many times as you like.\nWe have tried to balance the pace to keep the book both interesting and beginner-\nfriendly. One thing we can't do though, is teach Python in this book. There are \nseveral excellent books on the subject, but what I would recommend is trying a bit \nmore relaxed attitude while learning. One of the reasons Python is so popular is \nthat it's relatively simple, clean, and it reads well as English. Scrapy is a high-level \nframework that requires learning from Python beginners and experts alike. You \ncould call it \"the Scrapy language\". As a result, I would recommend going through \nthe material, and if you feel that you find the Python syntax confusing, supplement \nyour learning with some of the excellent online Python tutorials or free Python \nonline courses for beginners at Coursera or elsewhere. Rest assured, you can be  \nquite a good Scrapy developer without being a Python expert.\nThe importance of mastering automated \ndata scraping\nFor many of us, the curiosity and the mental satisfaction in mastering a cool \ntechnology like Scrapy is  sufficient to motivate us. As a pleasant surprise, while \nlearning this great framework, we enjoy a few benefits that derive from starting the \ndevelopment process from data and the community instead of the code.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a863931e-78a9-4d07-aa8c-23a063c59631": {"__data__": {"id_": "a863931e-78a9-4d07-aa8c-23a063c59631", "embedding": null, "metadata": {"page_label": "5", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84c1476f-04c0-4513-ac00-8d57c2608ec4", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e2bb79a930a57b31d00088778fdc478a22d674f5cf3d3e3d9e78d68b13cc518d", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 1[ 5 ]Developing robust, quality applications, and \nproviding realistic schedules\nIn order to develop modern high-quality applications, we need realistic, large datasets, \nif possible, before even writing a single line of code. Modern software development \nis all about processing large amounts of less-than-perfect data in real time to extract \nknowledge and actionable insights. When we develop software and apply it to large \ndatasets, small errors and oversights are difficult to detect and might lead us to costly \nerroneous decisions. It's easy, for example, to overlook entire states while trying to \nstudy demographics, just because of a bug that silently drops data when the state name \nis too long. By carefully scraping, and having production-quality, large, real-world \ndatasets during development (or even earlier) during design exploration, one can find \nand fix bugs, and make informed engineering decisions.\nAs another example, imagine that you want to design an Amazon-style \"if you like \nthis, you might also like that\"-style recommendation system. If you are able to crawl \nand collect a real-world dataset before you even start, you will quickly become aware \nof the issues related to invalid entries, discontinued products, duplicates, invalid \ncharacters, and performance issues due to skewed distributions. Data will force you \nto design algorithms robust  enough to handle the products bought by thousands of \npeople as well as new entries with zero sales. Compare that to software developed \nin isolation that will later, potentially after weeks of development, face the ugliness \nof real-world data. The two approaches might eventually converge, but the ability \nto provide schedule estimates you can commit to, and the quality of software as the \nproject's time progresses will be significantly different. Starting from data, leads to a \nmuch more pleasant and predictable software development experience.\nDeveloping quality minimum viable products \nquickly\nLarge realistic datasets are even more essential for start-ups. You might have \nheard of the \"Lean Startup\", a term coined by Eric Ries  to describe the business \ndevelopment process under conditions  of extreme uncertainty like tech-start-ups. \nOne of the key concepts of that framework is that of the minimum viable product  \n(MVP )\u2014a product with limited functionality that  one can quickly develop and \nrelease to a limited audience in order to measure reactions and validate business \nhypotheses. Based on the reactions, a start-up might choose to continue with further \ninvestments, or \"pivot\" to something more promising.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc464a98-8539-444e-a423-069f0226e54a": {"__data__": {"id_": "fc464a98-8539-444e-a423-069f0226e54a", "embedding": null, "metadata": {"page_label": "6", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4d848c0-2093-4308-aa3e-e837896edf23", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "5c13051ad503c379115d7b92fcf4775e56585d9fe5b76ce06f27976eacbeb67c", "class_name": "RelatedNodeInfo"}}, "text": "Introducing Scrapy[ 6 ]Some aspects of this process that are easy to overlook are very closely connected with \nthe data problems that Scrapy solves for us. When we ask potential customers to try \nour mobile app, for example, we as developers or entrepreneurs ask them to judge \nthe functionality imagining how this app will look when completed. This might \nbe a bit too much imagining for a non-expert. The distance between an app which \nshows \"product 1\", \"product 2\", and \"user 433\", and an application that provides \ninformation on \"Samsung UN55J6200 55-Inch TV\", which has a five star rating from \nuser \"Richard S.\" and working links that take you directly to a product detail page \n(despite the fact we didn't write it), is significant. It's very difficult for people to \njudge the functionality of an MVP objectively, unless the data that we use is realistic \nand somewhat exciting.\nOne of the reasons that some start-ups have data as an afterthought is the perception \nthat collecting them is expensive. Indeed, we would typically need to develop forms, \nadministration screens, and spend time entering data\u2014 or we could just use Scrapy \nand crawl a few websites before writing even a single line of code. You will see in \nChapter 4 , From Scrapy to a Mobile App , how easy it is to develop a simple mobile app \nas soon as you have data.\nScraping gives you scale; Google couldn't \nuse forms\nWhile on the subject of forms, let's consider how they affect the growth of a product. \nImagine for a second Google founders creating the first version of their engine \nincorporating a form that every webmaster has to fill, and copy-paste the text for \nevery page on their website. They should then accept the license agreement to \nallow Google to process, store, and present their content while pocketing most of \nthe advertising profits. Can you imagine the incredible amount of time and effort \nrequired to explain the vision and convince people to get involved in this process? \nEven if the market was starving for an excellent search engine (as it proved to be the \ncase), this engine wouldn't be Google because its growth would be extremely slow. \nEven the most sophisticated algorithms wouldn't be able to offset the lack of data. \nGoogle uses web crawlers that move through links from page to page, filling their \nmassive databases. Webmasters don't have to do anything at all. Actually, it requires \na bit of effort to prevent Google from indexing your pages.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4c8bb9e-30e0-40b2-8a7b-6e7747b77435": {"__data__": {"id_": "f4c8bb9e-30e0-40b2-8a7b-6e7747b77435", "embedding": null, "metadata": {"page_label": "7", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d1e51dd-15f4-4ead-a918-a966a805368f", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "13a6210e57c00ebfc9e8db93bff90012a866c86e358048051611207b4aa63f01", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 1[ 7 ]The idea of Google using forms might sound a bit ridiculous, but how many forms \ndoes a typical website require a user to fill? A login form, a new listing form, a \ncheckout form, and so on. How much do those forms really cost by hindering \napplication's growth? If you know your audience/customers enough, it is highly \nlikely that you have a clue on the other websites they are typically using, and might \nalready have an account with. For example, a developer will likely have a Stack \nOverflow and a GitHub account. Could you\u2014with their permission\u2014scrape those \nsites as soon as they give you their username, and auto-fill their photos, their bio, \nand a few recent posts? Can you perform some quick text analytics on the posts \nthey are mostly interested in, and use it to adapt your site's navigation structure \nand suggested products or services? I hope you can see how replacing forms with \nautomated data scraping can allow you to better serve your audience, and grow at \nweb-scale.\nDiscovering and integrating into your \necosystem\nScraping data naturally leads you to discover  and consider your relationship \nwith the communities related to your endeavors. When you scrape a data source, \nnaturally some questions arise: Do I trust their data? Do I trust the companies who \nI get data from? Should I talk to them to have a more formal cooperation? Am I \ncompeting or cooperating with them? How much would it cost me to get these \ndata from another source? Those business risks are there anyway, but the scraping \nprocess helps us become aware of them earlier, and develop mitigation strategies.\nYou will also find yourself wondering what do you give back to those websites or \ncommunities? If you give them free traffic, they will likely be happy. On the other \nhand, if your application doesn't provide some value to your source, maybe your \nrelationship is a bit ephemeral unless you talk to them and find a way to cooperate. \nBy getting data from various sources, you are primed to develop products friendlier \nto the existing ecosystem that respect established market players, disrupting only \nwhen it's worth the effort. Established players might also help you grow faster\u2014for \nexample, if you have an application that uses data feeds from two or three distinct \necosystems of a hundred thousand users each, your service might end up connecting \nthree hundred thousand users in a creative way which benefits everybody. For \nexample, if you create a start-up that combines a rock music and a t-shirt printing \ncommunity, you end up with a mixture of two ecosystems, and both you and the \ncommunities will likely benefit and grow.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62730dcb-d8be-4c10-8041-db51c4aee266": {"__data__": {"id_": "62730dcb-d8be-4c10-8041-db51c4aee266", "embedding": null, "metadata": {"page_label": "8", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a543f68-cc1d-4963-ba9f-b1690bfe8d93", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "55f509757441ddc0e4d3a6600e428a44cdd053344092f00189b3492545c47b49", "class_name": "RelatedNodeInfo"}}, "text": "Introducing Scrapy[ 8 ]Being a good citizen in a world full of \nspiders\nThere are a few things one needs to be aware of while developing scrapers. \nIrresponsible web scraping can be annoying and even illegal in some cases. The two \nmost important things to avoid are denial-of-service  (DoS ) attack like behavior and \nviolating copyrights.\nIn the first one, a typical visitor might be visiting a new page every few seconds. \nA typical web crawler might be downloading tens of pages per second. That is \nmore than ten times the traffic that a typical user generates. This might reasonably \nmake the website owners upset. Use throttling to reduce the traffic you generate \nto an acceptable user-like level. Monitor the response times, and if you see them \nincreasing, reduce the intensity of your crawl. The good news is that Scrapy  \nprovides out-of-the-box implementation of both these functionalities  \n(see Chapter 7 , Configuration and Management ).\nOn copyrights, obviously, take a look at the copyright notice of every website you \nscrape, and make sure you understand what is allowed and what is not. Most sites \nallow you to process information from their site as long as you don't reproduce them \nclaiming that it's yours. What is nice to have is a User-Agent  field on your requests \nthat allows webmasters to know who you are and what you do with their data. \nScrapy does this by default by using your BOT_NAME  as a User-Agent  when making \nrequests. If this is a URL or a name that clearly points to your application, then the \nwebmaster can visit your site, and learn more about how you use their data. Another \nimportant aspect is allowing any webmaster to prevent you from accessing certain \nareas of their website. Scrapy provides functionality ( RobotsTxtMiddleware ) that \nrespects their preferences as expressed on the web-standard robots.txt  file (see an \nexample of that file at http://www.google.com/robots.txt ). Finally, it's good to \nprovide the means for webmasters to express their desire to be excluded from your \ncrawls. At the very least, it must be easy for them to find a way to communicate with \nyou and express any concerns.\nLaws differ  from country to country, and I'm by no means in a position to give legal \nadvice. Please seek professional legal advice if you feel the need before relying too \nheavily on scraping for your projects. This applies to the entire content of this book.\nWhat Scrapy is not\nFinally, it's easy to misunderstand what Scrapy can do for you mainly because the \nterms Data Scraping  and all the related terminology is somewhat fuzzy, and many \nterms are used interchangeably. I will try to clarify some of these areas to prevent \nconfusion and save you some time.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05c682fb-ae90-4680-ac45-f5cc0e71a900": {"__data__": {"id_": "05c682fb-ae90-4680-ac45-f5cc0e71a900", "embedding": null, "metadata": {"page_label": "9", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "651ec303-63ef-4d10-a912-8b4f3f215f6a", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "8eff74395fb76e29434bab4c8a7cd0220ea3269844f205a5d60eb7077d5336c3", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 1[ 9 ]Scrapy is not Apache Nutch, that is, it's not a generic web crawler. If Scrapy visits a \nwebsite it knows nothing about, it won't be able to make anything meaningful out of \nit. Scrapy is about extracting structured information, and requires manual effort to \nset up the appropriate XPath or CSS expressions. Apache Nutch will take a generic \npage and extract information, such as keywords, from it. It might be more suitable \nfor some applications and less for others.\nScrapy is not Apache Solr, Elasticsearch, or Lucene; in other words, it has nothing \nto do with a search engine. Scrapy is not intended to give you references to the \ndocuments that contain the word \"Einstein\" or anything else. You can use the data \nextracted by Scrapy, and insert them into Solr or Elasticsearch as we do at the \nbeginning of Chapter 9 , Pipeline Recipes , but that's just a way of using Scrapy, and not \nsomething embedded into Scrapy.\nFinally, Scrapy is not a database like MySQL, MongoDB, or Redis. It neither stores \nnor indexes data. It only extracts data. That said, you will likely insert the data that \nScrapy extracts to a database, and there is support for many of them, which will \nmake your life easier. Scrapy isn't a database though, and its outputs could easily  \nbe just files on a disk or even no output at all\u2014although I'm not sure how this  \ncould be useful.\nSummary\nIn this chapter, we introduced you to Scrapy, gave you an overview of what it can \nhelp you with, and described what we believe is the best way to use this book. We \nalso presented several ways with which automated data scraping can benefit you \nby helping you quickly develop high-quality applications that integrate nicely with \nexisting ecosystems. In the following chapter, we will introduce you to HTML and \nXPath, two very important web languages that we will use in every Scrapy project.\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6619335f-d4f2-4bcf-807d-0fb231faffa5": {"__data__": {"id_": "6619335f-d4f2-4bcf-807d-0fb231faffa5", "embedding": null, "metadata": {"page_label": "10", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "531ba80c-edf1-41af-9db0-c50e5d1cb618", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "455619a151da20160593940f140c918ef1026014e345b6eb46d21e32d854d5ea", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5a31ceb-ec2e-4978-ae8c-60bb13b108cd": {"__data__": {"id_": "a5a31ceb-ec2e-4978-ae8c-60bb13b108cd", "embedding": null, "metadata": {"page_label": "11", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64000ff4-a3c7-48e5-912b-18ac77dcbfc4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "d5efca1d43e0311a45bbcb94187a0e7d5b0ed2194c22f730adce291937d1c53b", "class_name": "RelatedNodeInfo"}}, "text": "[ 11 ]Understanding HTML and \nXPath\nIn order to extract information from web pages, you have to understand a little \nbit more about their structure. We will have a quick look at HTML, the tree \nrepresentation of HTML, and XPath as a way of selecting information on web pages.\nHTML, the DOM tree representation,  \nand the XPath\nLet's spend  some time understanding the process that takes place from when a \nuser types a URL on the browser (or more often, when he/she clicks on a link or a \nbookmark) until a page is displayed on the screen. From  the perspective of this book, \nthis process has four steps:\n\u2022 A URL is typed on the browser. The first part of the URL (the domain name, \nsuch as gumtree.com ) is used to find the appropriate server on the web, and \nthe URL along with other data like cookies form a request which is sent to \nthat server.\n\u2022 The server replies by sending an HTML page to the browser. Note that the \nserver might also return other formats, such as XML or JSON, but for now we \nfocus on HTML.\n\u2022 The HTML gets translated to an internal tree representation inside the \nbrowser: the  infamous Document Object Model  (DOM ).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d463acf-35ad-499e-bf22-9d29437813f2": {"__data__": {"id_": "2d463acf-35ad-499e-bf22-9d29437813f2", "embedding": null, "metadata": {"page_label": "12", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0731307d-2717-4ad4-887a-d4eb1a598ddb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b8b6d1f325366e91e9c289d6e43a7fe2a8816d38cd2878a357e3685fd09b1538", "class_name": "RelatedNodeInfo"}}, "text": "Understanding HTML and XPath[ 12 ]\u2022 The internal representation is rendered, based on some layout rules, to the \nvisual representation that you see on the screen.\nLet's have a look at those steps and the  representations of the documents that they \nrequire. This will help you in locating the text that you want to scrape and in writing \nprograms that retrieve it.\nThe URL\nFor our purposes, the URL has two main parts. The first part helps us locate the \nappropriate server on the net via the Domain Name System  (DNS ). For example, \nwhen you send https://mail.google.com/mail/u/0/#inbox  to the browser, \nit creates a DNS request on mail.google.com , which resolves the IP address of a \nsuitable server such as 173.194.71.83 . Essentially, https://mail.google.com/\nmail/u/0/#inbox  translates to https://173.194.71.83/mail/u/0/#inbox .\nThe rest of the URL is important for the server to understand what the request is all \nabout. It might be an image, a document, or something that needs to trigger an action \nlike sending an e-mail on that server.\nThe HTML document\nThe server reads the URL, understands what we are asking for, and replies with \nan HTML document. This document is essentially a text file that we can open with \nTextMate, Notepad, vi, or Emacs. Unlike most text documents, an HTML document \nhas a format specified by the World Wide Web Consortium. The specification is \ncertainly beyond the scope of this book, but let's have a look at a simple HTML page. \nIf you head to http://example.com , you can see the associated HTML file in your \nbrowser by choosing View Page Source . The exact process is different on different \nbrowsers; on many systems, it's an option you can get by right clicking, and most \nbrowsers show the source if you press Ctrl + U, or Cmd  + U on a Mac.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfe56a87-3d01-4fa3-8d63-0ed914975c5c": {"__data__": {"id_": "dfe56a87-3d01-4fa3-8d63-0ed914975c5c", "embedding": null, "metadata": {"page_label": "13", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ddffa52-f628-44c6-9fec-9df76e7c6dec", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "4ae680798bf67a0091569d374859feceb34b6ba6995de4a38589389a951edbe4", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 2[ 13 ]In some pages, this feature might be disabled. and you will have to \nclick on the Chrome menu and then Tools  | View Source .\nThe following is currently the HTML code of http://example.com/ :\n<!doctype html>\n<html>\n  <head>\n      <title>Example Domain</title>\n      <meta charset=\"utf-8\" />\n      <meta http-equiv=\"Content-type\"\n              content=\"text/html; charset=utf-8\" />\n      <meta name=\"viewport\" content=\"width=device-width,\n              initial-scale=1\" />\n      <style type=\"text/css\"> body { background-color: ... \n              } }</style>\n  <body>\n      <div>\n              <h1>Example Domain</h1>\n              <p>This domain is established to be used for\n                 illustrative examples examples in documents.\n                 You may use this domain in examples without\n                 prior coordination or asking for permission.</p>\n              <p><a href=\"http://www.iana.org/domains/example\">\n                 More information...</a></p>\n      </div>\n  </body>\n</html>\nI formatted the HTML document to be readable, but you might well get all this text \nin a single line. In HTML, spaces and line breaks don't matter in most contexts.\nThe text between the angle brackets (for example, <html>  or <head> ) is called a tag. \n<html>  is an opening tag and </html>  is a closing tag. The only difference is the / \ncharacter. As this shows, tags come in pairs. Some web pages are sloppy about closing \ntags (using a single <p> tag to separate paragraphs, for instance), but the browser is \nvery permissive and tries to infer where a closing </p>  tag should be.\nEverything between <p> and </p>  is called an HTML element . Note that elements \nmight contain other elements, as is the case for the <div>  element in the example or  \nthe second <p>, which includes an <a> element.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "056caece-2f41-415a-847a-6b4619847017": {"__data__": {"id_": "056caece-2f41-415a-847a-6b4619847017", "embedding": null, "metadata": {"page_label": "14", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed16e6c6-3853-48b7-ab78-1d5917c4fb53", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "64d59e9dcb587b0b7d8d2c997df264653fa201c3e686466383118a1e99b687e0", "class_name": "RelatedNodeInfo"}}, "text": "Understanding HTML and XPath[ 14 ]Some tags are a bit more complex, such as <a href=\"http://www.iana.org/\ndomains/example\"> . The href  part with the URL is called an attribute .\nFinally, many  elements include text, such as the \" Example Domain \", within the <h1>  \nelement.\nThe good news is that not all this markup is important for us. The only things that \nare visible are the elements of the body element: what's between the <body>  and \n</body>  tags. The <head>  part is important to indicate meta-information such as \ncharacter encoding, but Scrapy takes care of most of those issues, so it is highly  \nlikely that you will never have to pay attention to that part of the HTML page.\nThe tree representation\nEvery browser has its own, complex internal data structures with the aid of which \nit renders web pages. The DOM representation is cross-platform and language-\nindependent, and is supported by most browsers.\nTo see the tree representation of a web page in Chrome, right-click on the element \nyou are interested in, and select Inspect Element . If this feature is disabled, you can \nstill access it by clicking on the Chrome menu and then Tools  | Developer Tools .\nAt this point, you see something that looks very similar to the HTML representation, \nbut it's not exactly the same. It's a tree representation of the HTML code. It will \nlook the same regardless of how the original HTML document uses spacing and \nline breaks. You can click on every element to inspect or manipulate attributes, and \nsuch, and see how these changes affect what you see on the screen in real time. For \nexample, if you double-click some text, modify it, and press the Enter  key, the text \non the screen will be updated with the new value. On the right, under the Properties  \ntag, you can see the properties of this tree representation, and at the bottom, you \ncan see a breadcrumb-like structure that shows the exact  position of your currently \nselected element in the hierarchy of HTML elements.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08ba64e9-cf35-4f8a-b12e-85b524aa6696": {"__data__": {"id_": "08ba64e9-cf35-4f8a-b12e-85b524aa6696", "embedding": null, "metadata": {"page_label": "15", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ee774ed-f554-44fd-9e7c-cbec957821bc", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "8d68015828496376c4618c56a64fa9712333e84339c3f9fe61f6c482d599fb40", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 2[ 15 ]\nThe important thing to keep in mind is that while HTML is just text, the tree \nrepresentation is an object inside the browser's memory that you can view and \nmanipulate programmatically, and in the case of Chrome, via the Developer Tools .\nWhat you see on the screen\nThe HTML text representation and the tree representation don't have anything that \nlooks like the beautiful view we usually see on our screen. This is actually one of \nthe reasons that HTML has been so successful. It is a document that is meant to be \nread by humans, and specifies the content of the page, but not the way it's going \nto render on the screen. This means it's the browser's responsibility to render the \nHTML document and make it look nice, whether it's a full-featured browser such as \nChrome, a mobile device browser, or even a text-only browser such as Lynx.\nThat said, the evolution of the web spurred great demand for both web developers \nand users to have more control over how a web page is rendered. CSS was created \nto give hints on how HTML elements are rendered. For scraping, though, we don't \nneed anything that has to do with CSS.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1049d8ed-6225-48cd-858c-2cbd9c21efd7": {"__data__": {"id_": "1049d8ed-6225-48cd-858c-2cbd9c21efd7", "embedding": null, "metadata": {"page_label": "16", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "493874c9-fc7a-46a5-952d-4cc40d7b5f44", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "def856cd44e70a8d0884ea59a13feae32b7280c0d02d77b97bba962b3200190b", "class_name": "RelatedNodeInfo"}}, "text": "Understanding HTML and XPath[ 16 ]So, how does the tree representation map to what we see on the screen? The \nanswer lies in what is called the box model. Just as a DOM tree element can contain \nother elements or text, in the same way, by default, when rendered on the screen, \neach box representation of an element contains the box representations of the \nembedded elements. In that sense, what we see on the screen is a two-dimensional \nrepresentation  of the original HTML document\u2014but the tree structure is a part of the \nrepresentation, in a hidden way. For instance, in the following image, we see how \nthree DOM elements\u2014a <div>  and two embedded elements, an <h1>  and a <p>\u2014\nappear in a browser and in the DOM:\nSelecting HTML elements with XPath\nIf you come  from a traditional software engineering  background, and have \nno knowledge of XPath, you will probably worry that, in order to access this \ninformation in HTML documents, you will have to do lots of string matching, \nsearching for tags on the document, handling special cases, and so on, or somehow \nparse the entire tree representation to extract what you want. The good news is that \nnone of those is necessary. You can select and extract elements, attributes, and text \nwith a language called XPath, specially designed for that purpose.\nIn order to use XPath with Google Chrome, click on the Console  tab of Developer \nTools  and use the $x utility function. For example, you can try $x('//h1')  on \nhttp://example.com/ . It will move the browser to the <h1>  element, as shown in \nthe following screenshot:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5de92e4e-cee4-4c1f-9161-6f76ccdeb20e": {"__data__": {"id_": "5de92e4e-cee4-4c1f-9161-6f76ccdeb20e", "embedding": null, "metadata": {"page_label": "17", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "282cd0a3-6fea-423a-aff0-07ce06bfc6c8", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "de2da7e97ce35558092162a0de41f2b3844fb79967b259c5a881557d78652294", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 2[ 17 ]\nWhat you will see returned on the Chrome Console  is a JavaScript array containing \nthe selected elements. If you put your mouse cursor over those variables, the selected \nelements will be highlighted on the screen. It's very handy.\nUseful XPath expressions\nThe hierarchy  of the document starts with the <html>  element, and you can use \nelement names and slashes to select elements of the document. For example, the \nfollowing is what various expressions will return from the http://example.com/  \npage:\n$x('/html')\n  [ <html>...</html> ]\n$x('/html/body')\n  [ <body>...</body> ]\n$x('/html/body/div')\n  [ <div>...</div> ]\n$x('/html/body/div/h1')\n  [ <h1>Example Domain</h1> ]\n$x('/html/body/div/p')\n  [ <p>...</p>, <p>...</p> ]\n$x('/html/body/div/p[1]')\n  [ <p>...</p> ]\n$x('/html/body/div/p[2]')\n  [ <p>...</p> ]\nNote that because two <p> elements lie under the <div>  on this particular page, \nhtml/body/div/p  returns two elements. You can use p[1]  and p[2]  to access the \nfirst and the second element respectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "197bd7b9-004e-416d-ae04-af04ac5e8a01": {"__data__": {"id_": "197bd7b9-004e-416d-ae04-af04ac5e8a01", "embedding": null, "metadata": {"page_label": "18", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "743ef640-2ca5-4ae4-b3f1-8389eb330a94", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "4ce13e7978a41cc2372c71c1ffe285d95756b1d98d3f38084200cf3f4a9b104e", "class_name": "RelatedNodeInfo"}}, "text": "Understanding HTML and XPath[ 18 ]Note that Document's title, may be the only interesting element from a scraping \nperspective, is in the head  section, and can be accessed with the following expression:\n$x('//html/head/title')\n  [ <title>Example Domain</title> ]\nFor large documents, you might have to write a very large XPath expressions to \nreach specific elements. In order to avoid this, the // syntax allows you to get \nelements of a particular type no matter where they are in the hierarchy. For example, \n//p will select all the p elements, and //a all the links for us.\n$x('//p')\n  [ <p>...</p>, <p>...</p> ]\n$x('//a')\n  [ <a href=\"http://www.iana.org/domains/example\">More \ninformation...</a> ]\nThe //a syntax can also be used anywhere in the hierarchy. For example, to find \nany links under any div, you can use //div//a . Note that //div/a  with single \nslash would give an empty array, because there isn't any `a` directly under `div` in \nexample.com:\n$x('//div//a')\n  [ <a href=\"http://www.iana.org/domains/example\">More \ninformation...</a> ]\n$x('//div/a')\n  [ ]\nYou can also select attributes. The only attribute on http://example.com/  is the \nhref  of the link, which you can access using the character @ as follows:\n$x('//a/@href')\n  [ href=\"http://www.iana.org/domains/example\" ]\nApparently, in recent chrome versions, @href  doesn't return \nthe URLs but empty strings instead. Don't worry, your XPath \nexpression is still correct.\nYou can also select just the text by using the text()  function:\n$x('//a/text()')\n  [ \"More information...\" ]\nYou can use the * character to select all elements at a specific hierarchy level.  \nFor example:\n$x('//div/*')\n[ <h1>Example Domain</h1>, <p>...</p>, <p>...</p> ]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00c30f45-4cb6-453f-8459-ace6ffc79dbc": {"__data__": {"id_": "00c30f45-4cb6-453f-8459-ace6ffc79dbc", "embedding": null, "metadata": {"page_label": "19", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "144cf655-c064-4541-ba7a-f8fd1a26cd71", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7ec52780ebc82bcda8d689f3b38fd014f32297e05ee544f9c264f2072bf437a1", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 2[ 19 ]You will find it very useful to select elements that have a specific attribute, such as \n@class , or that have a specific value as an attribute. You can do it by using more \nadvanced predicates than the numeric ones which we used on the p[1]  and p[2]  \nexamples earlier. For example, //a[@href]  selects link that contains href  attribute, \nand //a[@href=\"http://www.iana.org/domains/example\"]  selects link that have \nan attribute href  with the specified value.\nEven more useful is the ability to find links whose href  attribute starts with, or \ncontains, a specific substring. The following are some examples:\n$x('//a[@href]')\n  [ <a href=\"http://www.iana.org/domains/example\">More \ninformation...</a> ]\n$x('//a[@href=\"http://www.iana.org/domains/example\"]')  [ <a href=\"http://www.iana.org/domains/example\">More \ninformation...</a> ]\n$x('//a[contains(@href, \"iana\")]')  [ <a href=\"http://www.iana.org/domains/example\">More \ninformation...</a> ]\n$x('//a[starts-with(@href, \"http://www.\")]')  [ <a href=\"http://www.iana.org/domains/example\">More \ninformation...</a>]\n$x('//a[not(contains(@href, \"abc\"))]')  [ <a href=\"http://www.iana.org/domains/example\">More \ninformation...</a>]\nThere are tens of XPath functions like not() , contains() , and starts-with()  that \nyou can find in the online documentation ( http://www.w3schools.com/xsl/\nxsl_functions.asp ), but you can go quite far without using most of them.\nI might be getting a bit ahead of myself right now, but you can use the same XPath expressions in a Scrapy shell. To open a page and access the Scrapy shell, you just have to type the following:\nscrapy shell http://example.com\nThe shell gives you access to many variables that are typically available when you write spider code (see next chapter). The most important of them is response, which is an \nHtmlResponse  in case of HTML documents - a class that allows you via it's \nxpath()  method $x in chrome. The following are a few examples:\nresponse.xpath('/html').extract()\n  [u'<html><head><title>...</body></html>']response.xpath('/html/body/div/h1').extract()  [u'<h1>Example Domain</h1>']", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "186b9a0c-f180-481d-9c09-d25ba9dbe618": {"__data__": {"id_": "186b9a0c-f180-481d-9c09-d25ba9dbe618", "embedding": null, "metadata": {"page_label": "20", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83a88c77-8a6d-43d4-8ce1-bb6fa9f573ea", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "968973fa723c0590ccbbed758935059f7b13b8c9a2713182739fe721c3a27959", "class_name": "RelatedNodeInfo"}}, "text": "Understanding HTML and XPath[ 20 ]response.xpath('/html/body/div/p').extract()\n  [u'<p>This domain ... permission.</p>', u'<p><a href=\"http://www.\niana.org/domains/example\">More information...</a></p>']\nresponse.xpath('//html/head/title').extract()\n  [u'<title>Example Domain</title>']\nresponse.xpath('//a').extract()\n  [u'<a href=\"http://www.iana.org/domains/example\">More \ninformation...</a>']\nresponse.xpath('//a/@href').extract()\n  [u'http://www.iana.org/domains/example']\nresponse.xpath('//a/text()').extract()\n  [u'More information...']\nresponse.xpath('//a[starts-with(@href, \"http://www.\")]').extract()\n  [u'<a href=\"http://www.iana.org/domains/example\">More \ninformation...</a>']\nThis  means that you can use Chrome to develop XPath expressions, and then use \nthem in your Scrapy crawler as we will see in the following chapter.\nUsing Chrome to get XPath expressions\nChrome acts  even more developer-friendly by giving us basic XPath expressions. Start \nby inspecting an element as shown earlier: right-click on the desired element, and \nthen choose Inspect Element . This opens Developer Tools  and the HTML element \nin the tree representation will be highlighted. Now right-click on it, and select Copy \nXPath  from the menu; the XPath expression will be copied to the clipboard.\nYou can test the expression as always from the console:\n$x('/html/body/div/p[2]/a')\n  [ <a href=\"http://www.iana.org/domains/example\">More  \ninformation...</a>]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d950b9a-9753-405e-a32d-a0893977d55e": {"__data__": {"id_": "7d950b9a-9753-405e-a32d-a0893977d55e", "embedding": null, "metadata": {"page_label": "21", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b5305d9-064c-4cf7-bc3f-a73299ea7ce9", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "73b4f3cd2f9898218d03ae796d5e9ce31e94d63fee2b2a5720f79960a3f2c166", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 2[ 21 ]Examples of common tasks\nThere are some uses for XPath expressions that you will probably encounter quite \noften. Let's see some examples that work (right now) for Wikipedia pages. Wikipedia \nhas a very stable format, so I wouldn't expect them to break soon, but eventually \nthey will. Consider those expressions as illustrative examples.\n\u2022 Getting the text  of the span  under the div with id \"firstHeading\" :\n//h1[@id=\"firstHeading\"]/span/text()\n\u2022 Getting the URLs of links in an unordered list ( ul) inside a div with id \n\"toc\" :\n//div[@id=\"toc\"]/ul//a/@href\n\u2022 Getting the text anywhere inside a header element ( h1) inside any element \nwith a class  attribute containing \"ltr\"  and a class  attribute containing \n\"skin-vector\" . The two strings may be in the same class or different ones.\n//*[contains(@class,\"ltr\") and contains(@class,\"skin-vector\")]//\nh1//text()\nActually, you will often need to use classes in your XPath expressions. In these cases, \nyou should remember that due to some styling elements called CSS, you will often \nsee HTML elements having multiple classes stated on their class  attribute. This \nmeans that you will see, for example, some of your divs with their class  attribute set \nto \"link\"  and some others to \"link active\"  in a navigation system. The latter will \nbe the links that are currently active, thus visible or highlighted with a special color \n(via CSS). When scraping, you will usually be interested in elements that contain \na certain class, that is, both \"link\"  and \"link active\"  in the previous example. \nThe contains()  XPath function allows you to select all the elements that contain a \ncertain class.\n\u2022 To select the URL for the first image in the table that has a class  attribute \nwith value \"infobox\" , use the following:\n//table[@class=\"infobox\"]//img[1]/@src\n\u2022 To select all the URLs of the links under the div with a class  attribute that \nstarts with \"reflist\" :\n//div[starts-with(@class,\"reflist\")]//a/@href\n\u2022 To select all the URLs of links under the div element following an element \nwhose child element contains the text \"References\" :\n//*[text()=\"References\"]/../following-sibling::div//a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f198dc6-20f7-49ba-acc6-0fa7b11c3604": {"__data__": {"id_": "5f198dc6-20f7-49ba-acc6-0fa7b11c3604", "embedding": null, "metadata": {"page_label": "22", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a429bb-11a1-451a-85e1-6692cdb307f8", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "0b41ce6bdada338443e1c0d5552a8ba7e8226eb37dff9e7b5824937acb03a202", "class_name": "RelatedNodeInfo"}}, "text": "Understanding HTML and XPath[ 22 ]Note that this is quite fragile and easy to break, because it makes many \nassumptions on the structure of the document.\n\u2022 To get the URLs for every image on the page:\n//img/@src\nAnticipating changes\nScraping often targets pages on servers that are beyond our control. This means \nthat if their HTML changes in a way that makes our XPath expressions invalid, we \nwill have to go back to our  spiders and correct them. This doesn't usually take long, \nbecause the changes are typically small. However, it's certainly something we would \nprefer to avoid. Some simple rules help us reduce the odds that your expressions will \nbecome invalid:\n\u2022 Avoid array indexes (numbers)\nChrome will often give you expressions with lots of constant numbers  \nsuch as:\n//*[@id=\"myid\"]/div/div/div[1]/div[2]/div/div[1]/div[1]/a/img \nThis is quite fragile, because if something like an advertisement block adds \nan extra div somewhere in that hierarchy, those numbers will end up \npointing to different elements. The solution in this case is to go as close as \npossible to the target img tag, and find an element with an id or a class  \nattribute that you can use, such as:\n//div[@class=\"thumbnail\"]/a/img\n\u2022 Classes are not that good\nUsing class  attributes makes it easy to pinpoint elements, but they are \noften used to affect the looks of the page via CSS, and may thus change as a \nresult of minor alterations to the website's layout. For example, the following \nclass :\n//div[@class=\"thumbnail\"]/a/img\nThis might after a while, turn to:\n//div[@class=\"preview green\"]/a/img", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85823400-3c0e-44b4-b2db-a4d392954840": {"__data__": {"id_": "85823400-3c0e-44b4-b2db-a4d392954840", "embedding": null, "metadata": {"page_label": "23", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9decd28f-b1e6-4d2c-ac0e-5a5ef3b79ad9", "node_type": "4", "metadata": {"page_label": "23", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "bec1f91806286dc6086af393a64f44f83c63e064a9dab91214c758cf4977cde6", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 2[ 23 ]\u2022 Meaningful data-oriented classes are better than specific or layout-oriented \nones\nIn the previous example, both \"thumbnail\"  and \"green\"  are examples of \nbad class names to rely on. The name \"thumbnail\"  is certainly better than \n\"green\" , but both are inferior to something like \"departure-time\" . The \nfirst two are used for layout, whereas \"departure-time\"  is something \nmeaningful, related to the contents of the div. As a result, the latter is more \nlikely to remain valid even when the layout changes. It might also indicate \nthat the developers of the site are aware of the benefits of annotating their \ndata with meaningful and consistent ways.\n\u2022 IDs are  often the most reliable\nThe id attributes are usually the best choice for a target, as long as they are \nmeaningful and data-related. Partially, this is because JavaScript and external \nlink anchors often use them to reference specific parts of the document. For \nexample, the following XPath is quite robust:\n//*[@id=\"more_info\"]//text()\nExceptions to this are programmatically generated IDs that include unique \nreferences. Those render them useless for scraping. For example:\n//[@id=\"order-F4982322\"]\nThe above is a very bad XPath expression despite being an id. Also keep in \nmind that, even though IDs should be unique, you will find many HTML \ndocuments where they are not.\nSummary\nThe quality of markup continuously improves, and it's now much easier to create \nrobust XPath expressions that extract data from HTML documents. In this chapter, \nyou learned the basics of HTML documents and XPath expressions. You saw how to \nuse Google Chrome to automatically get some XPath expressions as a starting point \nthat we can later optimize. You also learned how to create such expressions directly \nby inspecting the HTML document, and how to tell a robust XPath expression from \na less robust one. We are now ready to use all this knowledge to write our first few \nspiders with Scrapy in Chapter 3 , Basic Crawling .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71244b1f-26e3-48fa-8a73-98e33d8c93df": {"__data__": {"id_": "71244b1f-26e3-48fa-8a73-98e33d8c93df", "embedding": null, "metadata": {"page_label": "24", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2afae01f-4b93-4f6a-9ac4-5aa43801a725", "node_type": "4", "metadata": {"page_label": "24", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ef69ee7bc35cddb31d1509549ec0cb1ebfad9866c878d2285d5506b6e1c85a69", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56a51535-bad6-4882-907b-22181ec3c54c": {"__data__": {"id_": "56a51535-bad6-4882-907b-22181ec3c54c", "embedding": null, "metadata": {"page_label": "25", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35474a31-1650-473f-a6a2-7d226b67a4db", "node_type": "4", "metadata": {"page_label": "25", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "8eb13942f0f968f2438c373854d621250a8d0c80e989c5a3cd65f497180863fe", "class_name": "RelatedNodeInfo"}}, "text": "[ 25 ]Basic Crawling\nThis is a very important chapter, which you will probably read several times, and \nreturn to often for finding solutions. We are going to start by explaining how to \ninstall Scrapy, and then move on to the methodology of developing Scrapy crawlers \nalong with numerous examples and alternative implementations. Before we start, \nlet's take a look at some important notions.\nSince we are quickly moving to the fun coding part, it's important to be able to use \nthe code segments you find in this book. When you see the following:\n$ echo hello world\nhello world\nit means you are to type echo hello world  on a terminal (skip the dollar sign). The \nline(s) that follow are the output as seen on your terminal.\nWe will use the terms 'terminal', 'console', and 'command line' \ninterchangeably. They don't make much difference in the context \nof this book. Please Google a bit to find out how to start the console \non your platform (Windows, OS X, or others). You will also find \ndetailed instructions in Appendix A , Installing Prerequisites .\nWhen you see the following:\n>>> print 'hi'\nhi\nit means you have to type print 'hi'  on a Python or Scrapy shell prompt  \n(skip >>>). Again, the line(s) that follow are the output of the command as seen  \non your terminal.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c53e19c7-96ee-4980-8f73-a9c450614c75": {"__data__": {"id_": "c53e19c7-96ee-4980-8f73-a9c450614c75", "embedding": null, "metadata": {"page_label": "26", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cff52d8b-0a61-4d44-97b4-984ce9c21efa", "node_type": "4", "metadata": {"page_label": "26", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e46927411e02c0cd653d148fbbb3d6bd960ba6ec17eeced68ceb992c5bef8df9", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 26 ]For this book, you will also need to edit files. The tools you're going to use depend \nheavily on your environment. If you use Vagrant (highly recommended), you will \nbe able to edit files with editors like Notepad, Notepad++, Sublime Text, TextMate, \nEclipse, or PyCharm on your PC/laptop. If you are more experienced with Linux/\nUnix, you might also like to edit files directly from the console with vim or emacs. \nBoth of them are powerful, but have a learning curve. If you are a beginner, and you \nhave to edit something in the console, you might also like to try the more beginner-\nfriendly nano editor.\nInstalling Scrapy\nThe installation of Scrapy is relatively easy, but it all depends on where you're \nstarting from. To be able to support as many people as possible, the \"official\" way \nof running/installing Scrapy as well as all the examples in this book is through \nVagrant\u2014a software that allows you to run a standard Linux box with all the tools \nthat we've set up for you inside your computer, no matter what operating system \nit runs on. We provide instructions for Vagrant, and a few instructions for some \npopular operating systems in the following sections.\nMacOS\nTo easily follow this book, please follow the instructions on Vagrant given later. \nIf you want to install Scrapy natively for MacOS, that's quite easy. Just type in the \nfollowing command:\n$ easy_install scrapy\nand everything should be taken care of for you. It might, in the process, ask you for \nyour password or installing Xcode. That's perfectly fine, and you can safely accept \nthe same.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afbc7918-d391-44c2-bb5b-6ac0093aeb11": {"__data__": {"id_": "afbc7918-d391-44c2-bb5b-6ac0093aeb11", "embedding": null, "metadata": {"page_label": "27", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97e39be8-dab5-4c1d-b529-e582e146bdd1", "node_type": "4", "metadata": {"page_label": "27", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7e6023ac6ebad0613fbf760246057028461352e5533ef19b967b72f3100ea93b", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 27 ]\nWindows\nInstalling Scrapy natively on Windows is somewhat advanced, and frankly, a bit \nof a pain. Additionally, installing all the software that you need to enjoy this book \nrequires a significant degree of courage and determination. We have you covered \nthough. Vagrant with Virtualbox runs great on every Windows 64-bit platform. \nJump to the relevant section given further in this chapter, and you will be up and \nrunning in minutes. If you really need to install Scrapy natively on Windows, consult \nthe resources on this book's website http://scrapybook.com\nLinux\nAs with the previous two operating systems, Vagrant is the recommended way to go \nas far as following this book is your goal.\nYou will likely need to install Scrapy in many cases on Linux servers, so a few more \ndetailed instructions might be beneficial.\nThe exact dependencies change quite often. The Scrapy \nversion we are installing at the time of writing is 1.0.3, and \nthe following are indicative instructions for different major \ndistributions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4dbda92-63a4-4b5b-88c0-87f66074dc8a": {"__data__": {"id_": "b4dbda92-63a4-4b5b-88c0-87f66074dc8a", "embedding": null, "metadata": {"page_label": "28", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "50ff17f2-18c0-4b6c-ad3c-b5051c6d0348", "node_type": "4", "metadata": {"page_label": "28", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "71d8a32165e86c5e52bbad6675a9558e60e29cea6e219cf2638233854d0a6481", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 28 ]Ubuntu or Debian Linux\nIn order to install Scrapy on Ubuntu (tested with Ubuntu 14.04 Trusty Tahr - 64 bit) \nor other distributions that use apt, the following three commands should be enough:\n$ sudo apt-get update\n$ sudo apt-get install python-pip python-lxml python-crypto python-\ncssselect python-openssl python-w3lib python-twisted python-dev libxml2-\ndev libxslt1-dev zlib1g-dev libffi-dev libssl-dev\n$ sudo pip install scrapy\nThis preceding process requires some compilation, and might break every now and \nthen, but it will give you the most recent version of Scrapy available on PyPI (that is, \nquite recent). If you want to avoid any complication, and are okay with a potentially \nless up-to-date version, google for \"install Scrapy Ubuntu packages\", and follow the \ninstructions given in the official Scrapy documentation.\nRed Hat or CentOS Linux\nIt's equally easy to install Scrapy on Red Hat or other distributions (tested with \nUbuntu 14.04 Trusty Tahr - 64 bit) that use yum. All you need is the following  \nthree lines:\nsudo yum update\nsudo yum -y install libxslt-devel pyOpenSSL python-lxml python-devel gcc\nsudo easy_install scrapy\nFrom the latest source\nIf you have followed the preceding instructions, you have all the dependencies that \nScrapy currently needs. Scrapy is 100 percent Python, so if you like hacking the \nsource code or test-driving the latest features, you can easily clone the latest version \nfrom https://github.com/scrapy/scrapy . To install Scrapy on your system just \ntype in the following commands:\n$ git clone https://github.com/scrapy/scrapy.git\n$ cd scrapy\n$ python setup.py install\nI guess if you belong to this class of Scrapy users, it's unnecessary for me to mention \nvirtualenv .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "329efccc-2202-4f99-9ada-3b82280913d7": {"__data__": {"id_": "329efccc-2202-4f99-9ada-3b82280913d7", "embedding": null, "metadata": {"page_label": "29", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7ea82c2-fb81-481c-a092-c8425d27edc4", "node_type": "4", "metadata": {"page_label": "29", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "d7c688ee78ba4cecb9da45ec4dca5d302230ee224c1f436cff0b768ada901dbe", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 29 ]Upgrading Scrapy\nScrapy gets upgraded rather often. You will find yourself needing to upgrade within \nno time, and you can do it with pip, easy_install , or aptitude :\n$ sudo pip install --upgrade Scrapy\nor\n$ sudo easy_install --upgrade scrapy\nIf you need to downgrade or choose a specific version, you can do it by specifying \nthe version you want, for example:\n$ sudo pip install Scrapy==1.0.0\nor\n$ sudo easy_install scrapy==1.0.0\nVagrant: this book's official way to run \nexamples\nThis book has some complex interesting examples some of which use many services. \nNo matter how beginner or advanced you are, you will be able to run the examples \nin this book, because a program called Vagrant allows us to set up this complex \nsystem with a single command.\nThe system used in this book\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b5fa268-c48d-4898-8f41-228a09085fd1": {"__data__": {"id_": "9b5fa268-c48d-4898-8f41-228a09085fd1", "embedding": null, "metadata": {"page_label": "30", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b4184a0-8836-4710-8249-a1954c07279c", "node_type": "4", "metadata": {"page_label": "30", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "94df491d4f9a858af23e22e0100f1f7292c0a5e4703276e07a531ea6bba24ba2", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 30 ]Your PC or laptop in Vagrant terminology is called the \"host\" machine. Vagrant uses \nour host machine to run a docker provider VM (virtual machine ). These technologies \nallow us to have an isolated system with its own private network where this book's \nexamples run regardless of the software and hardware of your host machine.\nMost chapters use just two services - the \"dev\" machine and the \"web\" machine. \nWe log-in to the dev machine and run Scrapy crawls that scrape pages from the \nweb machine. Later chapters use more services including databases and big data \nprocessing engines.\nFollow the instructions in Appendix A , Installing Prerequisites , to install Vagrant on \nyour operating system. By the end of that chapter you will have git and Vagrant  \ninstalled on your computer. You have your console/terminal/command prompt \nopen and you can now get the code of this book by doing:\n$ git clone https://github.com/scalingexcellence/scrapybook.git\n$ cd scrapybook\nYou can then start the Vagrant system by typing:\n$ vagrant up --no-parallel\nThis will take some time the first time you run it - depending on your internet \nconnection. After the first time, `vagrant up` should be instantaneous. As soon as \nyour system is up, you can log in to your virtual dev machine with:\n$ vagrant ssh\nYou are now on the dev console where you can follow the rest of the instructions in \nthis book. The code has been cloned from your host machine to the dev machine and \nyou can find it on the book directory:\n$ cd book\n$ ls", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "203920f2-f7cf-4859-a903-61cf0367d0ca": {"__data__": {"id_": "203920f2-f7cf-4859-a903-61cf0367d0ca", "embedding": null, "metadata": {"page_label": "31", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f2074fa-3df6-4cf8-a50f-bf32ab8ff17c", "node_type": "4", "metadata": {"page_label": "31", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "9dccc70f572fc388138fa55c937bbd984dcf884aaa45367cb3eb9551e78ee65d", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 31 ]ch03 ch04 ch05 ch07 ch08 ch09 ch10 ch11 ... \nOpen a few consoles and do vagrant ssh  to have multiple dev terminals to play \nwith. You can use vagrant halt  to shut the system down and vagrant status  \nto check their status. Note that vagrant halt  won't turn off the VM. If that's a \nproblem open VirtualBox and stop it manually or use vagrant global-status  to \nfind its id (name \"docker-provider\") and halt it with vagrant halt <ID> . Most of \nthe examples will be able to run even if you are offline which is a great side effect of \nusing Vagrant.\nNow that we have set up the system properly, we are ready to start learning Scrapy.\nUR2IM \u2013 the fundamental scraping \nprocess\nEvery website is different, and you will certainly need to do some extra study, or ask \nsome questions on the Scrapy mailing list if something is unusual. However, what is \nimportant in order to know where and how to search is to have an overview of the \nprocess, and know the related terminology. While working with Scrapy, the general \nprocess that you most often follow is the UR2IM process.\nThe UR2IM process", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dec05349-1670-4914-b209-87400343389a": {"__data__": {"id_": "dec05349-1670-4914-b209-87400343389a", "embedding": null, "metadata": {"page_label": "32", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f51e46b4-d1c6-49f8-bf21-278040d32ff7", "node_type": "4", "metadata": {"page_label": "32", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "3e1cb35a02f4cbfa065fe1370174861216225c302c12d88f8d4200f265cd9d69", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 32 ]The URL\nIt all starts with a URL. You will need a few example URLs from the site you want to \nscrape. I'm going to demonstrate this using the Gumtree classifieds site ( https://\nwww.gumtree.com/ ) as an example.\nBy visiting, for example, the London properties index page of Gumtree on http://\nwww.gumtree.com/flats-houses/london , you will be able to find numerous \nexamples of URLs of properties. You can copy them by right clicking on the \nclassifieds' listings, and clicking Copy Link Address  or the equivalent for your \nbrowser. One of them, for example, might look like this: https://www.gumtree.\ncom/p/studios-bedsits-rent/split-level . It's okay for you to play with a few \nURLs on the real site. Unfortunately, XPath expressions will likely stop working on \nthe real Gumtree site after some time as their website changes. Gumtree also doesn't \nreply, unless you set a \"user-agent header\". More on this a bit later, but for now, if \nyou want to load one of their pages, you can use the Scrapy shell, as follows:\nscrapy shell -s USER_AGENT=\"Mozilla/5.0\" <your url here  e.g. http://www.\ngumtree.com/p/studios-bedsits-rent/...>\nTo debug problems while using scrapy shell, add the --pdb  argument to enable \ninteractive debugging in case of exceptions. For example:\nscrapy shell --pdb https://gumtree.com\nThe Scrapy shell is an invaluable tool that helps us \ndevelop with Scrapy.\nObviously, we don't encourage you hitting Gumtree's website while learning with \nthis book, and we don't want the examples of this book to break anytime soon. We \nalso want you to be able to develop and play with our examples even if you aren't \nconnected to the Internet. That's why your Vagrant development environment \ncontains a web server that provides generated pages similar to those of Gumtree. \nThey might not look as nice as the real site, but from a scraper's perspective, they \nare exactly the same. That said, we still prefer all the screenshots of the chapter to \ncome from the real Gumtree site. From your Vagrant dev machine, you can hit the \nweb server at http://web:9312/ , and also from your web browser at http://\nlocalhost:9312/ .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6a8c80a-957b-4ca4-b5cc-227563d63500": {"__data__": {"id_": "e6a8c80a-957b-4ca4-b5cc-227563d63500", "embedding": null, "metadata": {"page_label": "33", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9ba5693-c090-498c-a2ff-3dccdd5e502c", "node_type": "4", "metadata": {"page_label": "33", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "57fe0a3947198bc2d124afe986616a22460ca69c5a4b58207ad31e8fddf36a19", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 33 ]Let's open a page from that server with the Scrapy shell, and play a bit by typing the \nfollowing on our dev machine:\n$ scrapy shell http://web:9312/properties/property_000000.html\n...[s] Available Scrapy objects:[s]   crawler    <scrapy.crawler.Crawler object at 0x2d4fb10>[s]   item       {}[s]   request    <GET http:// web:9312/.../property_000000.html>[s]   response   <200 http://web:9312/.../property_000000.html>[s]   settings   <scrapy.settings.Settings object at 0x2d4fa90>[s]   spider     <DefaultSpider 'default' at 0x3ea0bd0>[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local...[s]   view(response)    View response in a browser>>>\nWe got some output, and now you are on a (Python) prompt that you can use to \nexperiment with the page you just loaded (you can always exit with Ctrl + D).\nThe request and the response\nWhat you might notice in the preceding log is that the Scrapy shell did some work for us by itself. We gave it a URL, and it performed a default \nGET request and got a \nresponse with the success code 200. This means that the information from this page \nis already loaded and ready to be used. If we try to print the first 50 characters of \nresponse.body , we get the following:\n>>> response.body[:50]\n'<!DOCTYPE html>\\n<html>\\n<head>\\n<meta charset=\"UTF-8\"'\nWhat is this [:50]? It's the Python way of extracting the first \n50 characters (if available) from a textual variable (in this case, \nresponse.body). If you haven't seen Python before, just keep calm and follow along. Soon you will be familiar with and enjoy all these syntax tricks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfe13f5d-c6de-4f10-9cdc-0ca968cc3dbf": {"__data__": {"id_": "cfe13f5d-c6de-4f10-9cdc-0ca968cc3dbf", "embedding": null, "metadata": {"page_label": "34", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b1ee236-61e5-46ef-af8f-632cd957e2ff", "node_type": "4", "metadata": {"page_label": "34", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f3ef8ab8b63fb765908d4236391d268b2b53ad0afcd8b1313b31fe58344bd93c", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 34 ]This is the HTML content of the given page on Gumtree. The request and  \nresponse part didn't cause us too much trouble. However, there are many cases \nwhere you will need to do some work to get those right. We will see a few of those in \nChapter 5 , Quick Spider Recipes . For now, we keep things simple, and move to the next \npart\u2014the Item .\nThe Items\nThe next step is to try and extract data from the response into the fields of the Item . \nSince the format of this page is HTML, we use XPath expressions to do so. Let's first \nhave a look at the page:\nThe page, the fields we are interested in, and their HTML code", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0e25f00-6d03-43c1-a1c6-96a02073b1b0": {"__data__": {"id_": "c0e25f00-6d03-43c1-a1c6-96a02073b1b0", "embedding": null, "metadata": {"page_label": "35", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "598c72cb-9121-430a-abcb-869d79fff9fd", "node_type": "4", "metadata": {"page_label": "35", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f9c6b4b9a96b8bcb3c0afe586ec9c442899bde1ffffbcf85e6c70dc53363e331", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 35 ]As you can see in the preceding screenshot, there's lots of information here, but \nmost of it is layout: logos, search boxes, buttons, and so on. It is great, but not very \ninteresting from the scraping perspective. The fields we might be interested in might \nbe, for example, the title of the listing, the location, or the agent's telephone number. \nThose have a corresponding HTML element, and we will need to locate it, and \nextract data with the process we described in the previous chapter. Let's start with \nthe title.\nExtracting the title", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef77447e-ff08-4aad-802b-589c650fe907": {"__data__": {"id_": "ef77447e-ff08-4aad-802b-589c650fe907", "embedding": null, "metadata": {"page_label": "36", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d689d4bd-73e1-42bc-a78e-d3db93795f30", "node_type": "4", "metadata": {"page_label": "36", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "9083abb0b846d42772ac865168d207e15edf073b420358657f64bbc240289d91", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 36 ]Right-click on the title on the page, and select Inspect Element . This takes us to the \nrelevant HTML code. Now, try to extract the XPath expression of this title by right-\nclicking and selecting Copy XPath . You will notice that Chrome gives us an XPath \nexpression that is accurate, but it's very complicated, and thus, fragile. We will \nsimplify it a bit. We will just use the last part of it and select H1 elements wherever \nwe see them in the page by using the expression: //h1 . This is the cheat's method, \nbecause we don't really want every H1 in the page, but just the one that is used as \na title; however, it is considered good SEO practice to have a single H1 element in \nevery page, and most websites will have only one.\nSEO is the acronym for Search Engine Optimization: the \nprocess of optimizing a website's coding, content, and \ninbound and outbound links in order to promote it in the \nbest possible way to search engines.\nLet's check to see if this XPath expression works with the Scrapy shell:\n>>> response.xpath('//h1/text()').extract()\n[u'set unique family well']\nExcellent, it works fine. What you will notice is that I appended /text()  at the end \nof the //h1  expression. This is necessary in order to extract just the text contained \nin H1, and not the H1 element itself. We will almost always use /text()  for textual \nfields. If we skip it, we get the text for the whole element including markup, which is \nnot what we want:\n>>> response.xpath('//h1').extract()\n[u'<h1 itemprop=\"name\" class=\"space-mbs\">set unique family well</h1>']\nAt this point, we have the code to extract the first interesting property of this  \npage\u2014the title\u2014but if you take a better look, you will notice an easier and better \nway of doing so.\nGumtree has microdata markup", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e67f71f-79db-4666-8c2f-ca65f75f71af": {"__data__": {"id_": "9e67f71f-79db-4666-8c2f-ca65f75f71af", "embedding": null, "metadata": {"page_label": "37", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b22ec1c1-c5d5-40d3-83ba-53760794e3da", "node_type": "4", "metadata": {"page_label": "37", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "297240c6572b4491056ff92f762c0e169baf8b22c7eedf7c293308eabfd9fe0e", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 37 ]Gumtree has annotated their HTML with microdata markup. We can see, for \nexample, that there is an itemprop=\"name\"  attribute in their header. This is great. It \nallows us to use a simpler XPath expression that doesn't include any visual elements: \n//*[@itemprop=\"name\"][1]/text() . You might wonder why we select the first \nelement with itemprop=\"name\" .\nWait\u2014did you say first? If you are a seasoned programmer, \nyou've probably been used to array[1]  being the second \nelement of an array. Surprisingly, XPath is 1-based (!) thus \narray[1]  is the first element of the array.\nWe do so, not only because itemprop=\"name\"  is used in many different contexts in \nmicrodata but also because Gumtree has nested information for other properties in \nthe \"You may also like\u2026\" section of their page in a way that prevents us from easily \ndistinguishing them. Nevertheless, this is not a big problem. We just select the first \none, and we will do the same for all the other fields.\nLet's have a look at price. Price is contained in the following HTML structure:\n<strong class=\"ad-price txt-xlarge txt-emphasis\" itemprop=\"price\">  \n\u00a3334.39pw</strong>\nAgain we see itemprop=\"name\" , which is brilliant. Our XPath will be //*[@\nitemprop=\"price\"][1]/text() . Let's try it:\n>>> response.xpath('//*[@itemprop=\"price\"][1]/text()').extract()\n[u'\\xa3334.39pw']\nWe notice some Unicode characters (the pound sign \u00a3), and then the 350.00pw  price. \nIt is indicative that data isn't always as clean as we would wish, and we might need \nto clean them a bit more. In this case, for example, we might want to use a regular \nexpression to select just the numbers and the decimal dot. We can do so by using the \nre()  method and a simple regular expression instead of extract() :\n>>> response.xpath('//*[@itemprop=\"price\"][1]/text()').re('[.0-9]+')\n[u'334.39']", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c93fa4d5-a8e6-41ff-9a26-cc767995488f": {"__data__": {"id_": "c93fa4d5-a8e6-41ff-9a26-cc767995488f", "embedding": null, "metadata": {"page_label": "38", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8c1ab1e-850a-49e4-9e1e-597421df5649", "node_type": "4", "metadata": {"page_label": "38", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b80579a746b49cd47d923ede3e971f0e2b2894923a8bab414d0dded6b2af99c6", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 38 ]We use a response  object, and call its xpath()  method to extract \ninteresting values. But what are the values xpath() returns us? If we don't \nuse the .extract()  method with a trivial XPath, we get the following \nindicative output:\n>>> response.xpath('.')  \n[<Selector xpath='.' data=u'<html>\\n<head>\\n<meta \ncharse'>]\nxpath()  returns Selector  objects preloaded with the contents of \nthe page. We just used the xpath()  method, but it has another useful \nmethod: css() . Both xpath()  and css()  return selectors, and only \nwhen we call the extract()  or re()  method we get actual arrays of text. \nThis is brilliant, because it allows us to chain the xpath()  and css()  \noperations. For example, we could use css()  to extract the right HTML \nelement quickly:\n>>> response.css('.ad-price')  \n[<Selector xpath=u\"descendant-or-self::*[@class and  \ncontains(concat(' ', normalize-space(@class), ' '), '  \nad-price ')]\" data=u'<strong class=\"ad-price txt-xlarge  \ntxt-e'>]\nNotice that behind the scenes, css()  actually compiles an xpath()  \nexpression, but what we type is simpler than the XPath itself. Then we can \nchain xpath()  to extract just the text.\n>>> response.css('.ad-price').xpath('text()')\n[<Selector xpath='text()' data=u'\\xa3334.39pw'>]\nFinally, we might chain our regular expression with re()  to extract our \nvalue:\n>>> response.css('.ad-price').xpath('text()').re('[.0-\n9]+')\n[u'334.39']\nThis expression is, practically speaking, no better or worse than our \noriginal expression. Consider this as a thought-provoking illustrative \nexample. In this book, we will keep things as simple as possible, and we \nwill use old good XPath as much as we can. The key point to remember is \nthat xpath()  and css()  return Selector  objects that might be chained. \nIn order to get actual values, use either extract()  or re().  With each \nnew version of Scrapy there are new exciting and high-value features \nadded around those classes. The relevant Scrapy documentation section \n- http://doc.scrapy.org/en/latest/topics/selectors.html  \n- is excellent and make sure you have a good look at it to find the most \nefficient way to extract your data..", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa397134-a271-4afd-8bd7-a511cd337927": {"__data__": {"id_": "aa397134-a271-4afd-8bd7-a511cd337927", "embedding": null, "metadata": {"page_label": "39", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f10e9dc3-2a5a-468a-aaa1-b1d512035d41", "node_type": "4", "metadata": {"page_label": "39", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ffc953c960b593f55d14484626c35f9a9073e8b805acab7717e6183a716cc9d4", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 39 ]The story for the description text is similar. There is an itemprop=\"description\"  \nproperty that indicates description. The XPath expression is: //*[@\nitemprop=\"description\"][1]/text() . Similarly, the address is annotated with \nitemtype=\"http://schema.org/Place\";  thus, the XPath expression is: //*[@\nitemtype=\"http://schema.org/Place\"][1]/text() .\nSimilarly, the image has an itemprop=\"image\" . We thus use //img[@\nitemprop=\"image\"][1]/@src . Notice that we won't use /text()  in this case, because \nwe don't want any text but just the src attribute that contains the URL for this image.\nAssuming that this is all the information we want to extract, we can summarize it in \nthe following table:\nPrimary fields XPath expression\ntitle //*[@itemprop=\"name\"][1]/text()\nExample value: [u'set unique family well']\nprice //*[@itemprop=\"price\"][1]/text()\nExample value (using re() ):[u'334.39']\ndescription //*[@itemprop=\"description\"][1]/text()\nExample value: [u'website court warehouse\\r\\npool...']\naddress //*[@itemtype=\"http://schema.org/Place\"][1]/text()\nExample value: [u'Angel, London']\nimage_urls //*[@itemprop=\"image\"][1]/@src\nExample value: [u'../images/i01.jpg']\nNow this table is important, because if we had many websites with similar \ninformation, we would most likely need to create many similar spiders where only \nthe preceding expressions may need to be different. Additionally, if we wanted to \nscrape tens of websites, we could use such a table to split the workload.\nUp to this point, we used mostly HTML and XPath. From this point on, we will write \nsome real Python.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a855481-04db-4793-ab18-ed7814a19e75": {"__data__": {"id_": "7a855481-04db-4793-ab18-ed7814a19e75", "embedding": null, "metadata": {"page_label": "40", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4dcd26de-aad6-4043-8fb1-05429ef756b5", "node_type": "4", "metadata": {"page_label": "40", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f86ff1e003de361d960b4e089e14e47fc5dbc8e1ade6a6be10f487b4581f8ebf", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 40 ]A Scrapy project\nUp to now, we were \"playing\" with Scrapy shell. Now we have all the necessary \ningredients to start our first Scrapy project, and we can quit Scrapy shell by \nhitting Ctrl + D. Notice that everything you might have typed up to now gets lost. \nObviously, we don't want to type the code each time we want to crawl something, \nso it's important to remember that the Scrapy shell is just a utility to help us play \nwith pages, XPath expressions, and Scrapy objects. Don't invest much time in writing \ncomplicated code there, because it's bound to get lost as soon as you exit. In order \nto write real Scrapy code, we use projects. Let's create a Scrapy project and name it \n\"properties \", since we are scraping real estate properties.\n$ scrapy startproject properties\n$ cd properties\n$ tree\n.\n\u251c\u2500\u2500 properties\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 items.py\n\u2502   \u251c\u2500\u2500 pipelines.py\n\u2502   \u251c\u2500\u2500 settings.py\n\u2502   \u2514\u2500\u2500 spiders\n\u2502       \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 scrapy.cfg\n2 directories, 6 files\nJust to remind you that you can get all the source code of this book \nfrom GitHub. To download this code, use the following command:\ngit clone https://github.com/scalingexcellence/\nscrapybook\nThe code from this chapter is available in the ch03  directory, and \nfor this example in particular, in the ch03/properties  directory.\nWe can see the directory structure for this Scrapy project. The scrapy startproject \nproperties  command created a directory with the name of the project with three \ninteresting files: items.py , pipelines.py , and settings.py . There is also a \nsubdirectory named spiders , which is empty right now. In this chapter, we will \nwork mostly with items.py  and files in the spiders  directory. In later chapters, we \nwill explore more settings, pipelines, and the scrapy.cfg  file.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "506f08b2-4390-49c9-869a-13a1a8235bf2": {"__data__": {"id_": "506f08b2-4390-49c9-869a-13a1a8235bf2", "embedding": null, "metadata": {"page_label": "41", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5342f389-52cc-4e81-a12e-9f1a299143a3", "node_type": "4", "metadata": {"page_label": "41", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "0e97ffe208b0ae200775043d1f134aaa403058dc7ae83cae10147c579db175c9", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 41 ]Defining items\nLet's open items.py  with a file editor. There is already some template code in there, \nbut we will modify it for our use case. We will redefine the PropertiesItem class  \nto add the fields that we summarized in the previous table.\nWe will also add a few fields that we will use later for our application (so that we \nwon't have to touch this file again). We will explain them in depth later in this book. \nAn important thing to notice is that the fact that we declare a field doesn't mean \nwe are going to fill it in on every spider, or even use it altogether. Feel free to add \nwhatever fields you feel appropriate\u2014you can always correct them later.\nCalculated fields Python expressions\nimages The images pipeline will fill this in automatically based on \nimage_urls . More on this in a later chapter.\nlocation Our geocoding pipeline will fill this in later. More on this in a \nlater chapter.\nWe will also add a few housekeeping fields. Those are not application-specific, but \nare just fields that I personally find interesting and think that might help me debug \nmy spider in the future. You might or might not choose to have some of them for \nyour projects. If you have a look at them, you'll understand that they allow me to \nfind out where (server, url), when (date), and how (spider) an item got scraped. \nThey might let me automate tasks like expiring items and scheduling new scrape \niterations, or to drop items that came from a buggy spider. Don't worry if you don't \nunderstand all those expressions, especially the server one. Things will become clear \nas we move on to later chapters.\nHousekeeping fields Python expressions\nurl response.url\nExample value: 'http://web.../property_000000.\nhtml'\nproject self.settings.get('BOT_NAME')\nExample value: 'properties'\nspider self.name\nExample value: 'basic'\nserver socket.gethostname()\nExample value: 'scrapyserver1'\ndate datetime.datetime.now()\nExample value: datetime.datetime(2015, 6, 25...)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bc95951-d9f9-4190-90d1-c436f4b69747": {"__data__": {"id_": "9bc95951-d9f9-4190-90d1-c436f4b69747", "embedding": null, "metadata": {"page_label": "42", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f1ade06-2dd1-48d0-b7a3-bc36a540f6b8", "node_type": "4", "metadata": {"page_label": "42", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "3c24ae62751c98fcdbbe7682b752059aea650dc9cb5a00f37ce20a5b26d9980a", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 42 ]Given the list of fields, it's easy to modify and customize the PropertiesItem  class \nthat scrapy startproject  created for us. With our text editor, we modify the \nproperties/items.py  file until it contains the following:\nfrom scrapy.item import Item, Field\nclass PropertiesItem(Item):\n    # Primary fields\n    title = Field()\n    price = Field()\n    description = Field()\n    address = Field()\n    image_urls = Field()\n    # Calculated fields\n    images = Field()\n    location = Field()\n    # Housekeeping fields\n    url = Field()\n    project = Field()\n    spider = Field()\n    server = Field()\n    date = Field()\nSince this is essentially the first Python code we write in a file, it's important to mention \nthat Python uses indentation as part of its syntax. At the beginning of each field, there \nare exactly four spaces or one tab. This is important. If you start one line with four \nspaces and another with three you will get a syntax error. If you have four spaces in \none and a tab in another, that too will be a syntax error. Those spaces group the field \ndefinitions under the PropertiesItem  class. Other languages use curly braces ( {} ) or \nspecial keywords like begin - end  to group code, but Python uses spaces.\nWriting spiders\nWe are halfway there. Now we need to write a spider. Typically, there will be \none spider per website or a section of website if it's very large. A spider's code \nimplements the whole UR2IM process, as we will see in a moment.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b2ea6b3-f1c9-4363-bcb3-c4ef06e7c97f": {"__data__": {"id_": "0b2ea6b3-f1c9-4363-bcb3-c4ef06e7c97f", "embedding": null, "metadata": {"page_label": "43", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d56ea21-939a-4134-95be-abb3686a9fa2", "node_type": "4", "metadata": {"page_label": "43", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "03c67654c74f7b493529b9ed5f6801a2e03901f250c2c5b58b2061e2bcea8514", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 43 ]When do you use a spider and when do you use a project? A \nproject groups Items  and spiders. If you have many websites \nfrom which you extract the same type of Items , for example: \nproperties, then all those can be on a single project, and likely \nhave one spider for each source/website. On the other hand, you \nwould have different projects for sources with books and sources \nwith properties.\nOf course, you can create a new spider from scratch with a text editor, but it's better \nto save some typing by using the scrapy genspider  command as follows:\n$ scrapy genspider basic web\nCreated spider 'basic' using template 'basic' in module:\n  properties.spiders.basic\nNow if you rerun the tree  command, you will notice that the only thing that changed \nis that a new file basic.py  was added in your properties/spiders  directory. What \nthe preceding command did was to create a \"default\" spider with the name \"basic\" \nthat is restricted to crawl URLs on the web domain. We can remove this restriction \neasily if we want, but for now it's fine. This spider was created using the \"basic\" \ntemplate. You can see the other available templates by typing in scrapy genspider \n-l, and then creating spiders using any of those templates by using the -t parameter \nwhile doing scrapy genspider . We will see an example later in this chapter.\nScrapy has many subdirectories. We will always assume \nthat you are on the directory that contains a scrapy.cfg  \nfile. This is the \"top level\" directory for your project. Now \nwhenever we refer to Python \"packages\" and \"modules\", \nthey are set in such a way as to map the directory structure. \nFor example, the output mentions properties.spiders.\nbasic . This is the basic.py  file in the properties/\nspiders  directory. The class PropertiesItem  that \nwe defined earlier is on the properties.items  \nmodule, which corresponds to items.py  file inside the \nproperties  directory.\nIf we have a look at the properties/spiders/basic.py  file, we will see the \nfollowing code:\nimport scrapy\nclass BasicSpider(scrapy.Spider):\n    name = \"basic\"\n    allowed_domains = [\"web\"]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6964ace7-3441-4e1b-89a4-ba86ad81954e": {"__data__": {"id_": "6964ace7-3441-4e1b-89a4-ba86ad81954e", "embedding": null, "metadata": {"page_label": "44", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fb14a6b-6388-4877-b0bf-db13a8abf066", "node_type": "4", "metadata": {"page_label": "44", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f2a27d5e0a21cc10e4eb069cade6b5fa872be4f59262cc28db6bc15a09478136", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 44 ]    start_urls = (\n        'http://www.web/',\n    )\n    def parse(self, response):\n        pass\nThe import  statement allows us to use the existing Scrapy framework classes. \nAfter this, it's the definition of a BasicSpider  class that extends scrapy.Spider . \nBy 'extends' we mean that despite the fact that we didn't write any code, this class \nalready \"inherits\" quite some functionality from the Scrapy framework Spider  class. \nThis allows us to write very few extra lines of code and yet have a fully working \nspider. Then we see some parameters of the spider like its name and the domains \nthat we are allowed to crawl. Finally, we have the definition of an empty function \nparse()  that has a self  and a response  object as arguments. By using the self  \nreference, we can use interesting functionality of our spider. However, the other \nobject\u2014response  should be well familiar. It's exactly the same response  object that \nwe used to play with in the Scrapy shell.\nThis is your code\u2014your spider. Don't be afraid to modify \nit; you won't really break anything very badly. In the worst \ncase, you can always remove the file with rm properties/\nspiders/basic.py*  and regenerate it. Feel free to play.\nOkay, let's start hacking. First we will use the URL that we used with Scrapy shell by \nsetting start_urls  accordingly. Then we will use spider's predefined method log()  \nto output everything that we summarized in the primary fields table. The modified \ncode of properties/spiders/basic.py  will be as follows:\nimport scrapy\nclass BasicSpider(scrapy.Spider):\n    name = \"basic\"\n    allowed_domains = [\"web\"]\n    start_urls = (\n        'http://web:9312/properties/property_000000.html',\n    )\n    def parse(self, response):\n        self.log(\"title: %s\" % response.xpath(\n            '//*[@itemprop=\"name\"][1]/text()').extract())\n        self.log(\"price: %s\" % response.xpath(\n            '//*[@itemprop=\"price\"][1]/text()').re('[.0-9]+'))\n        self.log(\"description: %s\" % response.xpath(", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "404b88e0-0922-4a42-b267-bcf7f53fd1c4": {"__data__": {"id_": "404b88e0-0922-4a42-b267-bcf7f53fd1c4", "embedding": null, "metadata": {"page_label": "45", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bddd971a-2008-42ac-9747-8bc363c7aceb", "node_type": "4", "metadata": {"page_label": "45", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "78544b2e469ab8788c09b370742b71e7d2fbc9b16595dffb74eea68a4ba4b833", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 45 ]            '//*[@itemprop=\"description\"][1]/text()').extract())\n        self.log(\"address: %s\" % response.xpath(\n            '//*[@itemtype=\"http://schema.org/'\n            'Place\"][1]/text()').extract())\n        self.log(\"image_urls: %s\" % response.xpath(\n            '//*[@itemprop=\"image\"][1]/@src').extract())\nI'm going to modify the formatting every now and then to \nmake it fit nicely on the screen and on paper. It doesn't mean \nthat it has some particular meaning.\nAfter all this wait, it's high time we run our spider. We can do so using the command \nscrapy crawl  followed by the name of the spider:\n$ scrapy crawl basic\nINFO: Scrapy 1.0.3 started (bot: properties)\n...\nINFO: Spider opened\nDEBUG: Crawled (200) <GET http://...000.html>\nDEBUG: title: [u'set unique family well']\nDEBUG: price: [u'334.39']\nDEBUG: description: [u'website...']\nDEBUG: address: [u'Angel, London']\nDEBUG: image_urls: [u'../images/i01.jpg']\nINFO: Closing spider (finished)\n...\nExcellent! Don't get overwhelmed by the large number of log lines. We will examine \nsome of them more closely in a later chapter, but for now, just notice that all the data \nthat was collected using the XPath expressions actually got extracted with this simple \nspider code.\nLet's also play with another command\u2014 scrapy parse . It allows us to use the \"most \nsuitable\" spider to parse any URL given as an argument. I don't like to leave things to \nchance, so let's use it in conjunction with the --spider  parameter to set the spider:\n$ scrapy parse --spider=basic http://web:9312/properties/property_000001.\nhtml", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9f23c79-02c1-4ad3-b39c-9c0477632566": {"__data__": {"id_": "c9f23c79-02c1-4ad3-b39c-9c0477632566", "embedding": null, "metadata": {"page_label": "46", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06c61145-4bdd-4d04-8959-67ffc4fb5178", "node_type": "4", "metadata": {"page_label": "46", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "bf569de94c60a3d2a7c4300cb4587a8090ac80f1b3c943073c15b651c0a07593", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 46 ]You will see output similar to the previous one, but now for another property.\nscrapy parse  is also a tool for debugging and quite a handy \none. In any case, the main command if you need to do \"serious\" \nscrapping is scrapy crawl .\nPopulating an item\nWe will slightly modify the preceding code to populate PropertiesItem  items. As \nyou will see, the modification is going to be only slight, but it's going to \"unlock\" tons \nof new functionalities.\nFirst of all, we need to import the PropertiesItem  class. As we said earlier, this lies \nin the items.py  file in the properties directory, and thus, in the properties.items  \nmodule. We go back to our properties/spiders/basic.py  file and import it with \nthe following command:\nfrom properties.items import PropertiesItem\nThen we need to instantiate and return one. That's fairly simple. Inside our parse()  \nmethod, we add an item = PropertiesItem()  statement which creates a new item, \nand then we can assign expressions to its fields as follows:\nitem['title'] =  \nresponse.xpath('//*[@itemprop=\"name\"][1]/text()').extract()\nFinally, we return the item with return item . The updated code of properties/\nspiders/basic.py  looks like the following:\nimport scrapy\nfrom properties.items import PropertiesItem\nclass BasicSpider(scrapy.Spider):\n    name = \"basic\"\n    allowed_domains = [\"web\"]\n    start_urls = (\n        'http://web:9312/properties/property_000000.html',\n    )\n    def parse(self, response):\n        item = PropertiesItem()\n        item['title'] = response.xpath(\n            '//*[@itemprop=\"name\"][1]/text()').extract()\n        item['price'] = response.xpath(\n            '//*[@itemprop=\"price\"][1]/text()').re('[.0-9]+')\n        item['description'] = response.xpath(\n            '//*[@itemprop=\"description\"][1]/text()').extract()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9685144f-264c-4eae-8f2e-74ec1dc5533a": {"__data__": {"id_": "9685144f-264c-4eae-8f2e-74ec1dc5533a", "embedding": null, "metadata": {"page_label": "47", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecc9298c-fa21-413c-aca9-b403893a6a67", "node_type": "4", "metadata": {"page_label": "47", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7c4cc4af72eb3f06823aec06514050f52c26a2b9d1d668b0fc2f20c8aa87ff62", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 47 ]        item['address'] = response.xpath(\n            '//*[@itemtype=\"http://schema.org/'\n            'Place\"][1]/text()').extract()\n        item['image_urls'] = response.xpath(\n            '//*[@itemprop=\"image\"][1]/@src').extract()\n        return item\nNow if your run a scrapy crawl basic  as before, you will notice a slight but \nimportant difference. We are no longer logging the scraped values (so no DEBUG:  \nlines with field values). Instead, you will see the following line:\nDEBUG: Scraped from <200  \nhttp://...000.html>\n  {'address': [u'Angel, London'],\n   'description': [u'website ... offered'],\n   'image_urls': [u'../images/i01.jpg'],\n   'price': [u'334.39'],\n   'title': [u'set unique family well']}\nThis is a PropertiesItem  that got scraped from this page. This is great, because \nScrapy is built around the concept of Items , which means that you can now use \nthe pipelines we will present in later chapters to filter and enrich them, and \"Feed \nexports\" to export and save them on different formats and places.\nSaving to files\nTry, for example, the following crawls:\n$ scrapy crawl basic -o items.json\n$ cat items.json\n[{\"price\": [\"334.39\"], \"address\": [\"Angel, London\"], \"description\": \n[\"website court ... offered\"], \"image_urls\": [\"../images/i01.jpg\"], \n\"title\": [\"set unique family well\"]}]\n$ scrapy crawl basic -o items.jl\n$ cat items.jl\n{\"price\": [\"334.39\"], \"address\": [\"Angel, London\"], \"description\": \n[\"website court ... offered\"], \"image_urls\": [\"../images/i01.jpg\"], \n\"title\": [\"set unique family well\"]}\n$ scrapy crawl basic -o items.csv\n$ cat items.csv \ndescription,title,url,price,spider,image_urls...\n\"...offered\",set unique family well,,334.39,,../images/i01.jpg", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fdb5ea14-5a73-4454-ae38-41dc3b924c8b": {"__data__": {"id_": "fdb5ea14-5a73-4454-ae38-41dc3b924c8b", "embedding": null, "metadata": {"page_label": "48", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9aca6cf-b9c3-4718-a1ba-6f8422582f27", "node_type": "4", "metadata": {"page_label": "48", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "5d89eaa72bfa4ced62e66a52cc1f20c5056a04edb9d1e0ef7ef0bef0607af1e8", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 48 ]$ scrapy crawl basic -o items.xml\n$ cat items.xml \n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items><item><price><value>334.39</value></price>...</item></items>\nWithout us writing any extra code, we can save on all those different formats. What \nhappens behind the scenes is that Scrapy recognizes the file extension that you want \nto output, and exports the file in the appropriate format. The preceding formats \ncover some of the most common use cases. CSV and XML files are very popular, \nbecause spreadsheet programs like Microsoft Excel can open them directly. JSON \nfiles are very popular on the Web due to their expressiveness and close relationship \nto JavaScript. The slight difference between the JSON and the JSON Line format is \nthat the .json  files store the JSON objects in a large array. This means that if you \nhave such a file of 1 GB, you might have to store it all in the memory before you \nparse it with a typical parser. The .jl files on the other hand have one JSON object \nper line, so they can be read more efficiently.\nIt is also trivial to save your generated files in places other than your filesystem. By \nusing the following, for example, you will have Scrapy automatically upload the files \nfor you on an FTP or an S3 bucket:\n$ scrapy crawl basic -o \"ftp://user:pass@ftp.scrapybook.com/items.json \"\n$ scrapy crawl basic -o \"s3://aws_key:aws_secret@scrapybook/items.json\"\nNote that this example won't work unless the credentials and URLs are updated to \ncorrespond to your valid hosting/S3 provider.\nWhere is my MySQL driver? I was originally surprised by the lack \nof built-in support by Scrapy for MySQL or other databases. The \nfact is that there is nothing built-in, because it's fundamentally \nwrong for Scrapy's way of thinking. Scrapy is meant to be fast and \nscalable. It uses very little CPU and as much inbound bandwidth \nas possible. Inserting to most relational databases would be a \ndisaster from the perspective of performance. When you need to \ninsert your items to a database, you have to store them in files, \nand then import them using bulk load mechanisms. That said, \nin Chapter 9 , Pipeline Recipes,  we will see many efficient ways of \nimporting individual items in databases.\nOne more thing to notice is that if you try to use scrapy parse  now, it will show you \nthe scraped items and new requests (none in this case) that your crawl generated:\n$ scrapy parse --spider=basic http://web:9312/properties/property_000001.\nhtml", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0c5fde0-62e5-436f-b6ae-4f94114210a4": {"__data__": {"id_": "c0c5fde0-62e5-436f-b6ae-4f94114210a4", "embedding": null, "metadata": {"page_label": "49", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02197a79-7292-489c-a3e3-b093f58eb254", "node_type": "4", "metadata": {"page_label": "49", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "03f209e04017ffe2930b4b0e4c1cc5a0f391c8c9328b3c751f9186c50a64ed97", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 49 ]INFO: Scrapy 1.0.3 started (bot: properties)\n...\nINFO: Spider closed (finished)\n>>> STATUS DEPTH LEVEL 1 <<<\n# Scraped Items  ------------------------------------------------\n[{'address': [u'Plaistow, London'],\n  'description': [u'features'],\n  'image_urls': [u'../images/i02.jpg'],\n  'price': [u'388.03'],\n  'title': [u'belsize marylebone...deal']}]\n# Requests  ------------------------------------------------\n[]\nYou will appreciate scrapy parse  even more while debugging URLs that give \nunexpected results.\nCleaning up \u2013 item loaders and housekeeping \nfields\nCongratulations, you have done a great job in creating a basic spider! Let's make it a \nbit more professional-looking.\nWe start by using a great utility class, ItemLoader , in order to replace all those \nmessy looking extract()  and xpath()  operations. By using it, our parse()  method \nchanges to the following:\ndef parse(self, response):\n    l = ItemLoader(item=PropertiesItem(), response=response)\n    l.add_xpath('title', '//*[@itemprop=\"name\"][1]/text()')\n    l.add_xpath('price', './/*[@itemprop=\"price\"]'\n           '[1]/text()', re='[,.0-9]+')\n    l.add_xpath('description', '//*[@itemprop=\"description\"]'\n           '[1]/text()')\n    l.add_xpath('address', '//*[@itemtype='\n           '\"http://schema.org/Place\"][1]/text()')\n    l.add_xpath('image_urls', '//*[@itemprop=\"image\"][1]/@src')\n    return l.load_item()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e56240f-eef4-4a53-977a-daf93f9d851c": {"__data__": {"id_": "9e56240f-eef4-4a53-977a-daf93f9d851c", "embedding": null, "metadata": {"page_label": "50", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f01006f3-e81c-448b-aaf2-ace749dcbc12", "node_type": "4", "metadata": {"page_label": "50", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7cf11f80e711f29d999fc8672991c8f03a8b82b435a911a468e6ac9bcba13a34", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 50 ]Much better, isn't it? But it's actually a bit more than just being visually more \npleasant. It declares very clearly the intention of what we are trying to do without \nmessing it up with the details of implementation. This makes the code more \nmaintainable and self-documenting.\nItemLoaders  provide many interesting ways of combining data, formatting them, \nand cleaning them up. Note that they are very actively developed so check the \nexcellent documentation in http://doc.scrapy.org/en/latest/topics/loaders.\nhtml  to find the most efficient ways to use them. ItemLoaders  pass values from \nXPath/CSS expressions through different processor classes. Processors are fast yet \nsimple functions. An example of a processor is Join() . This processor, assuming that \nyou have selected multiple paragraphs with some XPath expression like //p, will \njoin their text together in a single entry. Another particularly interesting processor is \nMapCompose() . You can use it with any Python function or chain of Python functions \nto implement complex functionality. For example, MapCompose(float)  converts \nstring data to numbers, and MapCompose(unicode.strip, unicode.title)  gets \nrid of any excessive spaces and format strings with the first letter of each word \ncapitalized. Let's take a look at some examples of these processors:\nProcessor Functionality\nJoin() Concatenates multiple results into one.\nMapCompose(unicode.strip) Removes leading and trailing \nwhitespace characters.\nMapCompose(unicode.strip, unicode.\ntitle)Same as Mapcompose , but also gives \ntitle cased results.\nMapCompose(float) Converts strings to numbers.\nMapCompose(lambda i: i.replace(',', \n''), float)Converts strings to numbers, ignoring \npossible ',' characters.\nMapCompose(lambda i: urlparse.\nurljoin(response.url, i))Converts relative URLs to absolute \nURLs using response.url  as base.\nYou can use any Python expression as a processor. As you can see, it's easy to chain \nthem one after the other as we do, for example, with the strip and title-case example \ngiven previously. unicode.strip()  and unicode.title()  are simple in the sense \nthat they take a single argument and return a single result. We can use them directly \nin our MapCompose  processors. Other functions such as replace()  or urljoin()  \nare slightly more complex, and require multiple arguments. For those, we can use \nPython \"lambda expressions\". Lambda expressions are compact functions. For \nexample, the following compact lambda:\nmyFunction = lambda i: i.replace(',', '')", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25ad3d92-788b-4979-a823-e4ee9319131c": {"__data__": {"id_": "25ad3d92-788b-4979-a823-e4ee9319131c", "embedding": null, "metadata": {"page_label": "51", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b758595c-9fe9-4453-9f25-07e0f59cc78e", "node_type": "4", "metadata": {"page_label": "51", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "d16be9f7460b442a63a180134a03238404d0da6d081b9681d49ccafcf26dbac2", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 51 ]can be used instead of:\ndef myFunction(i):\n    return i.replace(',', '')\nBy using lambdas, we wrap functions like replace()  and urljoin()  to functions \nthat take a single argument and return a single result. To understand the processors \nof the previous table a little bit better, let's see a few examples of their usage. Open \nany URL with a scrapy shell , and try the following:\n>>> from scrapy.loader.processors import MapCompose, Join\n>>> Join()(['hi','John'])\nu'hi John'\n>>> MapCompose(unicode.strip)([u'  I',u' am\\n'])\n[u'I', u'am']\n>>> MapCompose(unicode.strip, unicode.title)([u'nIce cODe'])\n[u'Nice Code']\n>>> MapCompose(float)(['3.14'])\n[3.14]\n>>> MapCompose(lambda i: i.replace(',', ''), float)(['1,400.23'])\n[1400.23]\n>>> import urlparse\n>>> mc = MapCompose(lambda i: urlparse.urljoin('http://my.com/test/abc', \ni))\n>>> mc(['example.html#check'])\n['http://my.com/test/example.html#check']\n>>> mc(['http://absolute/url#help'])\n['http://absolute/url#help']\nThe key thing to take away is that processors are just simple and small functions that \npost-process our XPath/CSS results. Let's use a few such processors in our spider to \nshape its output exactly as we want:\ndef parse(self, response):\n    l.add_xpath('title', '//*[@itemprop=\"name\"][1]/text()',\n                MapCompose(unicode.strip, unicode.title))\n    l.add_xpath('price', './/*[@itemprop=\"price\"][1]/text()',\n                MapCompose(lambda i: i.replace(',', ''), float),\n                re='[,.0-9]+')\n    l.add_xpath('description', '//*[@itemprop=\"description\"]'\n                '[1]/text()', MapCompose(unicode.strip), Join())\n    l.add_xpath('address',", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0bc11349-1046-48de-a834-ed5a3d646e40": {"__data__": {"id_": "0bc11349-1046-48de-a834-ed5a3d646e40", "embedding": null, "metadata": {"page_label": "52", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8c4e4cc-3960-41b9-a068-cd651a991e71", "node_type": "4", "metadata": {"page_label": "52", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "2981dd1c7b975d70a83033653607a8763bcdc603105c01fb5c44b02115c2aa2d", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 52 ]                '//*[@itemtype=\"http://schema.org/Place\"][1]/text()',\n                MapCompose(unicode.strip))\n    l.add_xpath('image_urls', '//*[@itemprop=\"image\"][1]/@src',\n                MapCompose(\n                lambda i: urlparse.urljoin(response.url, i)))\nThe full listing is given a bit later in this chapter. If you run scrapy crawl basic  \nwith the code that we've developed up to now, you'll get far cleaner output values:\n'price': [334.39],\n'title': [u'Set Unique Family Well']\nFinally, we can add single values that we calculate with Python (instead of XPath/CSS \nexpressions) by using the add_value()  method. We can use it to set our \"housekeeping \nfields\"\u2014things like the URL, the spider name, timestamp, and so on. We directly use the \nexpressions summarized in the housekeeping fields table, as follows:\nl.add_value('url', response.url)\nl.add_value('project', self.settings.get('BOT_NAME'))\nl.add_value('spider', self.name)\nl.add_value('server', socket.gethostname())\nl.add_value('date', datetime.datetime.now())\nRemember to import datetime  and socket  in order to use some of those functions.\nThat's it! We have perfectly good looking Items . Now, I know that your first \nfeeling might be that this is all very complicated and you might be wondering if \nit's worth the effort. The answer is yes\u2014this is because more or less, this is all you \nneed to know in order to scrape everything in terms of extracting data from pages \nand storing them into items. This code typically, if written from scratch or in other \nlanguages, looks really ugly, and soon becomes unmaintainable. With Scrapy, \nit's just 25 lines of code and that's it. The code is clean, and indicates the intention \ninstead of implementation details. You know exactly what each line does, and it's \nstraightforward to modify, reuse, and maintain.\nAnother feeling you might have is that all those processors and ItemLoaders  \naren't worth the effort. If you are an experienced Python developer, it might feel a \nbit uncomfortable that you have to learn to use new classes for things you would \ntypically do with string operations, lambda expressions, and list comprehensions. \nStill, this was a brief introduction to ItemLoader  and its capabilities. If you dive a \nbit deeper, you will never look back. ItemLoaders  and processors are toolkits that \nwere developed based on the scraping needs of people who wrote and supported \nthousands of spiders. If you are planning to develop more than just a few spiders, it's \nworth learning how to use them.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36ac9d76-1c25-4d3a-aa74-63f1b6bccbce": {"__data__": {"id_": "36ac9d76-1c25-4d3a-aa74-63f1b6bccbce", "embedding": null, "metadata": {"page_label": "53", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0bced67-df54-4844-8c99-32c94a26e079", "node_type": "4", "metadata": {"page_label": "53", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e037ca1cfdaaeb0129695748de3e5d09decee0211d35400b10ee7fc3cae695ac", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 53 ]Creating contracts\nContracts are a bit like unit tests for spiders. They allow you to quickly know if \nsomething is broken. For example, let's assume that you wrote a scraper a few weeks \nago, and it had several spiders. You want to quickly check if everything is okay \ntoday. Contracts are included in the comments just after the name of a function \n(docstring), and they start with @. Let's look at the following contract for example:\ndef parse(self, response):\n    \"\"\" This function parses a property page.\n    @url http://web:9312/properties/property_000000.html\n    @returns items 1\n    @scrapes title price description address image_urls\n    @scrapes url project spider server date\n    \"\"\"\nThe preceding code says, \"check this URL and you should find one item with values \non those fields I enlist\". Now if you run scrapy check , it will go and check whether \nthe contracts are satisfied:\n$ scrapy check basic\n----------------------------------------------------------------\nRan 3 contracts in 1.640s\nOK\nIf it happens to leave the url field empty (by commenting out the line that sets it), \nyou get a descriptive failure:\nFAIL: [basic] parse (@scrapes post-hook)\n------------------------------------------------------------------\nContractFail: 'url' field is missing\nA contract might fail because either the spider code is broken, or some of the XPath \nexpressions are out-of-date with the URL you are checking against. Certainly, they \naren't exhaustive, but it's a very neat first line of defence against broken code.\nOverall, the following is the code for our first basic spider:\nfrom scrapy.loader.processors import MapCompose, Join\nfrom scrapy.loader import ItemLoader\nfrom properties.items import PropertiesItem\nimport datetime\nimport urlparse\nimport socket", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33e1c742-b479-4dd8-9490-d20d8eda4a82": {"__data__": {"id_": "33e1c742-b479-4dd8-9490-d20d8eda4a82", "embedding": null, "metadata": {"page_label": "54", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5591dfe6-336f-4aaf-9fbe-8803a4fdcc58", "node_type": "4", "metadata": {"page_label": "54", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "94fefdb4c1270f52495d8231fc446dfd40b212215b876980f11ef78f36e581e6", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 54 ]import scrapy\nclass BasicSpider(scrapy.Spider):\n    name = \"basic\"\n    allowed_domains = [\"web\"]\n    # Start on a property page\n    start_urls = (\n        'http://web:9312/properties/property_000000.html',\n    )\n    def parse(self, response):\n        \"\"\" This function parses a property page.\n        @url http://web:9312/properties/property_000000.html\n        @returns items 1\n        @scrapes title price description address image_urls\n        @scrapes url project spider server date\n        \"\"\"\n        # Create the loader using the response\n        l = ItemLoader(item=PropertiesItem(), response=response)\n        # Load fields using XPath expressions\n        l.add_xpath('title', '//*[@itemprop=\"name\"][1]/text()',\n                    MapCompose(unicode.strip, unicode.title))\n        l.add_xpath('price', './/*[@itemprop=\"price\"][1]/text()',\n                    MapCompose(lambda i: i.replace(',', ''),  \n                    float),\n                    re='[,.0-9]+')\n        l.add_xpath('description', '//*[@itemprop=\"description\"]'\n                    '[1]/text()',\n                    MapCompose(unicode.strip), Join())\n        l.add_xpath('address',\n                    '//*[@itemtype=\"http://schema.org/Place\"]'\n                    '[1]/text()',\n                    MapCompose(unicode.strip))\n        l.add_xpath('image_urls', '//*[@itemprop=\"image\"]'\n                    '[1]/@src', MapCompose(\n                    lambda i: urlparse.urljoin(response.url, i)))\n        # Housekeeping fields\n        l.add_value('url', response.url)\n        l.add_value('project', self.settings.get('BOT_NAME'))\n        l.add_value('spider', self.name)\n        l.add_value('server', socket.gethostname())\n        l.add_value('date', datetime.datetime.now())\n        return l.load_item()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f0f5697-016f-49fa-966c-87cd7a8c4c8f": {"__data__": {"id_": "3f0f5697-016f-49fa-966c-87cd7a8c4c8f", "embedding": null, "metadata": {"page_label": "55", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a31dff71-b5d6-4337-8b47-a4d535b4c5c5", "node_type": "4", "metadata": {"page_label": "55", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "4024feeb64832b2831c4c3d2df461641850331585fc6ae27eef05ebf2b729ad3", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 55 ]Extracting more URLs\nUp to now, we have been working with a single URL that we set in the spider's start_\nurls  property. Since that's a tuple, we can hardcode multiple URLs, for example:\nstart_urls = (\n    'http://web:9312/properties/property_000000.html',\n    'http://web:9312/properties/property_000001.html',\n    'http://web:9312/properties/property_000002.html',\n)\nNot that exciting. We might also use a file as the source of those URLs as follows:\nstart_urls = [i.strip() for i in  \nopen('todo.urls.txt').readlines()]\nThis is not very exciting either, but it certainly works. What will happen more often \nthat not is that the website of interest will have some index pages and some listing \npages. For example, Gumtree has the following index pages: http://www.gumtree.\ncom/flats-houses/london :\nGumtree's index page", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56a825aa-8929-49ae-868d-e3c83254d5c3": {"__data__": {"id_": "56a825aa-8929-49ae-868d-e3c83254d5c3", "embedding": null, "metadata": {"page_label": "56", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "032c09ea-7615-4c94-9857-1a0f998a0d2f", "node_type": "4", "metadata": {"page_label": "56", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7a3054c1e2574e7625ee83d67b875aea4c927f35b153ba747460496ee4a498aa", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 56 ]A typical index page will have links to many listing pages, and a pagination system \nthat allows you to move from one index page to the other.\nA typical crawler moves in two directions\nAs a result, a typical crawler moves in two directions:\n\u2022 Horizontally\u2014from an index page to another\n\u2022 Vertically\u2014from an index page to the listing pages to extract Items\nIn this book, we call the former horizontal crawling , since it crawls pages at the \nsame hierarchical level (for example, indices), and the latter vertical crawling , since \nit moves from a higher hierarchical level (for example, indices) to a lower one (for \nexample, listings).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f0e4379-7ed0-4d0c-b569-43d18a23e0a3": {"__data__": {"id_": "5f0e4379-7ed0-4d0c-b569-43d18a23e0a3", "embedding": null, "metadata": {"page_label": "57", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "265dc7fd-e5e9-42a2-8d38-8ab7264c27db", "node_type": "4", "metadata": {"page_label": "57", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b0de1c54deebfd3534023ce76ea1c01163808964988b2b978de45320c0766293", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 57 ]It's all easier than it sounds. All we need is two more XPath expressions. For the \nfirst expression, we right-click on the Next page  button, and we notice that the \nURL is contained in a link inside a li that has the class name next . As a result, the \nconvenient XPath expression //*[contains(@class,\"next\")]//@href  will work \nlike a charm.\nFinding the next index page URL XPath expression\nFor the second expression, we right-click and Inspect Element  on the title of a listing \nin the page:\nFinding the listing page URL XPath expression", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b007f94-19cf-45ef-a92e-0ece64151a9a": {"__data__": {"id_": "1b007f94-19cf-45ef-a92e-0ece64151a9a", "embedding": null, "metadata": {"page_label": "58", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db6741ce-55ee-420d-bb37-8414013e6fc3", "node_type": "4", "metadata": {"page_label": "58", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7b3906ff204e79f42bd08f194764cbd71112763e9745e43789e7b19b5c7b43a1", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 58 ]Notice that the URL has an interesting itemprop=\"url\"  attribute. As a result, //*[@\nitemprop=\"url\"]/@href  should work. Let's open a scrapy shell  to confirm both:\n$ scrapy shell http://web:9312/properties/index_00000.html\n>>> urls = response.xpath('//*[contains(@class,\"next\")]//@href').\nextract()\n>>> urls\n[u'index_00001.html']\n>>> import urlparse\n>>> [urlparse.urljoin(response.url, i) for i in urls]\n[u'http://web:9312/scrapybook/properties/index_00001.html']\n>>> urls = response.xpath('//*[@itemprop=\"url\"]/@href').extract()\n>>> urls\n[u'property_000000.html', ... u'property_000029.html']\n>>> len(urls)\n30\n>>> [urlparse.urljoin(response.url, i) for i in urls]\n[u'http://..._000000.html', ... /property_000029.html']\nExcellent! We can see that by using what we have already learned and the two XPath \nexpressions, we are able to extract the URLs that we need to do both horizontal and \nvertical crawling.\nTwo-direction crawling with a spider\nWe will copy our previous spider to a new one named manual.py :\n$ ls\nproperties  scrapy.cfg\n$ cp properties/spiders/basic.py properties/spiders/manual.py\nIn properties/spiders/manual.py , we import Request  by adding from scrapy.\nhttp import Request , change the spider's name  to 'manual' , change start_urls  \nto use our first index page, and rename the parse()  method to parse_item() . \nGreat! We are now ready to write a new parse()  method that will perform both \nhorizontal and vertical crawling:\ndef parse(self, response):\n    # Get the next index URLs and yield Requests\n    next_selector = response.xpath('//*[contains(@class,'\n                                   '\"next\")]//@href')\n    for url in next_selector.extract():", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f67156c-43ad-4fb9-8824-b011de53a1b8": {"__data__": {"id_": "6f67156c-43ad-4fb9-8824-b011de53a1b8", "embedding": null, "metadata": {"page_label": "59", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "907c1d0c-b994-4985-a2ec-fa6898e493e6", "node_type": "4", "metadata": {"page_label": "59", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "963b1776677f024636bb5e257b70bc564e28356f92ad03ff082c1000732d7daf", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 59 ]        yield Request(urlparse.urljoin(response.url, url))\n    # Get item URLs and yield Requests\n    item_selector = response.xpath('//*[@itemprop=\"url\"]/@href')\n    for url in item_selector.extract():\n        yield Request(urlparse.urljoin(response.url, url),\n                      callback=self.parse_item)\nYou might have noticed the yield  statement in the previous example. The \nyield  is a bit like return  in the sense that it returns values to the caller. \nHowever, in contrast to return , it doesn't exit the function, but continues \nwith the for loop instead. Functionally, the preceding example is roughly \nequivalent to the following:\nnext_requests = []\nfor url in...\n    next_requests.append(Request(...))\nfor url in...\n    next_requests.append(Request(...))\nreturn next_requests\nThe yield  is a piece of Python \"magic\" that makes coding efficient \nroutines easy.\nWe are now ready to run it. If you let this spider run though, it's going to scrape the \nentire 50k pages of the website. In order to avoid running for too long, we can tell \nthe spider to stop after a specific (for example, 90) number of items by using this \ncommand line parameter: -s CLOSESPIDER_ITEMCOUNT=90  (more details on those \nsettings are given in Chapter 7 , Configuration and Management ). We can now run it:\n$ scrapy crawl manual -s CLOSESPIDER_ITEMCOUNT=90\nINFO: Scrapy 1.0.3 started (bot: properties)\n...\nDEBUG: Crawled (200) <...index_00000.html> (referer: None)\nDEBUG: Crawled (200) <...property_000029.html> (referer: ...index_00000.\nhtml)\nDEBUG: Scraped from <200 ...property_000029.html>\n  {'address': [u'Clapham, London'],\n   'date': [datetime.datetime(2015, 10, 4, 21, 25, 22, 801098)],\n   'description': [u'situated camden facilities corner'],\n   'image_urls': [u'http://web:9312/images/i10.jpg'],\n   'price': [223.88],\n   'project': ['properties'],", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fc26ba3-d906-41d5-bfd8-dbe93f14b7d4": {"__data__": {"id_": "1fc26ba3-d906-41d5-bfd8-dbe93f14b7d4", "embedding": null, "metadata": {"page_label": "60", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e8e16ce-9737-4e0d-ae5c-460e14a4b254", "node_type": "4", "metadata": {"page_label": "60", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "252d69610bab69e1e40b28404064556c482b9192c8a0c4065b9a6a68493db5ec", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 60 ]   'server': ['scrapyserver1'],\n   'spider': ['manual'],\n   'title': [u'Portered Mile'],\n   'url': ['http://.../property_000029.html']}\nDEBUG: Crawled (200) <...property_000028.html> (referer: ...index_00000.\nhtml)\n...\nDEBUG: Crawled (200) <...index_00001.html> (referer: ...)\nDEBUG: Crawled (200) <...property_000059.html> (referer: ...)\n...\nINFO: Dumping Scrapy stats: ...\n   'downloader/request_count': 94, ...\n   'item_scraped_count': 90,\nIf you take a look at the preceding output, you will observe that we get both \nhorizontal and vertical crawling. First index_00000.html  is read, and then it spawns \nmany Requests . As they get executed, the debug messages indicate who initiated the \nRequest  with the referer  URL. We can see, for example, that property_000029.\nhtml , property_000028.html ...  and index_00001.html  have the same referer  \n(index_00000.html ). Then, property_000059.html  and others get index_00001.\nhtml  as referer , and the process continues.\nAs we observed in the example, Scrapy uses a last in, first out  (LIFO ) strategy for \nprocessing requests (depth first crawl). The last request you submit will be processed \nfirst. This default is convenient for most of our cases. For example, we like processing \neach listing page before moving to the next index page. Otherwise, we would fill \na huge queue of pending listing page URLs that would just consume memory. \nAdditionally, in many cases you might need auxiliary Requests  to complete a single \nRequest , as we will see in a later chapter. You need those auxiliary Requests  to be \ncompleted as soon as possible to free up the resources and have a steady flow of \nscraped items.\nWe can modify the default order by setting the priority Request()  argument to \na value greater than 0 to indicate a higher-than-default priority, or less than 0 to \nindicate a lower-than-default priority. In general, the Scrapy scheduler will execute \nhigher priority requests first, but don't spend much time thinking about the exact \nrequest that should be executed first. Its highly likely that you won't use more than \none or two request priority levels in most of your applications. Notice also that URLs \nare subject to duplication filtering, which is most often what we want. If we need \nto perform a request to the same URL more than once, we can set the dont_filter \nRequest()  argument to true .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b4386a2-945e-4096-85ce-21d6cc3a574b": {"__data__": {"id_": "4b4386a2-945e-4096-85ce-21d6cc3a574b", "embedding": null, "metadata": {"page_label": "61", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc0ace0f-4425-41aa-ad09-94b82b6b300c", "node_type": "4", "metadata": {"page_label": "61", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a79917acc9da5e0c63cb9513899a9e849a7c1e4e393bd94add8a8b6b0dab4c16", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 3[ 61 ]Two-direction crawling with a CrawlSpider\nIf you felt that this two-direction crawling was a bit too tedious, then you are \nreally getting it. Scrapy tries to simplify all those very common cases, and makes \nthem easier to code. The easiest way to achieve the same results is by using a \nCrawlSpider , a class that allows easier implementation of such crawls. To do so, we \nwill use the genspider  command, setting a -t crawl  parameter in order to create a \nspider using the crawl  spider template:\n$ scrapy genspider -t crawl easy web\nCreated spider 'crawl' using template 'crawl' in module:\n  properties.spiders.easy\nNow the file properties/spiders/easy.py  contains the following:\n...\nclass EasySpider(CrawlSpider):\n    name = 'easy'\n    allowed_domains = ['web']\n    start_urls = ['http://www.web/']\n    rules = (\n        Rule(LinkExtractor(allow=r'Items/'),  \ncallback='parse_item', follow=True),\n    )\n    def parse_item(self, response):\n        ...\nIf you see the auto-generated code, it looks similar to the previous spiders but in this \ncase in the class definition, this spider derives from CrawlSpider  instead of Spider. \nCrawlSpider  provides an implementation of the parse()  method that uses the \nrules  variable to do exactly what we did manually in the previous example.\nYou might be wondering why I provided the manual \nversion first instead of the shortcut. What you learned on \nthe manual example, yield 'ing Requests  with callbacks, \nis such a useful and fundamental technique that we will use \nagain and again in the following chapters, so understanding \nit is well worth the effort.\nWe will now set start_urls  to our first index page, and replace the predefined \nparse_item()  method with our previous implementation. We won't implement any \nparse()  method this time. We will replace the predefined rules  variable instead \nwith two rules, one for horizontal and one for vertical crawling:\nrules = (", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23c78c0b-cc84-4a69-ba09-e15c701078f8": {"__data__": {"id_": "23c78c0b-cc84-4a69-ba09-e15c701078f8", "embedding": null, "metadata": {"page_label": "62", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04f208d3-930b-4487-904f-35de70308c10", "node_type": "4", "metadata": {"page_label": "62", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "0cbbddfb04cd6ca5e3fa0f562002fc3357fa5eff1ce76929559d6e07c01d3fae", "class_name": "RelatedNodeInfo"}}, "text": "Basic Crawling[ 62 ]     \nRule(LinkExtractor(restrict_xpaths='//*[contains(@class,\"next\")]')),\nRule(LinkExtractor(restrict_xpaths='//*[@itemprop=\"url\"]'),\n         callback='parse_item')\n)\nThose two rules use the same XPath expressions we used in the manual example, \nbut without the a or href  constraints. As their name implies, LinkExtractors \nare specialized in extracting links, so by default, they are looking for the a (and \narea ) href  attributes. You can customize this by setting the tags  and attrs \nLinkExtractor() 's arguments. Also note that callbacks are now strings containing \nthe callback method name (for example 'parse_item' ) in contrast to method \nreferences, as was the case for Requests(self.parse_item) . Finally, unless \ncallback  is set, a Rule  will follow the extracted URLs, which means that it will scan \ntarget pages for extra links and follow them. If a callback  is set, the Rule  won't \nfollow the links from target pages. If you would like it to follow links, you should \neither return /yield  them from your callback  method, or set the follow  argument \nof Rule()  to true . This might be useful when your listing pages contain both Items  \nand extra useful navigation links.\nYou can run this spider and get exactly the same results as with the manual one, but \nnow with an even simpler source code:\n$ scrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=90\nSummary\nThis is probably the most important chapter for everyone starting with Scrapy. You \njust learned the basic methodology of developing spiders: UR2IM. You learned how \nto define custom Items  that fit our needs, use ItemLoaders , XPath expressions \nand processors to load Items , and how to yield Requests . We used Requests  to \nnavigate horizontally across multiple index pages and vertically towards listing \npages to extract Items . Finally, we saw how CrawlSpider  and Rules  can be used to \ncreate very powerful spiders with even less lines of codes. Please feel free to read this \nchapter as many times as you want to get a deeper understanding of the concepts, \nand of course, use it as a reference as you develop your own spiders.\nWe just got some information out of a website. Why is it such a big deal? I think the \nanswer will become obvious in the next chapter where in just a few pages, we are \ngoing to develop a simple mobile app, and use Scrapy to populate it with data. The \nresult, I think, is impressive.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2417, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d53a1acd-f317-425c-ad58-cfd97da8bbfe": {"__data__": {"id_": "d53a1acd-f317-425c-ad58-cfd97da8bbfe", "embedding": null, "metadata": {"page_label": "63", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8fcb4c7-820c-4627-8a61-c90a3d2474c9", "node_type": "4", "metadata": {"page_label": "63", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a13ed662f88ae06a86c7ae87514decd5be8e173ae090d4d2b6611d2bc82fb90e", "class_name": "RelatedNodeInfo"}}, "text": "[ 63 ]From Scrapy to a Mobile App\nI can hear people screaming, \"What does Appery.io, a proprietary platform for \nmobile applications, have to do with Scrapy?\" Well, seeing is believing. Showing \nsomeone (a friend, manager, or customer) your data on an Excel spreadsheet may \nhave impressed them a few years ago. Nowadays, unless your audience is quite \nsophisticated, their expectations will likely be quite different. In the next few pages, \nyou will see a simple mobile app, a minimum viable product, being built with just \na few clicks. Its aim is to communicate clearly to your stakeholders the power of \nthe data that you are extracting, and to demonstrate bringing value back to the \necosystem in the form of web traffic for the source website.\nI will try to keep the motivating examples short, and they are here to show you ways \nto make the most out of your data. Unless you have a specific application that will \nconsume your data, in which case you can safely skip this chapter, this chapter will \nshow you how to make your data available to the public in the most popular way \ntoday\u2014a mobile application.\nChoosing a mobile application framework\nFeeding data into a mobile application is quite easy if you use the appropriate \ntools. There are many good frameworks for cross-platform mobile application \ndevelopment, such as PhoneGap, Appcelerator with Appcelerator Cloud Services, \njQuery Mobile, and Sencha Touch.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "482c9d24-58d7-4a79-99e6-fde5309da079": {"__data__": {"id_": "482c9d24-58d7-4a79-99e6-fde5309da079", "embedding": null, "metadata": {"page_label": "64", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aeaba038-126b-481d-9977-6e49ca2805ab", "node_type": "4", "metadata": {"page_label": "64", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "0e2e9fa47b1c506824d544372c9c976025f21ca571fc86d13e47a8feb306934d", "class_name": "RelatedNodeInfo"}}, "text": "From Scrapy to a Mobile App[ 64 ]In this chapter, we will use Appery.io because it allows us to build iOS, Android, \nWindows Phone, and HTML5 mobile apps quickly using PhoneGap and jQuery \nMobile. There is no affiliation between me or Scrapy and Appery.io. I encourage you \nto conduct your own independent research to see whether it fits your needs beyond \nwhat I present in this chapter. Keep in mind that it's a paid service with a 14-day \ntrial but with a price that, in my opinion, makes it a no-brainer to develop a quick \nprototype, especially for someone who isn't a web expert. The main reason that I chose \nthis service is because it provides both mobile and backend services, which means \nthat we won't have to configure databases, write REST APIs, or have to use potentially \ndifferent languages for the server and the mobile application. As you will see, we won't \nhave to write any code at all! We will use their online tools; but at any point, you can \ndownload the app as a PhoneGap project and use the full range of PhoneGap features.\nYou will need an Internet connection in order to use Appery.io in this chapter. \nAlso, please note that the layout of their website may change in the future. Use our \nscreenshots as a guide but don't be surprised if their site doesn't look identical.\nCreating a database and a collection\nThe first step is to sign up to the free Appery.io plan by clicking on the Sign-Up  \nbutton on Appery.io and choosing the free plan. You will need to provide a name, \ne-mail address, and a password after which your new account is created. Give it a \nfew seconds until the account gets activated. Then you will be able to log in to the \nAppery.io dashboard. You are ready to create a new database and collection:\nCreating a new database and collection with Appery.io", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db7e80c8-4dcf-43e2-814a-4cd21a1db3bb": {"__data__": {"id_": "db7e80c8-4dcf-43e2-814a-4cd21a1db3bb", "embedding": null, "metadata": {"page_label": "65", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3277a520-1d7c-49c3-bc2f-f04e78b01107", "node_type": "4", "metadata": {"page_label": "65", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "6e2f6a250f54c8b6aa0125021a0c9732ab14ec802b3157e455ce96ba6c917287", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 4[ 65 ]In order to do so, please follow these steps:\n1. Click on the Databases  tab (1).\n2. Then click the green Create new database  (2) button. Name the new database \nscrapy  (3).\n3. Now, click the Create  button (4). This opens the Scrapy database's dashboard \nautomatically, and here, you can create a new collection.\nA database is a set of collections in Appery.io terminology. An application,  \nroughly speaking, uses a single database (at least initially), which will have many \ncollections, for example users, properties, messages, and so on. Appery.io already \nhas a Users  collection for us that holds usernames and passwords (they power lots  \nof its built-in functionality).\nCreating a new database and collection with Appery.io", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90c0e58c-af8c-4c6b-9785-49f713fce8fd": {"__data__": {"id_": "90c0e58c-af8c-4c6b-9785-49f713fce8fd", "embedding": null, "metadata": {"page_label": "66", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a72a4de0-7137-4a59-beb1-74ae324df776", "node_type": "4", "metadata": {"page_label": "66", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "3a5afc3fb980b952271edb75d9ea9086c2fa002f38f2a41100f60047e0f9e054", "class_name": "RelatedNodeInfo"}}, "text": "From Scrapy to a Mobile App[ 66 ]Let's add a user with the username, root, and a password, pass. Obviously, feel free \nto choose something more secure. You can do that by clicking on the Users  collection \non the sidebar (1), and then add a user/row by clicking +Row  (2). You fill in the \nusername and password on the two fields that just appeared (3, 4).\nWe will also create a new collection for the properties that we scrape with Scrapy, \nand we will name it properties. We create a new collection by clicking on the green \nCreate new collection  button (5), name it properties  (6), and click the Add  button \n(7). Now, we have to customize this collection a bit. We click on +Col  to add columns \n(8). Columns have types that help validate values. Most of our fields are simple \nstrings with the exception of price that is a number. We will add a few columns by \nclicking on +Col (8), filling in the name of the column (9), the type if it's not string \n(10), and then clicking on the Create column  button (11). We will repeat this process \nfive times to create the table that in shown in the following image:\nColumn title price description url image_urls\nType string number string string string\nBy the end of this collection, you should have the columns that you require, and it will \nlook like the preceding image. We are now ready to import some data from Scrapy.\nPopulating the database with Scrapy\nFirst of all, we will need one single number and that's the API key. We can find it in \nthe Settings  tab (1). We can copy it (2) and then go back to our properties collection \nby clicking on the Collections  tab (3):\nCreating a new database and collection with Appery.io", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1679, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8bc582d-1990-4c2b-bd28-b6dd09e6f6c4": {"__data__": {"id_": "e8bc582d-1990-4c2b-bd28-b6dd09e6f6c4", "embedding": null, "metadata": {"page_label": "67", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1ed563c-14c7-435b-aa8d-d9e2eaed4e62", "node_type": "4", "metadata": {"page_label": "67", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "744aff158314b3a8b72457b63544310cce6b85d0ad736480f22ce1dad155f056", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 4[ 67 ]Great! Now, let's modify the application that we created in the previous chapter to \nimport the data in Appery.io. We start by copying our project and our spider named \neasy  (easy.py ) to a spider named tomobile  (tomobile.py ). We also edit the file to \nset its name to tomobile :\n$ ls\nproperties  scrapy.cfg\n$ cat properties/spiders/tomobile.py\n...\nclass ToMobileSpider(CrawlSpider):\n    name = 'tomobile'\n    allowed_domains = [\"scrapybook.s3.amazonaws.com\"]\n    # Start on the first index page\n    start_urls = (\n        'http://scrapybook.s3.amazonaws.com/properties/'\n        'index_00000.html',\n    )\n...\nThe code from this chapter is in the ch04  directory \non GitHub.\nOne caveat you may have just noticed is that we don't use our web server ( http://\nweb:9312 ) as we did in the previous chapter. We use a publicly available copy of the \nsite that I keep on http://scrapybook.s3.amazonaws.com . Using it, exceptionally \nin this chapter, our images and URLs are publicly available, which allows us to share \nour app effortlessly.\nWe will use an Appery.io pipeline to insert the data. Scrapy pipelines are typically \nsmall Python classes that postprocess, clean, and store Scrapy items. We will discuss \nthem in depth in Chapter 8 , Programming Scrapy . For now, you can install it with \neasy_install  or pip, but if you use our Vagrant dev machine, you don't need to do \nanything because it's already installed:\n$ sudo easy_install -U scrapyapperyio\nor\n$ sudo pip install --upgrade scrapyapperyio", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cea2e09b-76f2-4447-8218-23dcf2ed48f7": {"__data__": {"id_": "cea2e09b-76f2-4447-8218-23dcf2ed48f7", "embedding": null, "metadata": {"page_label": "68", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1259d0c-71a6-434f-8da3-1904ce7961a6", "node_type": "4", "metadata": {"page_label": "68", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f3f8fe7b10466c996732e2309204535fdfd09e613d68634cf75ca282d980fb1e", "class_name": "RelatedNodeInfo"}}, "text": "From Scrapy to a Mobile App[ 68 ]At this point, you will have to modify a little bit of the main Scrapy settings file to \nadd the API key that you copied earlier. We are going to discuss settings in depth \nin Chapter 7 , Configuration and Management . For now, all we need to do is append the \nfollowing lines in properties/settings.py :\nITEM_PIPELINES = {'scrapyapperyio.ApperyIoPipeline': 300}\nAPPERYIO_DB_ID = '<<Your API KEY here>>'\nAPPERYIO_USERNAME = 'root'\nAPPERYIO_PASSWORD = 'pass'\nAPPERYIO_COLLECTION_NAME = 'properties'\nDon't forget to replace the APPERYIO_DB_ID  with the API key. Also make sure that \nyour settings have the same username and password as the one that you used when \nyou created a database user in Appery.io. To start filling up Appery.io's database \nwith data, start a Scrapy crawl as usual:\n$ scrapy crawl tomobile -s CLOSESPIDER_ITEMCOUNT=90\nINFO: Scrapy 1.0.3 started (bot: properties)\n...\nINFO: Enabled item pipelines: ApperyIoPipeline\nINFO: Spider opened\n...\nDEBUG: Crawled (200) <GET https://api.appery.io/rest/1/db/login?username=\nroot&password=pass>\n...\nDEBUG: Crawled (200) <POST https://api.appery.io/rest/1/db/collections/\nproperties>\n...\nINFO: Dumping Scrapy stats:\n  {'downloader/response_count': 215,\n   'item_scraped_count': 105,\n  ...}\nINFO: Spider closed (closespider_itemcount)\nThe output this time is slightly different. You can see the ApperyIoPipeline  item \npipeline getting enabled in one of the first few lines; but most notably, you will \nnotice that for about 100 items scrapped, there were about 200 requests/responses. \nThis is because the Appery.io pipeline makes an extra request per item to the \nAppery.io servers in order to write each item. These requests also appear in the logs \nwith an api.appery.io  URL.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b94d391-66e1-4448-8da9-2158ee8da54b": {"__data__": {"id_": "8b94d391-66e1-4448-8da9-2158ee8da54b", "embedding": null, "metadata": {"page_label": "69", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf6962ae-3762-4ff0-b100-7155941d0796", "node_type": "4", "metadata": {"page_label": "69", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "785c3d8e4a072b578bf3f95a1ec6ebe6164a18118e3032ce8bf20fd80e7d5930", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 4[ 69 ]\nThe properties collection is filled in with data\nIf we head back to Appery.io, we will see the properties  collection (1) filled in  \nwith data (2).\nCreating a mobile application\nStaring a new mobile application is trivial. We just click on the Apps  tab (1) and then \nthe Create new app  green button (2). We will name this application properties  (3) \nand click the Create  button (4) to create it:\nCreating a new mobile application and a database connection\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc84e1a2-e4e0-4b29-ac40-3b557def75f3": {"__data__": {"id_": "cc84e1a2-e4e0-4b29-ac40-3b557def75f3", "embedding": null, "metadata": {"page_label": "70", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b604ca9-42f9-4a4a-9d21-fb4b8440e738", "node_type": "4", "metadata": {"page_label": "70", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b88f118e679578535a6745ea7692f07e901477758a7d5211c6415c32937be1c1", "class_name": "RelatedNodeInfo"}}, "text": "From Scrapy to a Mobile App[ 70 ]Creating a database access service\nThe number of options when you start the new application may be a bit \noverwhelming. Using the Appery.io application editor, one can write complex \napplications, but for our purposes, we will keep things simple. What we are looking \nfor, to start with, is creating a service that gives us access to the Scrapy database from \nour application. In order to do this, we click on the square green CREATE NEW  \nbutton (5), and then we select Database Services  (6). A new dialog box appears that \nlets us chose where we want to connect to. We select the scrapy  database (7). We \nwon't use most of the options in this menu bar but just click to expand the properties  \nsection (8) and then select List (9). Behind the scenes, this writes code for us that \nmakes the data that we crawled with Scrapy available on the web. We finish by \nclicking the Import selected services  button (10).\nSetting up the user interface\nTake a deep breath. We are now going to create all the visual elements of our app. \nWe will work within the DESIGN  tab of their editor:\nSetting up the user interface", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d99d9c8-1639-44d5-8dcd-59587a13694e": {"__data__": {"id_": "7d99d9c8-1639-44d5-8dcd-59587a13694e", "embedding": null, "metadata": {"page_label": "71", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "730842b9-95f3-4ef6-9aea-37f0bb205b1b", "node_type": "4", "metadata": {"page_label": "71", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c284040afb624cbcaa306fc7cc0adb08fee0c50c6bae89b1aaf9b154ff118437", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 4[ 71 ]From the tree on the left of the page, we expand the Pages  folder (1) and then click \non startScreen  (2). The UI editor will open this page, and we can add a few controls. \nLet's edit the title to familiarize ourselves a bit with the editor. Click on the caption \nheader (3), and then you will notice that the properties section on the right of the \nscreen is being updated to show the header's properties, among which is the Text  \nproperty. Change that to Scrapy App . You will see the header in the middle screen \nupdating accordingly.\nThen, we will add a grid component. To do this, drag and drop a Grid  control from \nthe left palette (5). You will notice that it has two rows. We only need one row for \nour purposes; select the grid that we just added. You will know that the grid is \nselected when it's gray on the thumbnails section at the top of the mobile view (6). \nIf it isn't, click on it in order to select it. Then the properties on the right side will be \nupdated with grid's properties. Just edit the Rows  property and set it to 1 and then \nclick Apply  (7, 8). Now, the grid will be updated to have only one row.\nFinally drag and drop a few more controls inside the grid. First add an image control \non the left side of the grid (9), then a link on the right side of the grid (10), and finally, \na label just under the link (11).\nThat's more than enough in terms of layout. We will now feed data from the \ndatabase to the user interface.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76752ad2-6966-4150-a02b-b3bdff4bcec3": {"__data__": {"id_": "76752ad2-6966-4150-a02b-b3bdff4bcec3", "embedding": null, "metadata": {"page_label": "72", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12c875a7-0ba4-4910-b9a0-837d53d3bfb4", "node_type": "4", "metadata": {"page_label": "72", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f9dc85bd6ee993e09eeedbc2d487ff1329601915caa2dc4803a823c06adae4cc", "class_name": "RelatedNodeInfo"}}, "text": "From Scrapy to a Mobile App[ 72 ]Mapping data to the User Interface\nUntil now, we've spent most of our time in the DESIGN  tab setting up the visuals  \nof our application. In order to link available data to controls, we switch to the  \nDATA  tab (1):\nMapping data to the user interface\nWe select Service  (2) as the data source type. The service that we created previously \nis the only one available and gets automatically selected. so we can proceed to \nclicking the Add  button (3). The service properties will be listed just below it. As \nsoon as you press the Add  button, you will notice events, such as Before send  and \nSuccess . We will customize what happens when a call to the service succeeds by \nclicking on the Mapping  button that is next to Success .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e256b78a-4ac8-4d50-89ff-6638b6bddf53": {"__data__": {"id_": "e256b78a-4ac8-4d50-89ff-6638b6bddf53", "embedding": null, "metadata": {"page_label": "73", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3a9a72e-b45a-478a-9d4e-c38342d2494b", "node_type": "4", "metadata": {"page_label": "73", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f1bd580de5e9c08909a73c5ebc966089719339782fcb0bbd81500445eeba3229", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 4[ 73 ]The Mapping action editor  opens, and this is where we will do all our wiring. This \neditor has two sides. On the left are the fields available in the service's responses. \nOn the right, you will find the properties for the UI controls that we added in the \nprevious step. You will notice an Expand all  link on both sides. Click on it to see all \nthe data and controls that are available. You then need to perform the following five \nmappings (5) by dragging and dropping from the left side to the right:\nResponse Component Property Notes\n$[i] mobilegrid_2 This makes a for loop that creates and sets \nup each row.\ntitle mobilelink_8 Text This sets the text for the link.\nprice mobilelabel_9 Text This sets the price on the text field.\nimage_\nurlsmobileimage_7 Asset This loads the image from the URL on the \nimage container.\nurl mobilelink_8 URL This sets the URL for a link. When the user \nclicks on it, the associated page loads.\nMappings between database fields and User \nInterface controls\nThe numbers on the preceding table may be slightly different in your case, but as \nthere's only one of each type of control, the odds of something going wrong are \nminimal. By setting these mappings, we tell Appery.io to write all the code behind \nthe scenes that will load the controls with values when the database query succeeds. \nYou can then click on Save and return  (6) to continue.\nThis gets us back to the DATA  tab. We need to return to the UI editor, so we click \non the DESIGN  tab (7). On the lower part of the screen, you will notice an EVENTS  \nsection (8) that was always there but has just been expanded. With the EVENTS  \nsection, we make Appery.io do things as responses to UI events. This brings us to \nthe final step that we need to perform. This is to make our app call the service and \nretrieve data as soon as the UI loads. In order to do so, we choose startScreen  as a \ncomponent; we keep the default Load  option for the event. We then select Invoke \nservice  as an action  and leave  Datasource  as the default restservice1  option (9).  \nWe click Save  (10), and that's all we had to do for this mobile application.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6f7a3b7-c754-44d4-80c2-10a6f6e5fd75": {"__data__": {"id_": "f6f7a3b7-c754-44d4-80c2-10a6f6e5fd75", "embedding": null, "metadata": {"page_label": "74", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34c9675f-17b6-4882-a9b4-04e1d51c2265", "node_type": "4", "metadata": {"page_label": "74", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "3919d62650d2c142d4ef7522ef29a1000d08deff8cec343c694bebdf2a72b1a1", "class_name": "RelatedNodeInfo"}}, "text": "From Scrapy to a Mobile App[ 74 ]Testing, sharing, and exporting your  \nmobile app\nWe are now ready to test our app. All we have to do is click on the TEST  button at \nthe top of the UI builder (1):\nThis is the application running in your browser\nThe mobile application runs in your browser. The links are active (2) and ready \nto navigate. You can preview different mobile screen resolutions and device \norientations. You can also click on the View on Phone  button to display a QR code \nthat you can scan with your mobile device and preview the application there. You \njust share the link that will be provided, and others are able to play with the app in \ntheir browsers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b8a1845-6c9f-41f1-8e7a-e43583454239": {"__data__": {"id_": "9b8a1845-6c9f-41f1-8e7a-e43583454239", "embedding": null, "metadata": {"page_label": "75", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6330fe0-a870-4567-abe7-ae77c30f0378", "node_type": "4", "metadata": {"page_label": "75", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "96d72d7bb677c80367fb31a19dc0ab79f2e4b14b3a9b724b319fdbe63c1e8a69", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 4[ 75 ]With just a few clicks, we organized and presented the data we scraped with Scrapy on \na mobile application. You may want to further customize this application by following \nthe Appery.io tutorials at http://devcenter.appery.io/tutorials/ . When you are \nready, Appery.io also gives you lots of export options via the EXPORT  button:\nYou can export your application for most major mobile platforms\nYou can export project files to perform further development on your favorite IDE or \nget a binary that you can publish on each platform's mobile marketplace.\nSummary\nUsing these two tools, Scrapy and Appery.io, we have a system that scrapes a \nwebsite and inserts data to a database. We also have a RESTful API and a simple \nmobile application for Android and iOS. For advanced features and further \ndevelopment, you can dive deeper into these platforms, outsource part of the \ndevelopment to field experts, or research alternatives. At this point, you have a little \nproduct to demonstrate application concepts with minimal coding.\nYou will notice that our app looks quite good given its extremely short development \ntime. It has realistic data instead of Lorem Ipsum placeholders, and the links  \nwork and do something meaningful. We successfully built a minimum viable \nproduct that respects its ecosystem (source websites) and returns value back to  \nit in the form of traffic.\nWe are now ready to find out how to use Scrapy spiders to extract data under more \ncomplex scenarios.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e717cb25-d67f-4ffe-a5b7-616b2e4d642f": {"__data__": {"id_": "e717cb25-d67f-4ffe-a5b7-616b2e4d642f", "embedding": null, "metadata": {"page_label": "76", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df6399d0-534d-4ba8-899d-20d0cc62bf77", "node_type": "4", "metadata": {"page_label": "76", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "873f84f292623e614ba857451498f585b7fd8927638cd0323528d6301e48151f", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f4688a7-7d86-44a7-b197-cb98a15f26ad": {"__data__": {"id_": "5f4688a7-7d86-44a7-b197-cb98a15f26ad", "embedding": null, "metadata": {"page_label": "77", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5b2a9af-88bd-48b7-95e9-333bd1f1eb3b", "node_type": "4", "metadata": {"page_label": "77", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "356c34f6c8014e9d12948a30f54560b48ec19460f32d55d6fa5da55eb5b3092f", "class_name": "RelatedNodeInfo"}}, "text": "[ 77 ]Quick Spider Recipes\nIn Chapter 3 , Basic Crawling, we focused on how to extract information from pages \nand store them into Items . What we learned covers the most common Scrapy use \ncases, and it should be enough to get you up and running. In this chapter, we will \nexamine more specialized cases in order to become more familiar with the two \nmost important Scrapy classes\u2014 Request  and Response \u2014the two R's on the UR2IM \nscraping model we presented in Chapter 3 , Basic Crawling .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83dbbcd2-17bc-455e-93f5-3b6430631af6": {"__data__": {"id_": "83dbbcd2-17bc-455e-93f5-3b6430631af6", "embedding": null, "metadata": {"page_label": "78", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f98a2dfb-d725-424c-af17-2b263c963977", "node_type": "4", "metadata": {"page_label": "78", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "3afd49006aa930f6404527e5b3b10f1c7461f405b0c13444a4a390c39c648267", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 78 ]A spider that logs in\nQuite often, you may find yourself wanting to extract data from websites that have \na login mechanism. In the most common case, a website will require you to provide \na username and a password in order to log in. We are going to use the example that \nyou can find in: http://web:9312/dynamic  (from our dev machine) or http://\nlocalhost:9312/dynamic  (from your host's web browser). If you use the username, \n\"user\", and password, \"pass\", you will get access to a page with three links of property \npages. The question now is how do you perform the same operation with Scrapy?\nRequests and Responses while logging in on a website\nLet's use the Google Chrome debugger and try to understand how login works. First \nof all, we go to the Network  tab (1). Then, we fill in the username and password and \nclick Login  (2). If the username and password are correct, you will see a page with \nthree links. If there was a mistake, you will see an error page.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec0cba00-2cae-4c3c-85dd-6f105a9c1a38": {"__data__": {"id_": "ec0cba00-2cae-4c3c-85dd-6f105a9c1a38", "embedding": null, "metadata": {"page_label": "79", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6900a33-1c59-4ea3-bc89-37250cab5e54", "node_type": "4", "metadata": {"page_label": "79", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "9944edd298b5f5e31cc1842cf3f5758ac4cca7b97d1279d6be29a038aeb42338", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 5[ 79 ]As soon as you push the Login  button, on the Google Chrome debugger  \nNetwork  tab, you will see a request with Request Method: POST  to  \nhttp://localhost:9312/dynamic/login .\nPrevious chapters' requests were GET-type requests, which are \nmost often used to retrieve data that doesn't change, such as \nsimple web pages, and images. POST-type requests are typically \nused when we want to retrieve data that depends on the data \nthat we sent to the server, such as the username and password \nin this example.\nIf you click on it (3), you can inspect the data that was sent to the server, including \nForm Data  (4), which will have the username and the password that you entered. All \nthis data was sent as text to the server. Chrome just groups them nicely and shows \nthem to us. The server responds with 302 Found  (5) that redirects us to a new page: \n/dynamic/gated . This page has links that appear only after a successful login. If \nyou try to visit http://localhost:9312/dynamic/gated  directly without entering \nthe correct username and password, the server would find out that you cheated \nand redirect you to an error page: http://localhost:9312/dynamic/error . How \ndoes the server know you and your password? If you click on gated  on the left of \nthe debugger (6), you will notice a Cookie  value (8) that is set under the Request \nHeaders  section (7).\nHTTP cookies are some, usually short, pieces of text or numbers \nthat servers send to browsers. In turn, browsers send them back \nto servers in every subsequent request in order to identify you, \nthe user, and your session. This allows you to perform complex \noperations that require server-side state information, such as \nthe contents of your basket or your username and password.\nSummarizing, a single operation, such as logging in, may involve several server \nround-trips, including POST-requests, and HTTP redirects. Scrapy handles most of \nthese operations automatically, and the code that we need to write is simple.\nWe start with the spider named easy  from Chapter 3 , Basic Crawling,  and we create \na new spider named login  by renaming the file and changing the name  property \ninside the spider (it should look like this):\nclass LoginSpider(CrawlSpider):\n    name = 'login'", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0df9465d-1fe9-4c11-8f70-0674b2aae9a8": {"__data__": {"id_": "0df9465d-1fe9-4c11-8f70-0674b2aae9a8", "embedding": null, "metadata": {"page_label": "80", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67a5f6f5-c69b-430d-8519-8688e51feadb", "node_type": "4", "metadata": {"page_label": "80", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b127a0020beebaa0256866380d20b3da74bf67d3ea36e4924c16ff7b5e91a0c6", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 80 ]The code from this chapter is in ch05  directory in github. \nthis example in particular will be in ch05/properties .\nWe need to send the initial request that logs in by performing a POST request on \nhttp://localhost:9312/dynamic/login . We do this with Scrapy's FormRequest  \nclass. This class is similar to Request  from Chapter 3 , Basic Crawling,  but with an extra \nformdata  argument that we use to pass form data ( user  and pass ). To use this class, \nwe have to import it first with:\nfrom scrapy.http import FormRequest\nWe then replace the start_urls  statement with a start_requests()  method.  \nWe do this because in this case, we need to start with something a bit more custom \nthan just a few URLs. More specifically, we create and return a FormRequest  from \nthis function:\n# Start with a login request\ndef start_requests(self):\n  return [\n    FormRequest(\n      \"http://web:9312/dynamic/login\",\n      formdata={\"user\": \"user\", \"pass\": \"pass\"}\n         )]\nThat's it really. The default parse()  of CrawlSpider  (the base class of our \nLoginSpider ) handles Response  and uses our Rules  and LinkExtractors  exactly \nas it did in Chapter 3 , Basic Crawling . We have so little extra code because Scrapy \nhandles cookies transparently for us, and as soon as we log in, it passes them on to \nsubsequent requests in exactly the same manner as a browser. We can run this using \nscrapy crawl  as usual:\n$ scrapy crawl login \nINFO: Scrapy 1.0.3 started (bot: properties)\n...\nDEBUG: Redirecting (302) to <GET .../gated> from <POST .../login >\nDEBUG: Crawled (200) <GET .../data.php>\nDEBUG: Crawled (200) <GET .../property_000001.html> (referer: .../data.\nphp)\nDEBUG: Scraped from <200 .../property_000001.html>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36b9c8ef-5366-4079-9671-7aa7e644c6c5": {"__data__": {"id_": "36b9c8ef-5366-4079-9671-7aa7e644c6c5", "embedding": null, "metadata": {"page_label": "81", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9db7cb74-8532-4825-bc1c-e46dc2cc5282", "node_type": "4", "metadata": {"page_label": "81", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ed9de8ede003f06c06564cd6da708c6f57019a669145a85b23f9cff2671932b8", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 5[ 81 ]  {'address': [u'Plaistow, London'],\n   'date': [datetime.datetime(2015, 11, 25, 12, 7, 27, 120119)],\n   'description': [u'features'],\n   'image_urls': [u'http://web:9312/images/i02.jpg'],\n...\nINFO: Closing spider (finished)\nINFO: Dumping Scrapy stats:\n  {...\n   'downloader/request_method_count/GET': 4,\n   'downloader/request_method_count/POST': 1,\n...\n   'item_scraped_count': 3,\nWe can notice the redirection from dynamic/login  to dynamic/gated  on the log and \nthen a scrape of Items as usual. In the statistics, we see one POST request and four \nGET requests; one for dynamic/gated  index and three for property pages.\nIn this example, we don't protect the property pages \nthemselves but just the links to these pages. The code \nwould be the same in either case.\nIf we used the wrong user/pass, we would get a redirect to a page with no  \nitem URLs and the process would terminate at that point, as you can see in the \nfollowing run:\n$ scrapy crawl login\nINFO: Scrapy 1.0.3 started (bot: properties)\n...\nDEBUG: Redirecting (302) to <GET .../dynamic/error > from <POST .../\ndynamic/login>\nDEBUG: Crawled (200) <GET .../dynamic/error>\n...\nINFO: Spider closed (closespider_itemcount)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12e606a0-13c0-4de7-91dc-1e4d4ae32e90": {"__data__": {"id_": "12e606a0-13c0-4de7-91dc-1e4d4ae32e90", "embedding": null, "metadata": {"page_label": "82", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b8c466c-57cd-4fc9-821b-088f59bcf067", "node_type": "4", "metadata": {"page_label": "82", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "8863ed520655a7ae9c68e087fa780eee242ab1d13ec283fd7dd4ad17d6803b6b", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 82 ]This was a simple login example that demonstrates essential login mechanisms. Most \nwebsites will likely have slightly more complex mechanisms that Scrapy also handles \nwith ease. Some sites, for example, require you to pass some form variables from the \nform page to the login page while performing the POST request in order to confirm \nthat cookies are enabled, and also to make it a bit more difficult for you to try to \ncheck with brute-force thousands of user/pass combinations.\nRequest and Response on a more advanced login case using nonce", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "596b2e91-3ca4-4c9e-ba78-fe2bc7813b90": {"__data__": {"id_": "596b2e91-3ca4-4c9e-ba78-fe2bc7813b90", "embedding": null, "metadata": {"page_label": "83", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a115120-4afe-41cf-8de6-7f7b2b73df43", "node_type": "4", "metadata": {"page_label": "83", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c07a073f485211c6cfa48f33eeb283c4ca3b9fc60d53957b743de6bc7760bb89", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 5[ 83 ]For example, if you visit http://localhost:9312/dynamic/nonce , you will see \na page that looks identical, but if you use Chrome's Debugger, you will notice that \nthe form in this page has a hidden field called nonce . When you submit this form \n(to http://localhost:9312/dynamic/nonce-login ), the login won't be successful \nunless you pass not only the correct user/pass, but also the exact nonce  value that \nserver gave you when you visited this login page. There is no way for you to guess \nthat value as it typically will be random and single-use. This means that in order \nto successfully log in, you now need two requests. You have to visit the form page \nand then the login page and pass through some data. As usual, Scrapy has built-in \nfunctionality that helps.\nWe create a NonceLoginSpider  spider that is similar to the previous one. Now, in \nstart_requests() , we are going to return a simple Request  (don't forget to import \nit) to our form page, and will manually handle the response by setting its callback  \nproperty to our handler method named parse_welcome() . In parse_welcome() , \nwe use the helper from_response()  method of the FormRequest  object to create \nFormRequest  that is pre-populated with all the fields and values from the original \nform. FormRequest.from_response()  roughly emulates a submit click on the first \nform on the page with all the fields left blank.\nIt's worth spending some time familiarizing yourself with the \ndocumentation of from_response() . It has many useful \nfeatures like formname  and formnumber  that helps you \nselect the form you want if there's more than one in a page.\nWhat makes this very useful to us is that it effortlessly includes, verbatim, all the \nhidden fields of that form. All we need to do is to use the formdata  argument to fill \nin the user  and pass  fields and return the FormRequest . Here is the relevant code:\n# Start on the welcome page\ndef start_requests(self):\n    return [\n        Request(\n            \"http://web:9312/dynamic/nonce\",\n            callback=self.parse_welcome)\n    ]\n# Post welcome page's first form with the given user/pass\ndef parse_welcome(self, response):\n    return FormRequest.from_response(\n        response,\n        formdata={\"user\": \"user\", \"pass\": \"pass\"}\n    )", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acd4af57-f24e-448e-a07e-0b110d6390a9": {"__data__": {"id_": "acd4af57-f24e-448e-a07e-0b110d6390a9", "embedding": null, "metadata": {"page_label": "84", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "440a2d4c-60d8-471f-8199-ad6d5d5a7f35", "node_type": "4", "metadata": {"page_label": "84", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "59ed451483ba5ca55720694ba3bdae860af0ff44ad5920a565a6f809445eec93", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 84 ]We can run this spider as usual:\n$ scrapy crawl noncelogin \nINFO: Scrapy 1.0.3 started (bot: properties)\n...\nDEBUG: Crawled (200) <GET .../dynamic/nonce>\nDEBUG: Redirecting (302) to <GET .../dynamic/gated > from <POST .../\ndynamic/login-nonce>\nDEBUG: Crawled (200) <GET .../dynamic/gated>\n...\nINFO: Dumping Scrapy stats:\n  {...\n   'downloader/request_method_count/GET': 5,\n   'downloader/request_method_count/POST': 1,\n...\n   'item_scraped_count': 3,\nWe can see the first GET request to /dynamic/nonce  page, and then POST, and \nredirection on the /dynamic/nonce-login  page that leads us to /dynamic/gated  as \nit did before. This concludes our login discussion. This example used two steps to log \nin. With enough patience, one can form arbitrary long chains that are able to perform \nalmost every login operation.\nA spider that uses JSON APIs and AJAX \npages\nSometimes, you will find yourself exploring pages with data that you'll be unable to \nfind on the HTML of the page. For example, if you visit http://localhost:9312/\nstatic/  and right-click inspect element  (1, 2) somewhere in the page you will see \nthe DOM tree with all the usual HTML elements. On the other hand, if you use \nscrapy shell  or right-click on View Page Source  (3, 4) in Chrome, you will see that \nthe HTML code for this page doesn't contain any information relevant to properties. \nWhere does this data come from?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7167942a-073e-473c-80dd-3eff469a2da5": {"__data__": {"id_": "7167942a-073e-473c-80dd-3eff469a2da5", "embedding": null, "metadata": {"page_label": "85", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1817412a-eff8-4669-9db7-3a87f3eb975d", "node_type": "4", "metadata": {"page_label": "85", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "4e831c6df625446fe726419c84e388d979c5a4c4d88107080737d021c7ff82cd", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 5[ 85 ]\nRequest and Response on pages that load JSON objects dynamically\nIn these cases your next step is, as usual, to open the Network  tab of Chrome's \ndebugger (5) to find out what's going on. In the list on the left, we can see all the \nrequests that Chrome performed to load this page. In this simple page, there are \nonly three requests: static/  that we already checked, jquery.min.js  that retrieves the \ncode for a popular Javascript framework, and api.json  which seems interesting. If \nwe click on it (6) and we then click the Preview  tab on the right (7), we will notice \nthat it contains the data we have been looking for. Indeed http://localhost:9312/\nproperties/api.json  contains property IDs and names (8), as follows:\n[{\n    \"id\": 0,\n    \"title\": \"better set unique family well\"\n}, \n... {\n    \"id\": 29,\n    \"title\": \"better portered mile\"\n}]\nThis is a very simple example of a JSON API. More complex APIs may require you \nto log in, use POST requests, or return more interesting data structures. In any case, \nJSON is one of the easiest formats to parse because you don't have to write any \nXPath expressions to extract data from it.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5b00580-cdcd-42f2-ab29-1f50f8029d67": {"__data__": {"id_": "a5b00580-cdcd-42f2-ab29-1f50f8029d67", "embedding": null, "metadata": {"page_label": "86", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8f8fa17-04e5-42f6-a76e-4da0c97ce9e0", "node_type": "4", "metadata": {"page_label": "86", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "aa4f611a583258baa849046925a19fdffd7aba3f5b56ffa083bd2e06ed5fb33a", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 86 ]Python provides a great JSON parsing library. When we import json , we can use \njson.loads(response.body)  to parse JSON and convert it to an equivalent Python \nobject consisting of Python primitives, lists, and dicts.\nLet's do this by copying manual.py  from Chapter 3 , Basic Crawling . This is the best \noption to start with in this case as we need to manually build property URLs and \nRequest  objects from IDs that we find in the JSON object. We rename the file to api.\npy, rename the spider class to ApiSpider  and name it api. Our new start_urls  \nshould be the JSON API URL:\nstart_urls = (\n    'http://web:9312/properties/api.json',\n)\nIf you need to do POST requests or more complex operations, you can use the \nstart_requests()  method and the techniques we saw in previous sections. At this \npoint, Scrapy will open this URL and call our parse()  method with Response  as an \nargument. We can import json  and use the following code to parse the JSON object:\ndef parse(self, response):\n    base_url = \"http://web:9312/properties/\"\n    js = json.loads(response.body)\n    for item in js:\n        id = item[\"id\"]\n        url = base_url + \"property_%06d.html\" % id\n        yield Request(url, callback=self.parse_item)\nThe preceding code uses json.loads(response.body)  to parse the Response  JSON \nobject to a Python list that then it iterates through. For each item in the list, we put \ntogether a URL consisting of three parts: base_url , property_%06d  and .html . base_\nurl is a URL prefix that was defined previously. %06d  is a very useful piece of Python \nsyntax that allows us to create new strings by combining Python variables. In this case, \n%06d  will be replaced with the value of the id variable (the one after the % at the end \nof the line). id will be treated as a number ( %d means treat it as a number) and it will \nbe extended to six characters by prepending 0's if necessary. If id has, for example, the \nvalue 5, %06d  will be replaced with 000005, whereas if id happens to be 34322, %06d  \nwill be replaced with 034322. The end result is perfectly valid URLs for our properties. \nWe use this URL to form a new Request  object that we yield  exactly as we did in \nChapter 3 , Basic Crawling . We can run this example as usual with scrapy crawl :\n$ scrapy crawl api\nINFO: Scrapy 1.0.3 started (bot: properties)\n...\nDEBUG: Crawled (200) <GET ...properties/api.json>", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a280776-0a65-4e70-9d85-7b261843da8f": {"__data__": {"id_": "0a280776-0a65-4e70-9d85-7b261843da8f", "embedding": null, "metadata": {"page_label": "87", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5061526f-eb15-4894-be93-fdc86eb9ef1f", "node_type": "4", "metadata": {"page_label": "87", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "3fde8c5b30dc96221db39aef860ee1fdfc8f37e54e78010a02a982edaa9a6cd0", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 5[ 87 ]DEBUG: Crawled (200) <GET .../property_000029.html>\n...\nINFO: Closing spider (finished)\nINFO: Dumping Scrapy stats:\n...\n   'downloader/request_count': 31, ...\n   'item_scraped_count': 30,\nYou might notice in the stats at the end, 31 Requests\u2014one for each item as well as  \nan initial one for api.json .\nPassing arguments between responses\nIn many cases, you will have interesting information on your JSON APIs that you \nwill want to store to your Item . To demonstrate this case, in our example, for a given \nproperty, the JSON API returns its title prepended with \"better\". If property's title is \n\"Covent Garden\" for example, the API will have \"Better Covent Garden\" as its title. \nLet's assume that we want to store these \"better\" titles in our Items . How do we pass \ninformation from our parse()  to our parse_item()  method?\nYou won't be surprised to hear that we can do this by setting something in the \nRequest  that parse()  generates. We can then retrieve this from the Response  that \nparse_item()  receives. Request  has a dict named meta  that is directly accessible on \nResponse . For our example, let's set a title value on this dict to store the title from the \nJSON object:\ntitle = item[\"title\"]\nyield Request(url, meta={\"title\": title},callback=self.parse_item)\nInside parse_item() , we can use this value instead of the XPath expression that we \nused to have:\nl.add_value('title', response.meta['title'],\n     MapCompose(unicode.strip, unicode.title))\nYou will notice that we switched from calling add_xpath()  to add_value()  \nbecause we don't need to use any XPath for that field any more. We can now run \nthe new spider with scrapy crawl , and we will see titles from api.json  on our \nPropertyItems .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bb6ad28-dc38-404e-b34f-fd29e85b3dab": {"__data__": {"id_": "3bb6ad28-dc38-404e-b34f-fd29e85b3dab", "embedding": null, "metadata": {"page_label": "88", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea23fb20-3622-41be-9eeb-f7b1ffcd6f3e", "node_type": "4", "metadata": {"page_label": "88", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "607a1d313db8059c6a871ff24cf63ea2ccaf612986da90ce3bb252b66db107cc", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 88 ]A 30-times faster property spider\nThere is a tendency when you start with a framework to use, maybe, the most \nsophisticated and, typically, the most complex way for anything you do. You will \nlikely find yourself doing that with Scrapy too. Just before you go crazy with XPath \nand technology, it is worth to pause for a moment and wonder; is the way I chose the \neasiest way to extract data from this website?\nYou can have orders-of-magnitude savings if you avoid scraping every single listing \npage if you can extract about the same information from index pages.\nPlease keep in mind that many websites offer a different number \nof items on their index pages. For example, a website might be \nable to give you 10, 50 or 100 listings per index page by tuning \na parameter, such as &show=50 . If so, obviously, set it to the \nmaximum value available.\nFor example, in our properties case, all the information we need exists in the index \npages. They contain the title, the description, the price and the image. This means \nthat we can scrape a single index page and extract 30 items and a link to the next \nindex page. By crawling 100 index pages, we get 3000 items with just 100 requests \ninstead of 3000. That's just great!\nIn the actual Gumtree website, the description on the index pages is shorter than the \nfull description on the listing page. This may be okay or even desirable.\nIn many cases, you will have to trade off data quality with \nnumber of requests. Many sources throttle the number of \nrequests heavily (more on that in later chapter), so hitting \nindices might help you solve an otherwise hard problem.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9619577-c5ca-469a-a4ae-c4483deb66a9": {"__data__": {"id_": "c9619577-c5ca-469a-a4ae-c4483deb66a9", "embedding": null, "metadata": {"page_label": "89", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dbc7fdb-5578-43ad-98c6-24ba1c334a31", "node_type": "4", "metadata": {"page_label": "89", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b79061f1b34721bf2500f62ad97eba66b37487b068e3a1046d71f0a5de200efb", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 5[ 89 ]In our example, if we have a look at the HTML of one of the index pages, we will notice \nthat each listing in the index page has its own node indicated by itemtype=\"http://\nschema.org/Product\" . Within this node, we have all the information for each property \nannotated in exactly the same way as in the detail pages:\nMany properties can be extracted from a single index page\nLet's load the first index page in Scrapy shell and play a bit with XPath expressions:\n$ scrapy shell http://web:9312/properties/index_00000.html\nWhile within the Scrapy shell, let's try to select everything with the Product  tag:\n>>> p=response.xpath('//*[@itemtype=\"http://schema.org/Product\"]')\n>>> len(p)\n30\n>>> p\n[<Selector xpath='//*[@itemtype=\"http://schema.org/Product\"]' data=u'<li \nclass=\"listing-maxi\" itemscopeitemt'...]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "063cbd6b-e0d5-434d-bdf4-768debc755bb": {"__data__": {"id_": "063cbd6b-e0d5-434d-bdf4-768debc755bb", "embedding": null, "metadata": {"page_label": "90", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9073010-c58e-4f72-aaf8-3fc9fbdd9be8", "node_type": "4", "metadata": {"page_label": "90", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "57b7eb1ff5dfcfc401bfd42e593f68c1d7501afcedbe4b4eb64fe8fd3af76c06", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 90 ]We notice that we get a list of 30 Selector  objects, each pointing to one of our \nlistings. The Selector  objects are similar to the Response  objects in the sense that we \ncan use XPath expression on them and get information only from within whatever \nthey point to. The only caveat is that those expressions should be relevant XPath \nexpressions. Relevant XPath expressions are identical to the ones that we've seen \nalready but with a ' .' dot prepended to them. Let's see how this works by extracting \na title from, for example, the fourth listing using the .//*[@itemprop=\"name\"][1]/\ntext()  relevant XPath expression:\n>>> selector = p[3]\n>>> selector\n<Selector xpath='//*[@itemtype=\"http://schema.org/Product\"]' ... '>\n>>> selector.xpath('.//*[@itemprop=\"name\"][1]/text()').extract()\n[u'l fun broadband clean people brompton european']\nWe can use a for loop on the list of Selector  objects to extract information for all 30 \nitems of an index page.\nTo do so, we start again from our manual.py  file from Chapter 3 , Basic Crawling,  and \nname the new spider \"fast\" on a file named fast.py . We reuse most of the code with \nsmall changes in the parse()  and parse_item()  methods. The updated methods are \nas follows:\ndef parse(self, response):\n    # Get the next index URLs and yield Requests\n    next_sel = response.xpath('//*[contains(@class,\"next\")]//@href')\n    for url in next_sel.extract():\n        yield Request(urlparse.urljoin(response.url, url))\n    # Iterate through products and create PropertiesItems\n    selectors = response.xpath(\n        '//*[@itemtype=\"http://schema.org/Product\"]')\n    for selector in selectors:\n        yield self.parse_item(selector, response)\nThere are no changes in the first part of the code that yields the next index Request . \nThe only difference lies in the second part where instead of using yield  to create \nRequests  for the detail pages, we iterate through selectors and call our parse_\nitem() . This is also quite similar to our original code, as follows:\ndef parse_item(self, selector, response):\n    # Create the loader using the selector\n    l = ItemLoader(item=PropertiesItem(), selector=selector)\n    # Load fields using XPath expressions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d0eef9f-7cce-475d-9571-5aa13c679949": {"__data__": {"id_": "8d0eef9f-7cce-475d-9571-5aa13c679949", "embedding": null, "metadata": {"page_label": "91", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c947301-b1d4-4f9f-b0da-9d0638dd78ca", "node_type": "4", "metadata": {"page_label": "91", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "df892c46c2130b7f2c0e0cfb7646ff9a4a01a657bb5231283c1607ca2c706de3", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 5[ 91 ]    l.add_xpath('title', './/*[@itemprop=\"name\"][1]/text()',\n                MapCompose(unicode.strip, unicode.title))\n    l.add_xpath('price', './/*[@itemprop=\"price\"][1]/text()',\n                MapCompose(lambda i: i.replace(',', ''), float),\n                re='[,.0-9]+')\n    l.add_xpath('description',\n                './/*[@itemprop=\"description\"][1]/text()',\n                MapCompose(unicode.strip), Join())\n    l.add_xpath('address',\n                './/*[@itemtype=\"http://schema.org/Place\"]'\n                '[1]/*/text()',\n                MapCompose(unicode.strip))\n    make_url = lambda i: urlparse.urljoin(response.url, i)\n    l.add_xpath('image_urls', './/*[@itemprop=\"image\"][1]/@src',\n                MapCompose(make_url))\n    # Housekeeping fields\n    l.add_xpath('url', './/*[@itemprop=\"url\"][1]/@href',\n                MapCompose(make_url))\n    l.add_value('project', self.settings.get('BOT_NAME'))\n    l.add_value('spider', self.name)\n    l.add_value('server', socket.gethostname())\n    l.add_value('date', datetime.datetime.now())\n    return l.load_item() \nThe slight changes that we made are as follows:\n\u2022 ItemLoader  now uses selector  as a source rather than Response . This is a \nvery convenient feature of the ItemLoader  API, allowing us to extract from \nthe currently selected segment instead of the entire page.\n\u2022 XPath expressions turned to relative XPath by prepending the dot ( .).\nIt so happened that in our case, our XPath expressions were \nidentical in the detail and the index pages. This won't always \nbe the case and you may have to redevelop your XPath \nexpressions to match the structure of your index pages.\n\u2022 We have to compile the URL of Item  ourselves. Before response.url  was \ngiving us the URL for the listing page. Now, it gives the URL of the index \npage because this was the page that we crawled. We have to extract the \nURL of the listing using our familiar .//*[@itemprop=\"url\"][1]/@href  \nXPath expression and then convert it to an absolute URL with our usual \nMapCompose  processor.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3263f8d3-7032-4b3a-b952-66524a167bb5": {"__data__": {"id_": "3263f8d3-7032-4b3a-b952-66524a167bb5", "embedding": null, "metadata": {"page_label": "92", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1f50016-dded-490c-b586-327d60cd24fd", "node_type": "4", "metadata": {"page_label": "92", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "53cddb08615cc58ffacab7b234252dcdf7351fb648312aa8c11fdb5600cc6e10", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 92 ]Small changes have generated huge savings. Now, we can run this spider with the \nfollowing code:\n$ scrapy crawl fast -s CLOSESPIDER_PAGECOUNT=3\n...\nINFO: Dumping Scrapy stats:\n   'downloader/request_count': 3, ...\n   'item_scraped_count': 90,...\nAs promised, with just three requests, we scraped 90 items. We would need 93 \nrequests to do the same if we didn't hit the index. This is brilliant!\nIf you want to use scrapy parse  to debug, you would now have to set the spider  \nargument, as follows:\n$ scrapy parse --spider=fast http://web:9312/properties/index_00000.html\n...\n>>> STATUS DEPTH LEVEL 1 <<<\n# Scraped Items  --------------------------------------------\n[{'address': [u'Angel, London'],\n... 30 items...\n# Requests  ---------------------------------------------------\n[<GET http://web:9312/properties/index_00001.html>]\nExactly as we expected, parse()  returns 30 Items  and one Request  to the next index \npage. Feel free to experiment with scrapy parse  by, for example, passing --depth=2 .\nA spider that crawls based on an Excel file\nMost of the time you have one spider per source web-site, but there are cases where you \nwant to scrape data from many websites and the only thing that changes between them \nis the XPath expressions you use. In these cases, it feels like overkill to have a spider for \nevery site. Can you crawl through them all with a single spider? The answer is yes.\nLet's create a new project for this experiment as the items that we crawl are very \ndifferent (actually we won't define any in this project!). I assume that we were in the \nproperties  directory of ch05 . Let's go one level up, as follows:\n$ pwd\n/root/book/ch05/properties\n$ cd ..\n$ pwd\n/root/book/ch05", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5810a16d-6893-472f-8b5d-85e8e18912c6": {"__data__": {"id_": "5810a16d-6893-472f-8b5d-85e8e18912c6", "embedding": null, "metadata": {"page_label": "93", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00a2b1cd-e5ec-4a1f-ba02-4d0937b9856f", "node_type": "4", "metadata": {"page_label": "93", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a5e5093491906f3873da4a13ae3b522a06bef3159cbb3d3bd090e23cde0370a7", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 5[ 93 ]We can create a new project named generic  and a spider named fromcsv :\n$ scrapy startproject generic\n$ cd generic\n$ scrapy genspider fromcsv example.com\nNow let's create a .csv  file with what we want to extract. We can use a spreadsheet \nprogram, such as Microsoft Excel, to create this .csv  file. Fill in a few URLs and \nXPath expressions as shown in the following figure and then save it as todo.csv  \nin spider's directory (the one with scrapy.cfg ). To save as .csv , chose CSV file \n(Windows) as file format on the save dialog:\ntodo.csv contains URLs and XPath expressions\nGreat! If it all went fine, you should be able to see the file on your terminal:\n$ cat todo.csv \nurl,name,price\na.html,\"//*[@id=\"\"itemTitle\"\"]/text()\",\"//*[@id=\"\"prcIsum\"\"]/text()\"\nb.html,//h1/text(),//span/strong/text()\nc.html,\"//*[@id=\"\"product-desc\"\"]/span/text()\"\nPython has built-in libraries for .csv  files. We just have to import csv , and we can \nthen use the following straightforward code to read the lines one by one as dict . If \nwe open a Python prompt on the current directory, we can try the following:\n$ pwd\n/root/book/ch05/generic2\n$ python\n>>> import csv\n>>> with open(\"todo.csv\", \"rU\") as f:\n        reader = csv.DictReader(f)\n        for line in reader:\n            print line", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5df5063-dc7e-4e68-96ee-f511149643f5": {"__data__": {"id_": "a5df5063-dc7e-4e68-96ee-f511149643f5", "embedding": null, "metadata": {"page_label": "94", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fa9d533-1a89-41bc-82b0-df65b93469b7", "node_type": "4", "metadata": {"page_label": "94", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "6fd07e337191932859375d0359cc57ea90c877bdae6dd8b93c601dfd4955d400", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 94 ]The first line from the file will be treated automatically as a header and from that \nthe names of the keys for the dict  will be deduced. For each subsequent line, we \nget a dict  containing the data. We iterate each row with a for loop. If we run the \npreceding code, we get the following output:\n{'url': ' http://a.html', 'price': '//*[@id=\"prcIsum\"]/text()', \n'name': '//*[@id=\"itemTitle\"]/text()'}\n{'url': ' http://b.html', 'price': '//span/strong/text()', 'name': '//\nh1/text()'}\n{'url': ' http://c.html', 'price': '', 'name': '//*[@id=\"product-\ndesc\"]/span/text()'}\nThis is great. Let's now edit our generic/spiders/fromcsv.py  spider. We will use \nthe URLs from the .csv  file, and we don't want any domain restrictions. Thus, the \nfirst thing to do is to remove start_urls  and allowed_domains . Then we will read \nthe .csv  file.\nSince we want to start with URLs that we don't know in advance but we read from a \nfile instead, we will implement a start_requests()  method. For each row, we will \ncreate Request  and yield it. We will also store field names and XPaths from csv in \nrequest.meta  in order to use them in our parse()  function. Then, we use an Item  \nand an ItemLoader  to populate Item's  fields. Here's the full code:\nimport csv\nimport scrapy\nfrom scrapy.http import Request\nfrom scrapy.loader import ItemLoader\nfrom scrapy.item import Item, Field\nclass FromcsvSpider(scrapy.Spider):\n    name = \"fromcsv\"\ndef start_requests(self):\n    with open(\"todo.csv\", \"rU\") as f:\n        reader = csv.DictReader(f)\n        for line in reader:\n            request = Request(line.pop('url'))\n            request.meta['fields'] = line\n            yield request\ndef parse(self, response):\n    item = Item()\n    l = ItemLoader(item=item, response=response)\n    for name, xpath in response.meta['fields'].iteritems():\n        if xpath:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e88f3eb2-d0d5-4696-aefd-4befcf947bdf": {"__data__": {"id_": "e88f3eb2-d0d5-4696-aefd-4befcf947bdf", "embedding": null, "metadata": {"page_label": "95", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb05c358-6d20-4f48-976e-fb9949fe9f1e", "node_type": "4", "metadata": {"page_label": "95", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "df8e43565d4e5e73b7430f28749a784414e11a66f2ad99e4ca05774adb497acd", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 5[ 95 ]            item.fields[name] = Field()\n            l.add_xpath(name, xpath)\n    return l.load_item()\nLet's crawl and save the output to an out.csv  file:\n$ scrapy crawl fromcsv -o out.csv\nINFO: Scrapy 0.0.3 started (bot: generic)\n...\nDEBUG: Scraped from <200 a.html>\n{'name': [u'My item'], 'price': [u'128']}\nDEBUG: Scraped from <200 b.html>\n{'name': [u'Getting interesting'], 'price': [u'300']}\nDEBUG: Scraped from <200 c.html>\n{'name': [u'Buy this now']}\n...\nINFO: Spider closed (finished)\n$ cat out.csv \nprice,name\n128,My item\n300,Getting interesting\n,Buy this now\nThis is as neat and straightforward as it gets!\nThere are some things you may have noticed in the code. Since we don't define  \nproject-wide Items  for this project, we have to provide one to ItemLoader  manually \nas follows:\nitem = Item()\nl = ItemLoader(item=item, response=response)\nWe also add fields dynamically using the fields  member variable of Item . To add a \nnew field dynamically and have it populated by our ItemLoader , all we have to do \nis the following:\nitem.fields[name] = Field()\nl.add_xpath(name, xpath)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3e26604-c7c7-48ca-95c2-ee511b882588": {"__data__": {"id_": "c3e26604-c7c7-48ca-95c2-ee511b882588", "embedding": null, "metadata": {"page_label": "96", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25b4ba66-98af-45a2-a5d0-24ac58e101ae", "node_type": "4", "metadata": {"page_label": "96", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "9ba4ea655bd40ab5c260265352562781cb0c3465303353a88ecd5257fca566dc", "class_name": "RelatedNodeInfo"}}, "text": "Quick Spider Recipes[ 96 ]We can finally make our code a bit nicer. Hardcoding todo.csv  isn't very good \npractice. Scrapy gives us a very convenient way to pass arguments to spiders. If we \npass an -a command-line argument, for example, -a variable=value , a spider \nproperty is set for us and we are able to retrieve it with self.variable . In order to \ncheck for the variable and use a default if it isn't provided, we use the getattr()  \nPython method: getattr(self, 'variable', 'default') . Overall, we replace \nour original with open...  statement with the following one:\nwith open(getattr(self, \"file\", \"todo.csv\"), \"rU\") as f:\nNow, todo.csv  is the default value unless it's overridden by setting a source file \nexplicitly with an -a argument. Given a second file, another_todo.csv , we can run \nthe following:\n$ scrapy crawl fromcsv -a file=another_todo.csv -o out.csv\nSummary\nIn this chapter, we dived a bit deeper into the internals of Scrapy spiders. We used \nFormRequest  to log in, passed variables around with meta  of Request /Response , \nused relevant XPaths and Selectors , used .csv  files as sources, and much more.\nWe are now ready to see how we can deploy our spiders in the Scrapinghub cloud \nin the brief Chapter 6 , Deploying to Scrapinghub  before we move onto reviewing the \nwealth of Scrapy settings in Chapter 7 , Configuration and Management .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59f7f8c5-d043-485e-8a24-23ef521783d7": {"__data__": {"id_": "59f7f8c5-d043-485e-8a24-23ef521783d7", "embedding": null, "metadata": {"page_label": "97", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af6edcfa-ca2e-4446-a1bc-1ee64199c805", "node_type": "4", "metadata": {"page_label": "97", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b99183ca6c07ac712ef70db5e5125be17a73310378b065598679fa57da01aa85", "class_name": "RelatedNodeInfo"}}, "text": "[ 97 ]Deploying to Scrapinghub\nIn the last few chapters, we took a look at how to develop Scrapy spiders. As soon \nas we're satisfied with their functionality, we have two options. If all we want is to \nuse them for a single scrape, we may be okay with letting them run for some time \non our dev machine. On the other hand, quite often, we need to run scraping jobs \nperiodically. We may use cloud servers from Amazon, RackSpace, or any other \nprovider, but this requires some setup, configuration and maintenance. This is where \nScrapinghub comes into play.\nScapinghub is the Amazon of Scrapy hosting\u2014a cloud Scrapy infrastructure \nprovider that is built by lead Scrapy developers. It is a paid service, but they provide \na free tier with no need for a credit card. If you want to have your Scrapy crawler \nrunning on a professionally set up and maintained infrastructure within minutes, \nthen this chapter is for you.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dad6c292-4c19-49f9-993d-06899ac0c8e8": {"__data__": {"id_": "dad6c292-4c19-49f9-993d-06899ac0c8e8", "embedding": null, "metadata": {"page_label": "98", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e45c5900-a7b3-4807-afee-8d21a304a178", "node_type": "4", "metadata": {"page_label": "98", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "792bcbb1ae5ab7da2743cbcb76711b7df94c45c4cf7f79440657a2b5f768c266", "class_name": "RelatedNodeInfo"}}, "text": "Deploying to Scrapinghub[ 98 ]Signing up, signing in, and starting  \na project\nThe first step is to open an account on http://scrapinghub.com/ . All we need is an \ne-mail address and a password. After clicking on the link in the confirmation e-mail, \nwe can log in to the service. The first page that we see is our profile dashboard. We \ndon't have any projects, so we click on the + Service  button (1) to create one:\nCreating a new project with scrapinghub\nWe can name our project properties  (2) and click on the Create  button (3). Then, we \nclick on the new  link in the homepage (4) to open the project.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "216d1c06-32a8-48c2-99cf-a3b534695105": {"__data__": {"id_": "216d1c06-32a8-48c2-99cf-a3b534695105", "embedding": null, "metadata": {"page_label": "99", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58af4e95-83ec-465c-b917-532eaf22841a", "node_type": "4", "metadata": {"page_label": "99", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "68bb1a424bc045af75cbaa8ae80bd79d937a9272ce70caced2fbf8a09ddcc289", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 6[ 99 ]\nThe main menu\nThe project dashboard is the most important screen for our project. In the menu, on \nthe left, we can see several sections. Jobs  and Spiders  sections provide information \nabout our runs and spiders, respectively. Periodic Jobs  enables us to schedule \nrecurrent crawls. The other four sections are not that useful for us right now. \nSpider deployment settings\nWe can go directly to the Settings  section (1). In contrast to many websites' settings, \nScrapinghub's setting provide lots of functionality one should really be aware of.\nRight now, we are mostly interested in the Scrapy Deploy  section (2).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4877dcde-10bc-4f12-88f4-3dab34f84ffa": {"__data__": {"id_": "4877dcde-10bc-4f12-88f4-3dab34f84ffa", "embedding": null, "metadata": {"page_label": "100", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43dd8565-3892-47ea-a3ce-1fe64ef5e9fd", "node_type": "4", "metadata": {"page_label": "100", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "9f8fd9374183e71e41c4805b71dad1f83abf2141cb5b2e87bccdbf89e5815c8f", "class_name": "RelatedNodeInfo"}}, "text": "Deploying to Scrapinghub[ 100 ]Deploying our spiders and  \nscheduling runs\nWe will deploy directly from our dev machine. In order to do so, we just have to \ncopy the lines from the Scrapy Deploy  page (3) and put them on scrapy.cfg  of our \nproject, replacing the default [deploy]  section. You will note that we don't need \nto set a password. As an example, we will use the properties project from Chapter \n4, From Scrapy to a Mobile App . The reason that we use that spider is that we need \nour target data to be accessible from the Web exactly as we used to back in that \nchapter. Before we use it, we restore the original settings.py  file by removing any \nreferences to the Appery.io pipeline:\nThe code from this chapter is in the ch06  directory. This \nexample in particular is in the ch06/properties  directory.\n$ pwd\n/root/book/ch06/properties\n$ ls\nproperties  scrapy.cfg\n$ cat scrapy.cfg\n...\n[settings]\ndefault = properties.settings\n# Project: properties\n[deploy]\nurl = http://dash.scrapinghub.com/api/scrapyd/\nusername = 180128bc7a0.....50e8290dbf3b0\npassword = \nproject = 28814\nIn order to deploy the spider, we will use the shub  tool provided by Scrapinghub. \nWe can install it with pip install shub , and it is already installed on our dev \nmachine. We can log in to Scrapinghub using shub login , as follows:\n$ shub login\nInsert your Scrapinghub API key : 180128bc7a0.....50e8290dbf3b0\nSuccess.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf98c7ef-0995-4b43-bec1-89bf43575f1c": {"__data__": {"id_": "bf98c7ef-0995-4b43-bec1-89bf43575f1c", "embedding": null, "metadata": {"page_label": "101", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cf45b88-1dfb-4dc5-83ce-3228c341e2b0", "node_type": "4", "metadata": {"page_label": "101", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "5e0147a22191da227ea4172ad731e6c089bad4052e05ec4012b90f446b42c25e", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 6[ 101 ]We already copied and pasted our API key on the scrapy.cfg  file, but we can also find \nit by clicking on our username on the upper-right side on Scrapinghub's website and \nthen API Key . In any case, we are now ready to deploy the spider using shub deploy :\n$ shub deploy\nPacking version 1449092838\nDeploying to project \"28814\" in {\"status\": \"ok\", \"project\": 28814, \n\"version\": \"1449092838\", \"spiders\": 1}\nRun your spiders at: https://dash.scrapinghub.com/p/28814/\nScrapy packs all the spiders from this project and uploads them to Scrapinghub. \nWe will notice two new directories and a file. These are auxiliary, and we can safely \ndelete them if we wish although typically I don't mind them:\n$ ls\nbuild project.egg-info properties scrapy.cfgsetup.py\n$ rm -rf build project.egg-info setup.py\nNow, if we click on the Spiders  section (1) in Scrapinghub, we will find the tomobile  \nspider that we've just deployed:\nSelecting the spider", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34a13c87-334b-492b-b2a7-aa445877b052": {"__data__": {"id_": "34a13c87-334b-492b-b2a7-aa445877b052", "embedding": null, "metadata": {"page_label": "102", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "188e509c-5349-406e-981a-ddb9c0deaf80", "node_type": "4", "metadata": {"page_label": "102", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "2df1e8bb05067673d386ac5317173be427cd878fd854ad7fa52a832a88a0cc2f", "class_name": "RelatedNodeInfo"}}, "text": "Deploying to Scrapinghub[ 102 ]If we click on it (2), we get to the spider dashboard. This has lots of information, but \nall we need to do is click the green Schedule  button (3) on the top-right corner, and \nthen the second Schedule  button (4) on the popup.\nScheduling a spider run\nAfter a few seconds, we will notice a new row on the Running Jobs  section of the \npage and a bit later the number of Requests  and Items  increasing (5).\nYou will likely not reduce scraping speed compared to your dev \nruns. Scrapyhub uses an algorithm to estimate the number of \nrequests per second that you can perform without getting banned.\nLet it run for a while, and then select the checkbox of this job (6) and click Stop  (7).\nAfter a few more seconds our job will end up on the Completed Jobs  section. In \norder to inspect scraped items, we can click on the number of items link (8).\nAccessing our items\nThis takes us to the job dashboard page. From this page we can inspect our items (9) \nand make sure that they look okay. We can also filter them with the controls above \nthose items. As we scroll down the page, more items get automatically loaded.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4148335-4999-4e3c-91ff-f92df512baaa": {"__data__": {"id_": "c4148335-4999-4e3c-91ff-f92df512baaa", "embedding": null, "metadata": {"page_label": "103", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fb8712d-f8a5-4e20-b1d1-a0889a32e6be", "node_type": "4", "metadata": {"page_label": "103", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c4283780f5974fafc6e680450267a277926c648893ebd91db231d222cf33b838", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 6[ 103 ]\nInspecting and exporting items\nIf something went wrong, we can find useful information on our Requests  and \nLog just above Items  (10). We can navigate back to the spider or project using the \nbreadcrumbs at the top (11). Of course, we can download our items in the usual CSV, \nJSON, and JSON Lines formats by clicking on the green Items  button on the top left \n(12) and then selecting the appropriate option (13).\nAnother way to access our items is through Scrapinghub's Items API. All we need to \ndo is have a look at the URL of this job's or items' page. It will look similar to this:\nhttps://dash.scrapinghub.com/p/28814/job/1/1/\nOn this URL, 28814  is the project number (we have also set this in the scrapy.cfg  file \nbefore) then the first 1 is the number/ID of this spider (the one named \" tomobile \"), \nand the second 1 is the number of the job. Using these three numbers in this order, we \ncan use curl  from our console to retrieve our items by making a request to https://\nstorage.scrapinghub.com/items/<project id>/<spider id>/<job id>  and \nusing our username/API key to authenticate, as follows:\n$ curl -u 180128bc7a0.....50e8290dbf3b0: https://storage.scrapinghub.com/\nitems/28814/1/1\n{\"_type\":\"PropertiesItem\",\"description\":[\"same\\r\\nsmoking\\r\\nr...\n{\"_type\":\"PropertiesItem\",\"description\":[\"british bit keep eve...\n... \nIf it asks for a password, we can leave it blank. Having programmatic access to our \ndata allows us to write applications that use Scrapinghub as a data storage backend. \nPlease keep in mind though that data won't be stored indefinitely but for a limited \ntime depending on our subscription plan (seven days for the free plan).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22791e37-5bdb-487e-a96f-c328bdaa02bf": {"__data__": {"id_": "22791e37-5bdb-487e-a96f-c328bdaa02bf", "embedding": null, "metadata": {"page_label": "104", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e30d9eea-520f-402a-8e91-da6d4cd8e348", "node_type": "4", "metadata": {"page_label": "104", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "dbd9c547a6c9c8dfd6860db554ae0afbcca6741387709df926f8aa1bebc02013", "class_name": "RelatedNodeInfo"}}, "text": "Deploying to Scrapinghub[ 104 ]Scheduling recurring crawls\nI guess that by now you won't be surprised to hear that scheduling recurrent crawls \nis a matter of just a few clicks.\nScheduling recurrent crawls\nWe just go to the Periodic Jobs  section (1), click Add  (2), set the spider (3), adjust the \ncrawling frequency (4), and finally click Save  (5).\nSummary\nIn this chapter, we had our first experience with deploying a Scrapy project to the \ncloud using Scrapinghub. We scheduled a run and collected thousands of items \nthat we were able to browse and extract easily by using the API. In the following \nchapters, we will keep building our knowledge up to the level of setting up a small \nScrapinghub-like server for ourselves. We start in the next chapter by exploring \nconfiguration and management.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f35b98a3-2ea7-4cab-922c-739dca9256ef": {"__data__": {"id_": "f35b98a3-2ea7-4cab-922c-739dca9256ef", "embedding": null, "metadata": {"page_label": "105", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e2c8dd6-9a3c-4920-b630-4ad43af77f2f", "node_type": "4", "metadata": {"page_label": "105", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "308f557522e89f18a9a01ca964dc647d3718e82c8cabb83960c44c99b1e70bab", "class_name": "RelatedNodeInfo"}}, "text": "[ 105 ]Configuration and \nManagement\nWe just saw how easy it is to develop a simple spider with Scrapy and use it to \nextract data from the Web. Scrapy comes with lots of utilities and functionality that \nis made available through its settings. For many software frameworks, settings are \nthe \"geeky stuff\" that fine tune how the system works. For Scrapy, settings are one of \nthe most fundamental mechanisms, which, beyond tuning and configuration, enable \nfunctionality and allow us to extend the framework. We don't aim to compete with \nthe excellent Scrapy documentation but supplement it by helping you navigate the \nsettings landscape faster and find the ones that are most relevant to you. Please read \nthe details on the documentation before you move your changes to production.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd997ac1-cd96-4a8a-840e-843a500dc6db": {"__data__": {"id_": "fd997ac1-cd96-4a8a-840e-843a500dc6db", "embedding": null, "metadata": {"page_label": "106", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73362b9f-f597-4ee5-862f-802e822f499f", "node_type": "4", "metadata": {"page_label": "106", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7ad8dba3c84b13283b532d8103bcdb7302dede5c84ca630d717c6381a6fc8b8f", "class_name": "RelatedNodeInfo"}}, "text": "Configuration and Management[ 106 ]Using Scrapy settings\nWith Scrapy, you can modify settings in five levels of increasing priority. We will see \neach of them in turn. The first level is the default settings. You wouldn't typically need \nto modify the default settings but scrapy/settings/default_settings.py  (in your \nsystem's Scrapy source code or Scrapy's GitHub) is certainly an interesting read. The \ndefault settings get refined at command level. Practically speaking, you shouldn't ever \nhave to worry about this unless you are implementing custom commands. More often \nthan not, we modify settings just after the command level on our project's <project_\nname>/settings.py  file. Those settings apply on our current project only. This level \nis the most convenient because settings.py  gets packaged when we deploy the \nproject in a cloud service, and since it's a file, we can adjust tens of settings easily \nwith our favourite text editor. The next level is its per-spider settings level. By using a \ncustom_settings  attribute in our spider definitions, we can easily customize settings \nper spider. This would allow us, for example, to enable or disable Item Pipelines for \na given spider. Finally, for some last-minute modifications, we can pass settings on \nthe command line using the -s command-line parameter. We have already done this \nseveral times by setting, for example -s CLOSESPIDER_PAGECOUNT=3 , which enables \nthe close spider extension and closes the spider early. In this level, we may want to set \nAPI secrets, passwords, and so on. Don't store these things in settings.py  because \nmost likely, you don't want them to accidentally end up checked in to some  \npublic repository.\nThroughout this section, we will examine some very important commonly-used \nsettings. To get a feeling of different types, try the following with any random project:\n$ scrapy settings --get CONCURRENT_REQUESTS\n16\nWhat you get is the default value. Then, modify your project's <project_name>/\nsettings.py  file and set a value for CONCURENT_REQUESTS , for example, 14. The \npreceding scrapy settings  command will give you the value that you just set\u2014don't \nforget to revert it. Then, by setting the argument explicitly from the command line you \nget the following:\n$ scrapy settings --get CONCURRENT_REQUESTS -s CONCURRENT_REQUESTS=19\n19\nThe preceding output hints at an interesting thing. scrapy crawl  and scrapy \nsettings  are just commands. Every command uses the methodology that we just \ndescribed to load settings. An example of this is as follows:\n$ scrapy shell -s CONCURRENT_REQUESTS=19\n>>> settings.getint('CONCURRENT_REQUESTS')\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7678093a-6c1b-43ed-b2f2-1ed5cdd6501f": {"__data__": {"id_": "7678093a-6c1b-43ed-b2f2-1ed5cdd6501f", "embedding": null, "metadata": {"page_label": "107", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14e656fc-262c-4b05-be19-3d8ae7ebaf0c", "node_type": "4", "metadata": {"page_label": "107", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ca6c046565e4c793d504293d666107aff48161f73bb03b4eae336a73bb0082d9", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 7[ 107 ]Whenever you need to find out what the effective value for a setting is within  \nyour project, use one of the preceding methods. We will now have a closer look at \nScrapy's settings.\nEssential settings\nScrapy has so many settings that categorizing them becomes an urgent necessity. We \nstart with the most essential settings that we summarize in the following diagram. \nThey give you awareness of the important system features and you may find \nyourself adjusting them somewhat frequently.\nEssential Scrapy settings\nAnalysis\nBy using those settings you can configure the way Scrapy provides performance and \ndebugging information via logs, statistics, and the telnet facility.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dacff4fc-3f4d-44c5-adb5-44bb6537064e": {"__data__": {"id_": "dacff4fc-3f4d-44c5-adb5-44bb6537064e", "embedding": null, "metadata": {"page_label": "108", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c967edc4-6a3a-43bc-bb8f-0f443dc3ad91", "node_type": "4", "metadata": {"page_label": "108", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "af9ba61d39ece2255af2e7cf26de649c51a5f3dad9c8a2f0367ed2e8ee9804c2", "class_name": "RelatedNodeInfo"}}, "text": "Configuration and Management[ 108 ]Logging\nScrapy has different levels of logs based on their severity: DEBUG  (lowest level), INFO , \nWARNING , ERROR , and CRITICAL  (highest level). Beyond that, there's a SILENT  level \nthat you can use to get no logging whatsoever. You can restrict the log file to only \naccept logs above a certain level by setting LOG_LEVEL  to the minimum desired level. \nWe often set this value to INFO  because DEBUG  is a bit too verbose. One very useful \nScrapy extension is the Log Stats extension, which prints the number of items and \npages scraped per minute. Logging frequency is set with the LOGSTATS_INTERVAL  \nsetting to a default value of 60 seconds. This may be a bit too infrequent. While \ndeveloping, I like to set that to five seconds because most runs are short. Logs \nare written to the file that is set in LOG_FILE . Unless set, the output will go to the \nstandard error except if logging gets explicitly disabled by setting the LOG_ENABLED  \nsetting to False . Finally, you can tell Scrapy to record all of its standard output (for \nexample, \"print\" messages) to the log by setting LOG_STDOUT  to true.\nStats\nSTATS_DUMP  is enabled by default, and it dumps values from the Stats Collector to \nthe log once the spider is finished. You can control whether stats are recorded for the \ndownloader by setting DOWNLOADER_STATS  to False . You can also control whether \nstats are collected for site depth through the DEPTH_STATS  setting. For more detailed \ninformation on depth, set DEPTH_STATS_VERBOSE  to True . STATSMAILER_RCPTS  is a \nlist (for example, set to ['my@mail.com'] ) of e-mails to send stats to when a crawl \nfinishes. You won't adjust these settings that often but they can occasionally help \nwith debugging.\nTelnet\nScrapy includes a built-in telnet console that gives you a Python shell to the \nrunning Scrapy process. TELNETCONSOLE_ENABLED  is enabled by default, while \nTELNETCONSOLE_PORT  determines the port(s) that is used to connect to the console. \nYou may need to alter them in case of a conflict.\nExample 1 \u2013 using telnet\nThere will be cases where you would like to have a look on the internal status of \nScrapy while it is running. Let's see how we can do that with the telnet console:\nThe code from this chapter is in the ch07  directory. This \nexample in particular is in the ch07/properties  directory.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3294d776-e4ba-4bb6-8cad-113fa51f42ae": {"__data__": {"id_": "3294d776-e4ba-4bb6-8cad-113fa51f42ae", "embedding": null, "metadata": {"page_label": "109", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b78c2017-73bb-4dbb-944a-cc3545317c0f", "node_type": "4", "metadata": {"page_label": "109", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c2acd05bc32cf611c8aa04513ef69ea51e22a57cf7a5a6cb45300174b59f4db7", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 7[ 109 ]$ pwd\n/root/book/ch07/properties\n$ ls\nproperties  scrapy.cfg\nStart a crawl as follows:\n$ scrapy crawl fast\n...\n[scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023:6023\nThe preceding message means that telnet is activated and listening in port 6023 . \nNow on another terminal, use the telnet command to connect to it:\n$ telnet localhost 6023\n>>>\nNow, this console gives you a Python console inside Scrapy. You can inspect several \ncomponents, such as the engine using the engine  variable, but in order to get a quick \noverview of the status, you can use the est()  command:\n>>> est()\nExecution engine status\ntime()-engine.start_time                        : 5.73892092705\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 8\n...\nlen(engine.slot.inprogress)                     : 10\n...\nlen(engine.scraper.slot.active)                 : 2\nWe will explore a few of the metrics there in Chapter 10 , Understanding Scrapy's \nPerformance . You will notice that you are still running this inside the Scrapy  \nengine. Let's assume that you use the following command:\n>>> import time\n>>> time.sleep(1) # Don't do this!\nYou will notice a short pause in the other terminal. Certainly this console isn't the \nright place to calculate the first million digits of Pi. Some other interesting things you \ncan do in the console are to pause, continue, and stop the crawl. You may find these \nand the terminal in general very useful while working with Scrapy sessions in  \nremote machines:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b59f1de-1355-4908-9904-f96b8849f0eb": {"__data__": {"id_": "1b59f1de-1355-4908-9904-f96b8849f0eb", "embedding": null, "metadata": {"page_label": "110", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e79a7249-4bcc-4c2f-a34b-411c57c0e680", "node_type": "4", "metadata": {"page_label": "110", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b322a4d17bf9d7895f70c0a3a7c8cda693f99157ccf2289573dfbb862fd1b21e", "class_name": "RelatedNodeInfo"}}, "text": "Configuration and Management[ 110 ]>>> engine.pause()\n>>> engine.unpause()\n>>> engine.stop()\nConnection closed by foreign host.\nPerformance\nWe will have a detailed look at these settings in Chapter 10 , Understanding Scrapy's \nPerformance , but here is a little summary. Performance settings let you adjust the \nperformance characteristics of Scrapy to your particular workload. CONCURRENT_\nREQUESTS  sets the maximum number of requests to be performed simultaneously. \nThis mostly protects your server's outbound capacity in case you are crawling many \ndifferent websites (domains/IPs). Unless that's the case, you will typically find \nCONCURRENT_REQUESTS_PER_DOMAIN  and CONCURRENT_REQUESTS_PER_IP  more \nrestrictive. These two protect remote servers by limiting the number of simultaneous \nrequests for each unique domain or IP address, respectively. If CONCURRENT_\nREQUESTS_PER_IP  is non-zero, CONCURRENT_REQUESTS_PER_DOMAIN  gets ignored. \nThese settings are not per second. If CONCURRENT_REQUESTS  = 16 and the average \nrequest takes a quarter of a second, your limit is 16 / 0.25 = 64  requests per second. \nCONCURRENT_ITEMS  sets the maximum number of items from each response that \ncan be processed simultaneously. You may find this setting way less useful than it \nseems because quite often there's a single Item  per page/request. The default value \nof 100 is also quite arbitrary. If you reduce it to, for example, 10 or 1 you might even \nsee performance gains depending on the number of Item s/request as well as how \ncomplex your pipelines are. You will also note that as this value is per request, if you \nhave a limit of CONCURRENT_REQUESTS  = 16, CONCURRENT_ITEMS  = 100 might mean up \nto 1600 items concurrently trying to be written in your databases, and so on. I would \nprefer a little bit more conservative value for this setting in general.\nFor downloads, DOWNLOAD_TIMEOUT  determines the amount of time the downloader \nwill wait before canceling a request. This is 180 seconds by default, which by all \nmeans seems excessive (with 16 concurrent requests this would mean five pages/\nminute for a site that is down). I would recommend reducing it to, for example, \n10 seconds if you have timeout issues. By default, Scrapy sets the delay between \ndownloads to zero to maximize scraping speed. You can modify this to apply a more \nconservative download speed using the DOWNLOAD_DELAY  setting. There are websites \nthat measure the frequency of requests as an indication of \"bot\" behavior. By setting \nDOWNLOAD_DELAY , you also enable a \u00b150% randomizer on download delay. You can \ndisable this feature by setting RANDOMIZE_DOWNLOAD_DELAY  to False .\nFinally, for faster DNS lookups, an in-memory DNS cache is enabled by default via \nthe DNSCACHE_ENABLED  setting.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31586065-9298-446f-b62a-d80751687c55": {"__data__": {"id_": "31586065-9298-446f-b62a-d80751687c55", "embedding": null, "metadata": {"page_label": "111", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7bd7f6f-6002-4892-af39-65dd40f50eb0", "node_type": "4", "metadata": {"page_label": "111", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "72e92c9c00bdf56d895c7ce7f745256e84bc02853d8e618fcc857202231f1fb0", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 7[ 111 ]Stopping crawls early\nScrapy's CloseSpider extension automatically stops a spider crawl when a condition \nis met. You can configure the spider to close after a period of time, after a number \nof items have been scraped, after a number of responses have been received, or after \na number of errors have occurred using the CLOSESPIDER_TIMEOUT  (in seconds), \nCLOSESPIDER_ITEMCOUNT , CLOSESPIDER_PAGECOUNT , and CLOSESPIDER_ERRORCOUNT  \nsettings, respectively. You will usually set them from the command line while \nrunning the spider as we've done a few times in previous chapters:\n$ scrapy crawl fast -s CLOSESPIDER_ITEMCOUNT=10\n$ scrapy crawl fast -s CLOSESPIDER_PAGECOUNT=10\n$ scrapy crawl fast -s CLOSESPIDER_TIMEOUT=10\nHTTP caching and working offline\nScrapy's HttpCacheMiddleware  component (deactivated by default) provides a \nlow-level cache for HTTP requests and responses. If enabled, the cache stores every \nrequest and its corresponding response. By setting HTTPCACHE_POLICY  to scrapy.\ncontrib.httpcache.RFC2616Policy , we can enable a way more sophisticated \ncaching policy that respects website's hints according to RFC2616. To enable this \ncache, set HTTPCACHE_ENABLED  to True  and HTTPCACHE_DIR  to a directory on the \nfilesystem (using a relative path will create the directory in the project's data folder).\nYou can optionally specify a database backend for your cached files by setting \nthe storage backend class HTTPCACHE_STORAGE  to scrapy.contrib.httpcache.\nDbmCacheStorage  and, optionally, adjusting the HTTPCACHE_DBM_MODULE  setting \n(defaults to anydbm). There are a few more settings that fine-tune cache's behavior \nbut the defaults are likely to serve you fine.\nExample 2 \u2013 working offline by using the cache\nLet's assume that you run the following code:\n$ scrapy crawl fast -s LOG_LEVEL=INFO -s CLOSESPIDER_ITEMCOUNT=5000\nYou will notice that it takes about a minute to complete. If you didn't have access to \nthe web server though, you would be unable to crawl anything. Let's assume that \nyou now run the crawl again as follows:\n$ scrapy crawl fast -s LOG_LEVEL=INFO -s CLOSESPIDER_ITEMCOUNT=5000 -s \nHTTPCACHE_ENABLED=1\n...\nINFO: Enabled downloader middlewares:...*HttpCacheMiddleware*", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbd06ac2-fe81-4d34-bb23-7ca7ac23f581": {"__data__": {"id_": "bbd06ac2-fe81-4d34-bb23-7ca7ac23f581", "embedding": null, "metadata": {"page_label": "112", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64d808ba-1df0-4e93-a70a-6046f91fc9b6", "node_type": "4", "metadata": {"page_label": "112", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "2071dd75c2cbd3abb4e870f09d73a80622e135bc2a4d9a5fee4480fa9420911d", "class_name": "RelatedNodeInfo"}}, "text": "Configuration and Management[ 112 ]You will notice that HttpCacheMiddleware  got enabled, and if you look into the \nhidden directories on your current directory you will find a new .scrapy  directory \nas follows:\n$ tree .scrapy | head\n.scrapy\n\u2514\u2500\u2500 httpcache\n    \u2514\u2500\u2500 easy\n        \u251c\u2500\u2500 00\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 002054968919f13763a7292c1907caf06d5a4810\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 pickled_meta\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 request_body\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 request_headers\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 response_body\n...\nNow if you rerun your scrape even in the case when you don't have access to the \nweb server for a bit fewer items, you will notice it finishing faster:\n$ scrapy crawl fast -s LOG_LEVEL=INFO -s CLOSESPIDER_ITEMCOUNT=4500 -s \nHTTPCACHE_ENABLED=1\nWe use a bit fewer items as a limit because when stopping using CLOSESPIDER_\nITEMCOUNT , we often read a few more pages before the crawler stops completely, and \nwe wouldn't like to hit ones not available in our cache. To clean the cache, just delete the \ncache directory:\n$ rm -rf .scrapy\nCrawling style\nScrapy lets you adjust how it chooses which pages to crawl first. You can set a \nmaximum depth in the DEPTH_LIMIT  setting, with 0 meaning no limit. Requests can \nbe assigned priorities based on their depth through the DEPTH_PRIORITY  setting. \nMost notably this allows you to perform a Breadth First Crawl by setting this value \nto a positive number and switching scheduler's queues from LIFO to FIFO:\nDEPTH_PRIORITY = 1\nSCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'\nSCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8d90375-da1d-4489-a9fd-c1a34cad52bf": {"__data__": {"id_": "b8d90375-da1d-4489-a9fd-c1a34cad52bf", "embedding": null, "metadata": {"page_label": "113", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e29db17-a67a-45bb-b96f-5126cca22574", "node_type": "4", "metadata": {"page_label": "113", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "0af3b477f4d800dc08d7b076cac152af4038d70c5aabc3c8c0ff6a052fbb1e43", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 7[ 113 ]This is useful when you crawl, for example, a news portal that has the most recent \nnews closer to the home page while each news page has links to other related news. \nThe default Scrapy behavior would be to go as deeply as possible in the first few news \nstories in the home page and only after that continue with subsequent front-page \nnews. BFO order would crawl top-level news before proceeding further, and when \ncombined with a DEPTH_LIMIT , such as 3, it might allow you to quickly scan the latest \nnews on a portal.\nSites declare their crawler policies and hint at uninteresting parts of their structure \nwith a web-standard robots.txt  file in their root directory. Scrapy can take it into \nconsideration if you set the ROBOTSTXT_OBEY  setting to True . If you enable it, keep it \nin mind while debugging in case you notice any unexpected behavior.\nThe CookiesMiddleware  transparently takes care of all cookie-related operations, \nenabling among others, session tracking, which allows you to log in, and so \non. If you want to have more \"stealth\" crawling, you can disable this by setting \nCOOKIES_ENABLED  to False . Disabling cookies also slightly reduces the bandwidth \nthat you use and might speed up your crawling a little bit depending on the \nwebsite. Similarly, the REFERER_ENABLED  setting is True  by default, enabling \nRefererMiddleware , which populates Referer headers. You can define custom \nheaders using DEFAULT_REQUEST_HEADERS . You may find this useful for weird sites \nthat ban you unless you have particular request headers. Finally, the automatically \ngenerated settings.py  file recommends that we set USER_AGENT . This defaults to \nthe Scrapy version, but we should change it to something that allows website owners \nto be able to contact us.\nFeeds\nFeeds let you export the data that is scraped by Scrapy to the local filesystem or to a \nremote server. The location of the feed is determined by FEED_URI .FEED_URI  and may \nhave named parameters. For example, scrapy crawl fast -o \"%(name)s_%(time)\ns.jl\"  will automatically have the current time and spider name ( fast ) filled in on \nthe output file. If you needed a custom parameter, such as %(foo)s , feed exporter \nwill expect you to provide a foo attribute in your spider. The storage of the feed, \nsuch as S3, FTP, or local filesystem is defined in the URI as well. For example, FEED_\nURI='s3://mybucket/file.json'  will upload your file to Amazon's S3 using your \nAmazon credentials ( AWS_ACCESS_KEY_ID  and AWS_SECRET_ACCESS_KEY ). The format \nof the feed\u2014JSON, JSON Lines, CSV, and XML\u2014is determined by FEED_FORMAT . If it \nis not set, Scrapy will guess it according to the extension of FEED_URI . You can choose \nto export empty feeds by setting FEED_STORE_EMPTY  to true . You can also choose to \nexport only certain fields using the FEED_EXPORT_FIELDS  setting. This is particularly \nuseful for .csv  files that feature fixed header columns. Finally, FEED_URI_PARAMS  is \nused to define a function to postprocess any parameters to FEED_URI .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50bc80b1-5974-4341-990b-406f24607fee": {"__data__": {"id_": "50bc80b1-5974-4341-990b-406f24607fee", "embedding": null, "metadata": {"page_label": "114", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54bd53fb-b3c9-43b0-92d1-cbc7984ce1b4", "node_type": "4", "metadata": {"page_label": "114", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "66c76c34389d10764af49199e2352481be47cc1b24bf9a2ba80de988662ab87b", "class_name": "RelatedNodeInfo"}}, "text": "Configuration and Management[ 114 ]Downloading media\nScrapy can download media content using the Image Pipeline, which can also convert \nimages to different formats, generate thumbnails, and filter images based on size.\nThe IMAGES_STORE  setting sets the directory where images are stored (using a relative \npath will create a directory in the project's root folder). The URLs for the images for \neach Item  should be in its image_urls  field (this can be overridden by the IMAGES_\nURLS_FIELD  setting) and filenames for the downloaded images will be set to a new \nimages  field (this can be overridden by the IMAGES_RESULT_FIELD  setting). You can \nfilter out smaller images by setting IMAGES_MIN_WIDTH  and IMAGES_MIN_HEIGHT . \nIMAGES_EXPIRES  determines the number of days that images will be kept in the \ncache before they expire. For thumbnail generation, the IMAGES_THUMBS  setting lets \nyou define one or more thumbnails to generate along with their dimensions. For \ninstance, you could have Scrapy generate one icon-sized thumbnail and one medium \nthumbnail for each downloaded image.\nOther media\nYou can download other media files using the Files Pipeline. Similarly to images, \nFILES_STORE  determines where files get downloaded and FILES_EXPIRES  determines \nthe number of days that files are retained. The FILES_URLS_FIELD  and FILES_\nRESULT_FIELD  settings have similar functionality to their IMAGES_*  counterparts.  \nBoth the files and image pipelines can be active at the same time without conflict.\nExample 3 \u2013 downloading images\nIn order to use image functions, we have to install the image package with sudo pip \ninstall image . In our dev machine, this has already been done for us. To enable \nthe Image Pipeline, you just have to edit your project's settings.py  file and add a \nfew settings. The first one is including scrapy.pipelines.images.ImagesPipeline  \non your ITEM_PIPELINES . Also, set IMAGES_STORE  to a relative path \"images\", and \noptionally a description for some thumbnails by setting IMAGES_THUMBS , as follows:\nITEM_PIPELINES = {\n...\n    'scrapy.pipelines.images.ImagesPipeline': 1,\n}\nIMAGES_STORE = 'images'\nIMAGES_THUMBS = { 'small': (30, 30) }\nWe already have an image_urls  field set appropriately for our Item , so we are ready \nto run it as follows:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ba86243-8cac-41ee-b74b-c7746d4638e1": {"__data__": {"id_": "6ba86243-8cac-41ee-b74b-c7746d4638e1", "embedding": null, "metadata": {"page_label": "115", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90448a71-83ab-49dd-b4e8-c7bb32a0bb19", "node_type": "4", "metadata": {"page_label": "115", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "997f264aeccbc2ad43c0fc2f59d14d2c86caabf0a93a26e45dcff53dda80527a", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 7[ 115 ]$ scrapy crawl fast -s CLOSESPIDER_ITEMCOUNT=90\n...\nDEBUG: Scraped from <200 http://http://web:9312/.../index_00003.html/\nproperty_000001.html>{\n   'image_urls': [u'http://web:9312/images/i02.jpg'],\n   'images': [{'checksum': 'c5b29f4b223218e5b5beece79fe31510',\n               'path': 'full/705a3112e67...a1f.jpg',\n               'url': 'http://web:9312/images/i02.jpg'}],\n...\n$ tree images \nimages\n\u251c\u2500\u2500 full\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 0abf072604df23b3be3ac51c9509999fa92ea311.jpg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1520131b5cc5f656bc683ddf5eab9b63e12c45b2.jpg\n...\n\u2514\u2500\u2500 thumbs\n    \u2514\u2500\u2500 small\n        \u251c\u2500\u2500 0abf072604df23b3be3ac51c9509999fa92ea311.jpg\n        \u251c\u2500\u2500 1520131b5cc5f656bc683ddf5eab9b63e12c45b2.jpg\n...\nWe see that the images were successfully downloaded and thumbnails were created. \nThe JPG names for the main files get stored in the images  field as expected. It's easy \nto infer thumbnails' paths. To clean the images we can use rm -rf images .\nAmazon Web Services\nScrapy has built-in support to access Amazon web services. You can store your AWS \naccess key in the AWS_ACCESS_KEY_ID  setting, and you can store your secret key in \nthe AWS_SECRET_ACCESS_KEY  setting. Both of these settings are empty by default. \nThey are used as follows:\n\u2022 When you download URLs that start with s3:// (that instead of http://,  \nand so on)\n\u2022 When you use s3:// paths to store files or thumbnails with the  \nmedia pipelines\n\u2022 When you store your output Item  feed on an s3:// directory\nIt's a good idea to NOT store these settings in your settings.py  file in case it \nbecomes public one day for any reason.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e09c3867-c431-494b-87db-209906c84a9e": {"__data__": {"id_": "e09c3867-c431-494b-87db-209906c84a9e", "embedding": null, "metadata": {"page_label": "116", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7583019a-8eba-4fbf-b74a-3f19ff212db8", "node_type": "4", "metadata": {"page_label": "116", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "39c77ebd3e109fb43f31351e47f4356a45dad9b7b37b2d0bbcc811f192aaccfc", "class_name": "RelatedNodeInfo"}}, "text": "Configuration and Management[ 116 ]Using proxies and crawlers\nScrapy's HttpProxyMiddleware  component lets you use the proxy settings that are \ndefined by the http_proxy , https_proxy , and no_proxy  environment variables in \naccordance with the Unix convention. This component is enabled by default.\nExample 4 \u2013 using proxies and Crawlera's  \nclever proxy\nDynDNS (or any similar service) provides a free online tool to check your current \nIP address. Using a Scrapy shell, we'll make a request to checkip.dyndns.org  and \nexamine the response to find our current IP address:\n$ scrapy shell http://checkip.dyndns.org\n>>> response.body\n'<html><head><title>Current IP Check</title></head><body>Current IP \nAddress: xxx.xxx.xxx.xxx</body></html>\\r\\n'\n>>> exit()\nTo start proxying requests, exit the shell and use the export  command to set a new \nproxy. You can test a free proxy by searching through HMA's public proxy list \n(http://proxylist.hidemyass.com/ ). For example, let's assume that from this list, \nwe chose the proxy with IP 10.10.1.1  and port 80 (not a real one\u2014replace it with \nyour own), we have the following:\n$ # First check if you already use a proxy\n$ env | grep http_proxy\n$ # We should have nothing. Now let's set a proxy\n$ export http_proxy=http://10.10.1.1:80\nRerun the Scrapy shell, as we just did, and you will see that the request was \nperformed using a different IP. You will also notice that it will typically be quite \nslower, and in some cases it won't be successful, in which case you could try another \nproxy. To disable the proxy, exit the Scrapy shell and unset http_proxy  (or restore \nits previous value).\nCrawlera is a service by Scrapinghub and lead developers of Scrapy that acts \nlike a very clever proxy. Apart from using a large pool of IPs behind the scenes \nto route your requests, it also adjusts the delays and retries failures to give you a \nstable stream of successful responses as much as possible while remaining as fast \nas possible. It's essentially a scraper's dream come true, and you can use it just by \nsetting the http_proxy  environment variable as before:\n$ export http_proxy=myusername:mypassword@proxy.crawlera.com:8010", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86c0d7f7-e884-45be-954d-88d2116193d4": {"__data__": {"id_": "86c0d7f7-e884-45be-954d-88d2116193d4", "embedding": null, "metadata": {"page_label": "117", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b9ed295-4a6f-405f-8676-4e3c226d1d32", "node_type": "4", "metadata": {"page_label": "117", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ab6d3eae40661024b9da90012c3f4bc223807319de78893fc601cb0ae49ff958", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 7[ 117 ]Beyond HTTP proxy, Crawlera can also be used via its own middleware component \nfor Scrapy.\nFurther settings\nWe will now explore some less common aspects of Scrapy and settings related to \nextending Scrapy, which we will see in more detail in later chapters.\nFurther Scrapy settings", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81bb5a0f-67fe-4ae1-8f68-17342acc3df2": {"__data__": {"id_": "81bb5a0f-67fe-4ae1-8f68-17342acc3df2", "embedding": null, "metadata": {"page_label": "118", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf7d5fdc-0c39-4329-bc87-1e908af0b536", "node_type": "4", "metadata": {"page_label": "118", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "5cc8bc3cf650c5853f840e6867368282af72c39c5eb883da04097c156bb32624", "class_name": "RelatedNodeInfo"}}, "text": "Configuration and Management[ 118 ]Project-related settings\nUnder this umbrella, you will find housekeeping settings related to a specific project, \nsuch as BOT_NAME , SPIDER_MODULES , and so on. It's good to have a quick look at \nthem in the documentation because they may increase your productivity for specific \nuse cases, but typically, Scrapy's startproject  and genspider  commands provide \nsensible defaults and you may be okay without ever explicitly changing them.  \nMail-related settings, such as MAIL_FROM , allow you to configure the MailSender  \nclass, which is currently used to mail stats (see also: STATSMAILER_RCPTS ) and \nmemory usage (see also:  MEMUSAGE_NOTIFY_MAIL ). There are also two environment \nvariables, SCRAPY_SETTINGS_MODULE  and SCRAPY_PROJECT , that allow you to fine \ntune the way a Scrapy project integrates within, for example, a Django project. \nscrapy.cfg  also allows you to adjust the name of your settings module.\nExtending Scrapy settings\nThese are settings that allow you to extend and modify almost every aspect of \nScrapy. The king of these settings is definitely ITEM_PIPELINES . It allows you to \nuse Item Processing Pipelines on your projects. We will see many such examples in \nChapter 9 , Pipeline Recipies . Beyond pipelines, we can extend Scrapy in various ways, \nsome of them are summarized in Chapter 8 , Programming Scrapy . COMMANDS_MODULE  \nallows us to add custom commands. For example, let's assume that we add in a \nproperties/hi.py  file to the following:\nfrom scrapy.commands import ScrapyCommand\nclass Command(ScrapyCommand):\n    default_settings = {'LOG_ENABLED': False}\n    def run(self, args, opts):\n        print(\"hello\")\nAs soon as we add COMMANDS_MODULE='properties.hi'  on our settings.py  file, \nwe activate this trivial command making it show up in Scrapy's help and run with \nscrapy hi . The settings that are defined in a command's default_settings  get \nmerged into a project's settings overriding the defaults but with lower priority to \nsettings that defined on your settings.py  file or set in the command line.\nScrapy uses the -_BASE  dictionaries (for example, FEED_EXPORTERS_BASE ) to store \ndefault values for various framework extensions and then allows us to customize \nthem in our settings.py  file and command line by setting their non- _BASE  versions \nof them (for example, FEED_EXPORTERS ).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "576752bc-8452-4af8-b894-a2559fb1829b": {"__data__": {"id_": "576752bc-8452-4af8-b894-a2559fb1829b", "embedding": null, "metadata": {"page_label": "119", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa79d2c3-2afc-4ade-8b45-f59d50e199ad", "node_type": "4", "metadata": {"page_label": "119", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "30a8cb0bcf7ca8b3e61a3a916b5ea9a27f9be0c609b84d4e8581e279985ab406", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 7[ 119 ]Finally, Scrapy uses settings, such as DOWNLOADER  or SCHEDULER , which hold \npackage/class names for essential components of the system. We could potentially \ninherit from the default downloader ( scrapy.core.downloader.Downloader ), \noverload a few methods, and then set our custom class on the DOWNLOADER  setting. \nThis allows developers to experiment wildly with experimental features and eases \nautomated testing, but you shouldn't ever have to modify them unless you really \nknow what you're doing.\nFine-tuning downloading\nThe RETRY_ *, REDIRECT_ *, and METAREFRESH_ * settings configure the Retry, Redirect \nand Meta-Refresh middleware, respectively. For example, REDIRECT_PRIORITY_\nADJUST  set to 2 means that every time there's a redirect, the new request will be \nscheduled after all non-redirected requests get served, and REDIRECT_MAX_TIMES  \nset to 20 means that after 20 redirects the downloader will give up and return \nwhatever it has. It's nice to be aware of these settings in case you crawl some ill-cased \nwebsites, but the default values will serve you fine in most cases. The same applies to \nHTTPERROR_ALLOWED_CODES  and URLLENGTH_LIMIT .\nAutothrottle extension settings\nThe AUTOTHROTTLE_ * settings enable and configure the autothrottle extension. This \ncomes with a great promise, but in practice, I find that it tends to be somewhat \nconservative and difficult to tune. It uses download latencies to get a feeling of how \nloaded our and the target server are and adjusts downloader's delay accordingly. If \nyou have a hard time finding the best value for DOWNLOAD_DELAY  (defaults to 0), you \nshould find this module useful.\nMemory UsageExtension settings\nThe MEMUSAGE_ * settings enable and configure the memory usage extension. This \nshuts down the spider when it exceeds a memory limit. This could be useful in \na shared environment where processes have to be very polite. More often, you \nmay find it useful to receive just its warning e-mail by disabling the shut down \nfunctionality by setting MEMUSAGE_LIMIT_MB  to 0. This extension works only on \nUnix-like platforms.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98177f92-4733-42a7-bfd3-3f25b500ba46": {"__data__": {"id_": "98177f92-4733-42a7-bfd3-3f25b500ba46", "embedding": null, "metadata": {"page_label": "120", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "412db2f0-0a38-4e23-99fd-1ebf965970b0", "node_type": "4", "metadata": {"page_label": "120", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "536e65b19c9ff869a20f61a0cca7af022d8f60ee19c622365b7b293ddeaef726", "class_name": "RelatedNodeInfo"}}, "text": "Configuration and Management[ 120 ]MEMDEBUG_ENABLED  and MEMDEBUG_NOTIFY  enable and configure the memory \ndebugger extension, printing the number of live references on spider close. Overall, \nI would say that chasing memory leaks isn't fun or easy (okay, it might be a bit fun). \nRead the excellent documentation on Debugging memory leaks with trackref , but most \nimportantly, I would suggest keeping your crawls relatively short, batched, and in \naccordance with your server's capacity. I think there's no good reason to run batches \nof more than a few thousand pages or more than a few minutes long.\nLogging and debugging\nFinally, there are a few logging and debugging functions. LOG_ENCODING , LOG_\nDATEFORMAT  and LOG_FORMAT  let you fine tune your logging formats, which you \nmay find useful if you intend to use log-management solutions, such as Splunk, or \nLogstash, and Kibana. DUPEFILTER_DEBUG  and COOKIES_DEBUG  will help you debug \nrelatively complex situations where you get less than expected requests, or your \nsessions get lost unexpectedly.\nSummary\nBy reading this chapter, I'm sure you appreciate the depth and breadth of the \nfunctionality that you get using Scrapy when compared with a crawler that you \nmight write from scratch. If you need to fine-tune or extend Scrapy's functionality, \nyou have plenty of options, as we will see in the following chapters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1381, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68021843-5627-4056-8fb4-c2e5aa00f90a": {"__data__": {"id_": "68021843-5627-4056-8fb4-c2e5aa00f90a", "embedding": null, "metadata": {"page_label": "121", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b458b23-f178-4f3c-98b8-afde6ae5afff", "node_type": "4", "metadata": {"page_label": "121", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "4899d9ab5a65526d4450a16937cde86ea8dbd898a48208d224a4957bed3f123d", "class_name": "RelatedNodeInfo"}}, "text": "[ 121 ]Programming Scrapy\nUp to this point, we wrote spiders whose main responsibility is to define the \nway we crawl data sources and how we extract information from them. Beyond \nspiders, Scrapy provides mechanisms that allow us to fine-tune most aspects of its \nfunctionality. For example, you may often find yourself dealing with some of the \nfollowing problems:\n1. You copy and paste lots of code among spiders of the same project. The \nrepeated code is more related to data (for example, performing calculations \non fields) rather than data sources.\n2. You have to write scripts that postprocess Item s doing things like dropping \nduplicate entries or postprocessing values.\n3. You have repeated code across projects to deal with infrastructure. \nFor example, you might need to log in and transfer files to proprietary \nrepositories, add Item s to databases, or trigger postprocessing operations \nwhen crawls complete.\n4. You find aspects of Scrapy that are not exactly as you wish, and you need to \napply customizations or workarounds on many of your projects.\nScrapy developers designed its architecture in a way that allows us to solve such \nrecurrent problems. We will investigate this architecture later in this chapter.  \nFirst though, let's start with an introduction to the engine that powers Scrapy. It's \ncalled Twisted .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67b5cc36-765c-4750-a1f6-42e799fbd1b0": {"__data__": {"id_": "67b5cc36-765c-4750-a1f6-42e799fbd1b0", "embedding": null, "metadata": {"page_label": "122", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83bb0a77-8bbd-45d6-8d8b-442686baa93f", "node_type": "4", "metadata": {"page_label": "122", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "fd3bbc18612872dc0dbc4bb344534584cb2851da1cbf0eca4ad5987cc86e6cc8", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 122 ]Scrapy is a Twisted application\nScrapy is a scraping application built using the Twisted Python framework. Twisted \nis indeed somewhat unusual because it's event-driven and encourages us to write \nasynchronous code. Getting used to it takes some time, but we will make our task \neasier by studying only the parts of it that are relevant to Scrapy. We will also be a \nbit relaxed in terms of error handling. The full code on GitHub has more thorough \nerror handling, but we will skip it for this book.\nLet's start from the beginning. What makes Twisted different is its main mantra.\nDo not, under any circumstances, write code that blocks.\nThe implications are severe. Code that might block includes:\n\u2022 Code that accesses files, databases or the Web\n\u2022 Code that spawns new processes and consumes their output, for example, \nrunning shell commands\n\u2022 Code that performs hacky system-level operations, for example, waiting for \nsystem queues\nTwisted provides us with methods that allow us to perform all these and many more \nwithout blocking code execution.\nTo showcase the difference, let's assume that we have a typical synchronous scrapping \napplication. It has, for example, four threads, and at a given moment, three of them \nare blocked waiting for responses, and one of them is blocked performing a database \nwrite access to persist an Item . At any given moment, it's quite unlikely to find a \ngeneral-purpose thread of a scrapping application doing anything else but waiting \nfor some blocking operation to complete. When blocking operations complete, some \ncomputations may take place for a few microseconds and then threads block again on \nother blocking operations that likely last for at least a few milliseconds. Overall the \nserver isn't idle because it runs tens of applications utilizing thousands of threads, thus, \nafter some careful tuning, CPUs remain reasonably utilized.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ffc0892-86ea-4d16-bc1c-fe67fc19732c": {"__data__": {"id_": "6ffc0892-86ea-4d16-bc1c-fe67fc19732c", "embedding": null, "metadata": {"page_label": "123", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa89f926-9c71-4505-9d25-560d16ed12b8", "node_type": "4", "metadata": {"page_label": "123", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a1022103dd518635fe255eb1e88cd1ca90eae3872a44f8311c3fdd3066e6ea40", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 123 ]\nMultithreaded code versus Twisted asynchronous code\nTwisted/Scrapy's approach favors using a single thread as much as possible. It uses \nmodern Operating System's I/O multiplexing functions (see select() , poll() , \nand epoll() ) as a \"hanger\". Where we would typically have a blocking operation, \nfor example result = i_block() , Twisted provides us with an alternative \nimplementation that returns immediately. However, it doesn't return the actual value \nbut a hook, for example deferred = i_dont_block() , where we can hang whatever \nfunctionality we want to run whenever the value becomes available (for example, \ndeferred.addCallback(process_result) ). A Twisted application is made of chains \nof such deferred operations. The single main Twisted thread is called a Twisted Event \nReactor thread and it monitors the hanger until some resource becomes available (for \nexample, a server response to our Request s). When this happens, it fires the topmost \ndeferred in the chain, which performs some computations and, in turn, fires the next \none. Some of these deferreds might initiate further I/O operations, which will bring \nthe chain of deferreds back to the hanger and free the CPU to perform other work, if \navailable. Since we are single-threaded, we don't suffer the costs of context switches \nand save resources (like memory) that extra threads require. In other words, using \nthis nonblocking infrastructure, we get performance that is similar to if we had \nthousands of threads, while using a single one.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "609f97b6-26cb-4808-a87f-5ce928d6c058": {"__data__": {"id_": "609f97b6-26cb-4808-a87f-5ce928d6c058", "embedding": null, "metadata": {"page_label": "124", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da579029-b1e5-494e-94da-2b4308c358fd", "node_type": "4", "metadata": {"page_label": "124", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "44f93e63777ba1fab160287054909db2e5c3ca3d5da20aa32484383c1dd8a73c", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 124 ]To be perfectly honest, OS developers have been optimizing thread operations for \ndecades making them very fast. The performance argument is not as strong as it used \nto be. One thing that everyone can agree on though, is that writing correct thread-\nsafe code for complex applications is very difficult. After you get over the initial \nshock of having to think in terms of deferreds/callbacks, you will find Twisted code \nsignificantly simpler than threaded code. The inlineCallbacks  generator utility \nmakes code even simpler. We will explore them further in the following sections.\nArguably, the most successful nonblocking I/O system until now \nis Node.js, mainly because it started with high performance/\nconcurrency in mind, and nobody argued about whether that's a \ngood or bad thing. Every Node.js application uses just nonblocking \nAPIs. In the Java world, Netty is probably the most successful NIO \nframework powering applications, such as Apache Storm and \nSpark. C++11's std::future  and std::promise  (quite similar to \ndeferreds) make it easier to write asynchronous code using libraries, \nsuch as libevent or plain POSIX.\nDeferreds and deferred chains\nDeferreds are the most essential mechanism that Twisted offers to help us write \nasynchronous code. Twisted APIs use deferreds to allow us to define sequences of \nactions that take place when certain events occur. Let's have a look at them.\nYou can get all the source code of this book from GitHub. To \ndownload this code go to git clone https://github.com/\nscalingexcellence/scrapybook\nThe full code from this chapter will be in the ch08  directory, and \nfor this example in particular, in the ch08/deferreds.py  file, \nand you can run it with ./deferreds.py 0 .\nYou can use a Python console to run the following experiments interactively:\n$ python\n>>> from twisted.internet import defer\n>>> # Experiment 1\n>>> d = defer.Deferred()\n>>> d.called\nFalse\n>>> d.callback(3)\n>>> d.called\nTrue", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13caefff-6538-41c8-8fe3-7016614e0cbb": {"__data__": {"id_": "13caefff-6538-41c8-8fe3-7016614e0cbb", "embedding": null, "metadata": {"page_label": "125", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68b9ca2b-3cdc-4e47-ab12-b55a24d96994", "node_type": "4", "metadata": {"page_label": "125", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b040d38f0163e2709dadc2657547a362c10863be58f987c358a34066d40cba4e", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 125 ]>>> d.result\n3\nWhat we see is that Deferred  is essentially a thing representing a value that we \ndon't have immediately. When we fire d (call its callback  method) it's called  state \nbecomes True , and the result  attribute is set to the value that we set on the callback:\n>>> # Experiment 2\n>>> d = defer.Deferred()\n>>> def foo(v):\n...     print \"foo called\"\n...     return v+1\n... \n>>> d.addCallback(foo)\n<Deferred at 0x7f...>\n>>> d.called\nFalse\n>>> d.callback(3)\nfoo called\n>>> d.called\nTrue\n>>> d.result\n4\nThe most powerful feature of deferreds is that we can chain other operations to be \ncalled when a value is set. In the last example, we add a foo()  function as a callback \nfor d. When we fire d by calling callback(3) , function foo()  gets called printing the \nmessage, and the value that it returns is set as the final result  value for d:\n>>> # Experiment 3\n>>> def status(*ds):\n...     return [(getattr(d, 'result', \"N/A\"), len(d.callbacks)) for d in \nds]\n>>> def b_callback(arg):\n...     print \"b_callback called with arg =\", arg\n...     return b\n>>> def on_done(arg):\n...     print \"on_done called with arg =\", arg\n...     return arg\n>>> # Experiment 3.a\n>>> a = defer.Deferred()\n>>> b = defer.Deferred()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25cc1f12-e825-4d6d-8bad-3e6f99b6b7d6": {"__data__": {"id_": "25cc1f12-e825-4d6d-8bad-3e6f99b6b7d6", "embedding": null, "metadata": {"page_label": "126", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "faa98306-561d-4487-bb4c-3457f11054e8", "node_type": "4", "metadata": {"page_label": "126", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "1ad73b5a42b2a236946d0bf4923a35c0ff415da7d32fbdab74d17f59aee8099a", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 126 ]>>> a.addCallback(b_callback).addCallback(on_done)\n>>> status(a, b)\n[('N/A', 2), ('N/A', 0)]\n>>> a.callback(3)\nb_callback called with arg = 3\n>>> status(a, b)\n[(<Deferred at 0x10e7209e0>, 1), ('N/A', 1)]\n>>> b.callback(4)\non_done called with arg = 4\n>>> status(a, b)\n[(4, 0), (None, 0)]\nThis example gets us to more complex deferred behaviors. We see a normal \ndeferred, a, set up exactly as before, but now it has two callbacks. The first one is \nb_callback() , which returns a b deferred instead of a value. The second one is the \non_done()  printing function. We also have a little status()  function that prints \nthe status of deferreds. After the initial setup in both cases, we have the same state, \n[('N/A', 2), ('N/A', 0)] , meaning that both deferreds haven't been fired, and \nthe first one has two callbacks, while the second one has none. Then, if we fire a first, \nwe get into a weird [(<Deferred at 0x10e7209e0>, 1), ('N/A', 1)]  state, \nwhich shows that a now has a value, which is a deferred (the b deferred actually), \nand it also has a single callback, which is reasonable because b_callback()  has \nalready been called and only on_done()  is left. The unexpected fact is that b now has \na callback. Indeed a registered behind the scenes a callback, which will update its \nvalue as soon as b gets fired. Once this happens, on_done()  also gets called and the \nfinal state is [(4, 0), (None, 0)] , which is exactly what we expected:\n>>> # Experiment 3.b\n>>> a = defer.Deferred()\n>>> b = defer.Deferred()\n>>> a.addCallback(b_callback).addCallback(on_done)\n>>> status(a, b)\n[('N/A', 2), ('N/A', 0)]\n>>> b.callback(4)\n>>> status(a, b)\n[('N/A', 2), (4, 0)]\n>>> a.callback(3)\nb_callback called with arg = 3\non_done called with arg = 4\n>>> status(a, b)\n[(4, 0), (None, 0)]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d3a096a-7a30-44ed-b67b-e302b888be7b": {"__data__": {"id_": "0d3a096a-7a30-44ed-b67b-e302b888be7b", "embedding": null, "metadata": {"page_label": "127", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a4c0e3a-68f2-416b-b32e-a35b32c28d1e", "node_type": "4", "metadata": {"page_label": "127", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "0c802f136ac5884fd7d59a7e98c94497beb6ec3b2babdb204dd944ef2e767b50", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 127 ]On the other hand, if b gets fired before a as experiment 3.b shows, the status \nbecomes [('N/A', 2), (4, 0)] , and then when a gets fired both callbacks get \ncalled and the final state ends up being the same as before. It's interesting to note that \nregardless of the order, the result is the same. The only difference between the two \ncases is that in the first case, the value of b value remains deferred for a bit longer \nbecause it gets fired second, while in the second example, b gets fired first and from \nthat point on its value is used immediately when needed.\nAt this point, you have a quite good understanding of what deferreds are and how \nthey can be chained and used to represent values that aren't yet available. We finish \nour exploration with a fourth example showing you how to fire something that \ndepends on a number of other deferreds. This in Twisted is implemented using the \ndefer.DeferredList  class:\n>>> # Experiment 4\n>>> deferreds = [defer.Deferred() for i in xrange(5)]\n>>> join = defer.DeferredList(deferreds)\n>>> join.addCallback(on_done)\n>>> for i in xrange(4):\n...     deferreds[i].callback(i)\n>>> deferreds[4].callback(4)\non_done called with arg = [(True, 0), (True, 1), (True, 2),\n                           (True, 3), (True, 4)]\nWhat we notice is that it doesn't matter that four out of five get fired with the for \nstatement, on_done()  doesn't get called until all the deferreds in the list get fired, \nthat is, after our final deferreds[4].callback()  call. The argument for on_done()\nis a list of tuples where each tuple corresponds to a deferred and contains True  for \nsuccess or False  for failure and deferred's value.\nUnderstanding Twisted and nonblocking  \nI/O \u2013 a Python tale\nNow that we have a grasp of the primitives, let me tell you a little Python story. All \ncharacters appearing in this work are fictitious. Any resemblance to real persons is \npurely coincidental:\n# ~*~ Twisted - A Python tale ~*~\nfrom time import sleep\n# Hello, I'm a developer and I mainly setup Wordpress.\ndef install_wordpress(customer):", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2079, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50aeefcf-09e0-48c3-9dd0-848cc2f952d5": {"__data__": {"id_": "50aeefcf-09e0-48c3-9dd0-848cc2f952d5", "embedding": null, "metadata": {"page_label": "128", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "120c6e9f-a10c-4bb1-b4a2-5d432821ef3c", "node_type": "4", "metadata": {"page_label": "128", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "981d79f425a2ea690fecaa60482c053d9f487eb36e05803785d4b2b976f0cb44", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 128 ]    # Our hosting company Threads Ltd. is bad. I start installation \nand...\n    print \"Start installation for\", customer\n    # ...then wait till the installation finishes successfully. It is\n    # boring and I'm spending most of my time waiting while consuming\n    # resources (memory and some CPU cycles). It's because the process\n    # is *blocking*.\n    sleep(3)\n    print \"All done for\", customer\n# I do this all day long for our customers\ndef developer_day(customers):\n    for customer in customers:\n        install_wordpress(customer)\ndeveloper_day([\"Bill\", \"Elon\", \"Steve\", \"Mark\"])\nLet's run it:\n$ ./deferreds.py 1\n------ Running example 1 ------\nStart installation for Bill\nAll done for Bill\nStart installation\n...\n* Elapsed time: 12.03 seconds\nWhat we get is sequential execution. Four customers with three seconds processing \neach means twelve seconds overall. This doesn't scale very well, so we add some \nthreading in our second example:\nimport threading\n# The company grew. We now have many customers and I can't handle  \nthe\n# workload. We are now 5 developers doing exactly the same thing.\ndef developers_day(customers):\n    # But we now have to synchronize... a.k.a. bureaucracy\n    lock = threading.Lock()\n    #\n    def dev_day(id):\n        print \"Goodmorning from developer\", id\n        # Yuck - I hate locks...\n        lock.acquire()\n        while customers:\n            customer = customers.pop(0)\n            lock.release()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b06129b8-7300-46e0-ad67-5c57caf159a7": {"__data__": {"id_": "b06129b8-7300-46e0-ad67-5c57caf159a7", "embedding": null, "metadata": {"page_label": "129", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d53bbfd3-1a0a-4b43-ba15-e59e94eb55f6", "node_type": "4", "metadata": {"page_label": "129", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "8b94359c5a31d945d003ae9151ff82c8fb16a76ce2572a81f3632224430e64ec", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 129 ]            # My Python is less readable\n            install_wordpress(customer)\n            lock.acquire()\n        lock.release()\n        print \"Bye from developer\", id\n    # We go to work in the morning\n    devs = [threading.Thread(target=dev_day, args=(i,)) for i in  \nrange(5)]\n    [dev.start() for dev in devs]\n    # We leave for the evening\n    [dev.join() for dev in devs]\n# We now get more done in the same time but our dev process got more\n# complex. As we grew we spend more time managing queues than doing dev\n# work. We even had occasional deadlocks when processes got extremely\n# complex. The fact is that we are still mostly pressing buttons and\n# waiting but now we also spend some time in meetings.\ndevelopers_day([\"Customer %d\" % i for i in xrange(15)])\nLet's run it as follows:\n$ ./deferreds.py 2\n------ Running example 2 ------\nGoodmorning from developer 0Goodmorning from developer \n1Start installation forGoodmorning from developer 2 \nGoodmorning from developer 3Customer 0\n...\nfrom developerCustomer 13 3Bye from developer 2\n* Elapsed time: 9.02 seconds\nWhat you get is parallel execution using five worker threads. 15 customers with \nthree seconds processing each means 45 seconds overall, but with five workers in \nparallel it ends up taking just nine seconds. The code got a bit ugly though. Instead \nof focusing on the algorithm or business logic, now a good fraction of the code is \nthere just to manage concurrency. Additionally, output became something between \nmessy and unreadable. It's quite hard to get even easy multithreaded code perfectly \nright, which leads us to Twisted:\n# For years we thought this was all there was... We kept hiring more\n# developers, more managers and buying servers. We were trying harder\n# optimising processes and fire-fighting while getting mediocre\n# performance in return. Till luckily one day our hosting\n# company decided to increase their fees and we decided to\n# switch to Twisted Ltd.!\nwww.allitebooks.com", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "116bfcf3-1466-4eeb-863e-e52124d07c4c": {"__data__": {"id_": "116bfcf3-1466-4eeb-863e-e52124d07c4c", "embedding": null, "metadata": {"page_label": "130", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "482ed358-fee5-48d6-b8ae-28a68d81bd88", "node_type": "4", "metadata": {"page_label": "130", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "da3a77dbdb52a5cd05b53517d93e8d9bb13ab92f8505e3a32dbc578a94c85792", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 130 ]from twisted.internet import reactor\nfrom twisted.internet import defer\nfrom twisted.internet import task\n# Twisted has a slightly different approach\ndef schedule_install(customer):\n    # They are calling us back when a Wordpress installation completes.\n    # They connected the caller recognition system with our CRM and\n    # we know exactly what a call is about and what has to be done  \n    # next.\n    #\n    # We now design processes of what has to happen on certain events.\n    def schedule_install_wordpress():\n        def on_done():\n            print \"Callback: Finished installation for\", customer\n        print \"Scheduling: Installation for\", customer\n        return task.deferLater(reactor, 3, on_done)\n    #\n    def all_done(_):\n        print \"All done for\", customer\n    #\n    # For each customer, we schedule these processes on the CRM  \n    # and that\n    # is all our chief-Twisted developer has to do\n    d = schedule_install_wordpress()\n    d.addCallback(all_done)\n    #\n    return d\n# Yes, we don't need many developers anymore or any synchronization.\n# ~~ Super-powered Twisted developer ~~\ndef twisted_developer_day(customers):\n    print \"Goodmorning from Twisted developer\"\n    #\n    # Here's what has to be done today\n    work = [schedule_install(customer) for customer in customers]\n    # Turn off the lights when done\n    join = defer.DeferredList(work)\n    join.addCallback(lambda _: reactor.stop())\n    #\n    print \"Bye from Twisted developer!\"", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6274619-0c2d-4350-9efe-7d1d7a7f14e3": {"__data__": {"id_": "e6274619-0c2d-4350-9efe-7d1d7a7f14e3", "embedding": null, "metadata": {"page_label": "131", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1720d139-5059-4eef-bc86-48cf875a2f3d", "node_type": "4", "metadata": {"page_label": "131", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "212e8b3fc1f6af9132bda7ad6e1ffb809a84109db72605fe7314cfd33374dfdf", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 131 ]# Even his day is particularly short!\ntwisted_developer_day([\"Customer %d\" % i for i in xrange(15)])\n# Reactor, our secretary uses the CRM and follows-up on events!\nreactor.run()\nLet's run it:\n$ ./deferreds.py 3\n------ Running example 3 ------\nGoodmorning from Twisted developer\nScheduling: Installation for Customer 0\n....\nScheduling: Installation for Customer 14\nBye from Twisted developer!\nCallback: Finished installation for Customer 0\nAll done for Customer 0\nCallback: Finished installation for Customer 1\nAll done for Customer 1\n...\nAll done for Customer 14\n* Elapsed time: 3.18 seconds\nWhat we get is perfect working code and nicely looking output while using no \nthreads. We process all 15 customers in parallel, that is, 45 seconds computation in \njust three seconds! The trick is that we replaced all blocking calls to sleep()  with its \nTwisted counterpart task.deferLater()  and callback functions. As processing now \ntakes place somewhere else, we can effortlessly serve 15 customers simultaneously.\nI mentioned that the preceding processing is now being done \nsomewhere else. Is this cheating? The answer is no. Algorithmic \ncomputation still happens in CPUs but CPU operations are \nvery fast nowadays when compared to disk and the network \noperations. As a result bringing data to CPUs and sending \ndata from one CPUs or storage to another take most of the \ntime. We save all this time for our CPUs using nonblocking \nI/O operations. They, exactly like task.deferLater() , use \ncallbacks that get fired when data transfers complete.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2c2892e-c563-44f4-a6c9-dbc56809415d": {"__data__": {"id_": "e2c2892e-c563-44f4-a6c9-dbc56809415d", "embedding": null, "metadata": {"page_label": "132", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79e0157c-ec8d-45ed-b9ae-b05e97cf40d0", "node_type": "4", "metadata": {"page_label": "132", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "01e09a88c5f6196ab230c3e48640a913ea87a69c2dc0306335266de0da54394a", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 132 ]Another very important thing to notice is the Goodmorning from Twisted \ndeveloper  and Bye from Twisted developer!  messages. They are printed \ninstantly when our code starts. If our code reaches that point so early, when does our \napplication really run? The answer is that a Twisted application (including Scrapy) \nruns entirely within reactor.run() ! By the time you call that method, you must have \nevery possible deferred chain your application is expected to use in place (equivalent \nto setting up steps and processes in the CRM system in the preceding story). Your \nreactor.run()  (the secretary) performs the event monitoring and fires callbacks.\nThe main rule of the reactor is; I can do anything as long as \nit's a fast nonblocking operation.\nExcellent! The code doesn't have any threading nonsense but still these callback \nfunctions look a bit ugly. This leads us to the next example:\n# Twisted gave us utilities that make our code way more readable!\n@defer.inlineCallbacks\ndef inline_install(customer):\n    print \"Scheduling: Installation for\", customer\n    yield task.deferLater(reactor, 3, lambda: None)\n    print \"Callback: Finished installation for\", customer\n    print \"All done for\", customer\ndef twisted_developer_day(customers):\n   ... same as previously but using inline_install()\n       instead of schedule_install()\ntwisted_developer_day([\"Customer %d\" % i for i in xrange(15)])\nreactor.run()\nLet's run it as follows:\n$ ./deferreds.py 4\n... exactly the same as before\nThe preceding code does exactly the same as the previous one but looks nicer. The \ninlineCallbacks  generator makes the code of inline_install()  pause and \nresume using a few Python mechanisms. inline_install()  becomes a deferred \nand gets executed in parallel for every customer. Every time we yield , execution \npauses on the current instance of inline_install()  and resumes when the deferred \nthat we yielded gets fired.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbad8c5d-9e3a-4359-a41f-ee27b76f5860": {"__data__": {"id_": "cbad8c5d-9e3a-4359-a41f-ee27b76f5860", "embedding": null, "metadata": {"page_label": "133", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cc0ac07-6850-4445-af05-252842503dba", "node_type": "4", "metadata": {"page_label": "133", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "17680ec62fced9e6d82f8a6a08dd048494e26bf840ae49d50f80093adb9e5a5e", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 133 ]The only problem that we have now is that if instead of 15 customers, we had, for \nexample 10000, this code would shamelessly start 10000 simultaneous sequences of \nprocessing (call it HTTP requests, database write operations, and so on). This may be \nokay or it could cause all sorts of failures. In massively concurrent applications such \nas Scrapy, we often have to limit the amount of concurrency to acceptable levels. \nIn this example, we can do this using a task.Cooperator() . Scrapy uses the same \nmechanism to limit the amount of concurrency in item processing pipelines (the \nCONCURRENT_ITEMS  setting):\n@defer.inlineCallbacks\ndef inline_install(customer):\n   ... same as above\n# The new \"problem\" is that we have to manage all this concurrency to\n# avoid causing problems to others, but this is a nice problem to have.\ndef twisted_developer_day(customers):\n    print \"Goodmorning from Twisted developer\"\n    work = (inline_install(customer) for customer in customers)\n    #\n    # We use the Cooperator mechanism to make the secretary not\n    # service more than 5 customers simultaneously.\n    coop = task.Cooperator()\n    join = defer.DeferredList([coop.coiterate(work) for i in xrange(5)])\n    #\n    join.addCallback(lambda _: reactor.stop())\n    print \"Bye from Twisted developer!\"\ntwisted_developer_day([\"Customer %d\" % i for i in xrange(15)])\nreactor.run()\n# We are now more lean than ever, our customers happy, our hosting\n# bills ridiculously low and our performance stellar.\n# ~*~ THE END ~*~\nLet's run it:\n$ ./deferreds.py 5\n------ Running example 5 ------\nGoodmorning from Twisted developer\nBye from Twisted developer!\nScheduling: Installation for Customer 0\n...\nCallback: Finished installation for Customer 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "791b711f-69de-4a21-b01c-4283251863ba": {"__data__": {"id_": "791b711f-69de-4a21-b01c-4283251863ba", "embedding": null, "metadata": {"page_label": "134", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf7665bc-412f-41de-ae27-21b1f6e753eb", "node_type": "4", "metadata": {"page_label": "134", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "82d10bf2f554a83a49a5f15d25da33a5a99d25985e35d9a210a0d34d2bd2bd36", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 134 ]All done for Customer 4\nScheduling: Installation for Customer 5\n...\nCallback: Finished installation for Customer 14\nAll done for Customer 14\n* Elapsed time: 9.19 seconds\nWhat we observe is that we now have something that is similar to five processing \nslots for customers. Processing for a new customer doesn't start unless there's an \nempty slot, which, effectively, in our case that customer processing time is always \nthe same (three seconds), leads to batches of five customers at a time. We end up \nwith the same performance with our threaded example but now using just one \nthread while enjoying simpler and more correct code.\nCongratulations, you had a\u2014frankly put\u2014quite intense introduction to Twisted and \nnonblocking I/O programming.\nOverview of Scrapy architecture\nThe following diagram summarizes Scrapy's architecture:\nScrapy's architecture", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4c48bf6-02d2-4a2e-bb1b-8c17f995d0ba": {"__data__": {"id_": "c4c48bf6-02d2-4a2e-bb1b-8c17f995d0ba", "embedding": null, "metadata": {"page_label": "135", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88809d3f-2ef8-45ae-b4c1-b65680c677a2", "node_type": "4", "metadata": {"page_label": "135", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "43dde5dfeb6d3fc03c9209284e1851bd27508c15bb7813c1b2b6cb4ed6fcd8c8", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 135 ]You may notice three familiar types of objects upon which this architecture operates; \nRequest s, Response s, and Item s. Our spiders lie right at the core of the architecture. \nThey create Request s, process Response s, and generate Item s and more Request s.\nEach Item  generated by a spider is postprocessed by a sequence of Item Pipelines \nusing their process_item()  method. Typically, process_item()  modifies Item s \nand passes them to the subsequent pipelines by returning them. Occasionally (for \nexample, in the case of a duplicate or invalid data), we may need to drop an Item,  \nand we do so by raising a DropItem  exception. In this case, subsequent pipelines \nwon't receive it. If we also provide an open_spider()  and/or close_spider()  \nmethod, it will get called on spider open and close, respectively. That's an opportunity \nfor initializations and cleanups. Item Pipelines are typically used to perform problem \ndomain or infrastructure operations, such as cleaning up data, or inserting Item s into \ndatabases. You will also find yourself reusing them to great extent between projects, \nespecially if they deal with your infrastructure's specifics. The Appery.io pipeline that \nwe used in Chapter 4 , From Scrapy to a Mobile App , is an example of an Item Pipeline \nthat performs infrastructure work, that is, with minimal configuration it uploads \nItem s to Appery.io.\nWe typically send Request s from our Spiders and get back Response s and it \njust works. Scrapy takes care of cookies, authentication, caching, and so on, in a \ntransparent manner, and all we need to do is occasionally adjust a few settings. Most \nof this functionality is implemented in the form of downloader middlewares. They \nare often quite sophisticated and highly technical dealing with Request /Response  \ninternals. You may create custom ones to make Scrapy work exactly the way you \nwant it to in terms of Request  processing. A typical successful middleware will be \nreused across many projects and likely provide functionality that is useful to many \nScrapy developers, thus, it would be nice to be shared with the community. You \nwon't write a downloader middleware very often. If you want to have a look at the \ndefault downloader middlewares, check the DOWNLOADER_MIDDLEWARES_BASE  setting \nin settings/default_settings.py  in Scrapy's GitHub.\nDownloader is the engine that performs the actual downloads. You will never have \nto modify this unless you are a Scrapy contributor.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9eaf72d8-ccab-4694-9f02-b63a9f53941b": {"__data__": {"id_": "9eaf72d8-ccab-4694-9f02-b63a9f53941b", "embedding": null, "metadata": {"page_label": "136", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db2aad2b-71ab-4233-9288-efe47b532330", "node_type": "4", "metadata": {"page_label": "136", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c3dbe85069b721eccefaad8933f89d1650a9cf723f903882639f60cf31a7ea25", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 136 ]Every now and then you might have to write Spider middlewares. They process \nRequest s just after the spider and before any downloader middleware and Response s \nin the opposite order. With a downloader middleware you may, for example, decide \nto rewrite all your URLs to use HTTPS instead of HTTP regardless of what a spider \nextracts from pages. It implements functionality that is specific to your project's needs \nand shared across all spiders. The main thing that differentiates between downloader \nmiddlewares and spider middlewares is that when a downloader middleware gets a \nRequest , it should return a single Response . On the other hand, it's okay for spider \nmiddleware to drop Request s if they don't like them or, for example, emit many \nRequest s for each input Request  if this serves your application's purpose. You could \nsay that spider middlewares are for Request s and Response s what item pipelines are \nfor Item s. Spider middlewares receive Item s as well but typically don't modify them \nbecause this can be done more easily with an item pipeline. If you want to have a look \nat the default spider middlewares, check the SPIDER_MIDDLEWARES_BASE  setting in \nsettings/default_settings.py  in Scrapy's git.\nFinally, there are extensions. Extensions are quite common\u2014actually the next most \ncommon thing after Item Pipelines. They are plain classes that get loaded at crawl \nstartup and can access settings, the crawler, register callbacks to signals, and define \ntheir own signals. Signals is an essential Scrapy API that allows callbacks to be called \nwhen something happens in the system, for example, an Item  gets crawled, dropped, \nor when a spider opens. There are lots of useful predefined signals, and we will see \nsome of them later. Extensions are a Jack of all trades in the sense that they allow \nyou to write every utility you can possibly imagine but without really giving you \nany help(like, for example, the process_item() method of Item Pipelines). We have \nto hook to signals and implement the functionality we need ourselves. For example, \nstopping the crawl after a specific number of pages or Item s is implemented \nwith an extension. If you want to have a look at the default extensions, check the \nEXTENSIONS_BASE  setting in settings/default_settings.py  in Scrapy's git.\nMiddleware hierarchy", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ad807a7-035a-481b-958d-aa719aa1bd62": {"__data__": {"id_": "2ad807a7-035a-481b-958d-aa719aa1bd62", "embedding": null, "metadata": {"page_label": "137", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "322b07fd-e851-4ecc-a25a-158ef097b19d", "node_type": "4", "metadata": {"page_label": "137", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b459ba45c0af58912a45c251794a72511a5cfa5d78dc703efd2b81305c29feff", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 137 ]A bit more strictly speaking, Scrapy treats all these classes as middlewares (managed \nby decedents of the MiddlewareManager  class) and allows us to initialize them from \na Crawler  or a Settings  object by implementing the from_crawler()  or from_\nsettings()  class-methods, respectively. Since one can get the Settings  easily from \nCrawler  (crawler.settings ), from_crawler()  is way more popular. If one doesn't \nneed Settings  or Crawler , it's fine not to implement them.\nHere is a table that can help you decide what the best mechanism for a given \nproblem is:\nProblem Solution\nSomething that is specific to the website that I'm crawling. Modify your Spider.\nModifying or storing Item s\u2014domain-specific, may be \nreused across projects.Write an Item Pipeline.\nModifying or dropping Requests /Responses \u2014domain-\nspecific, may be reused across projects.Write a spider middleware.\nExecuting Requests /Responses \u2014generic, for example, \nto support some custom login scheme or a special way to \nhandle cookies.Write a downloader \nmiddleware.\nAll other problems. Write an extension.\nExample 1 - a very simple pipeline\nLet's assume that we have an application with several spiders, which provide the \ncrawl date in the usual Python format. Our databases require it in string format in \norder to index it. We don't want to edit our spiders because there are many of them. \nHow can we do it? A very simple pipeline can postprocess our items and perform \nthe conversion we need. Let's see how this works:\nfrom datetime import datetime\nclass TidyUp(object):\n    def process_item(self, item, spider):\n        item['date'] = map(datetime.isoformat, item['date'])\n        return item\nAs you can see, this is nothing more than a simple class with a process_item()  \nmethod. This is all we need for this simple pipeline. We can reuse the spiders from \nChapter 3 , Basic Crawling,  and add the preceding code in a tidyup.py  file inside a \npipelines  directory.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ce02b55-97ba-45ce-be17-2c560490c541": {"__data__": {"id_": "3ce02b55-97ba-45ce-be17-2c560490c541", "embedding": null, "metadata": {"page_label": "138", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff0457e6-8a07-421f-a198-602ffd46e3ca", "node_type": "4", "metadata": {"page_label": "138", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "1955985d197331821cbe1b00bb6ac43e0c325a441ef34c45c72918a4a365f7d4", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 138 ]We can put this item pipeline's code anywhere we want, \nbut a separate directory is a good idea.\nWe now have to edit our project's settings.py  file and set ITEM_PIPELINES  to:\nITEM_PIPELINES = {'properties.pipelines.tidyup.TidyUp': 100 }\nThe number 100 on preceding dict  defines the order in which pipelines are going to \nbe connected. If another pipeline has a smaller number, it will process Item s prior to \nthis pipeline.\nThe full code for this example is in the ch08/properties  \nfolder on GitHub.\nWe are now ready to run our spider:\n$ scrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=90\n...\nINFO: Enabled item pipelines: TidyUp\n...\nDEBUG: Scraped from <200 ...property_000060.html>\n...\n   'date': ['2015-11-08T14:47:04.148968'],\nAs we expected, the date is now formatted as an ISO string.\nSignals\nSignals provide a mechanism to add callbacks to events that happen in the system, \nsuch as when a spider opens, or when an item  gets scraped. You can hook to them \nusing the crawler.signals.connect()  method (an example of using it can be \nfound in the next section). There are just 11 of them and maybe the easiest way \nto understand them is to see them in action. I created a project where I created an \nextension that hooks to every available signal. I also created one Item Pipeline, one \nDownloader and one spider middleware, which also logs every method invocation. \nThe spider it uses is very simple. It just yields two items and then raises an exception:\ndef parse(self, response):\n    for i in range(2):\n        item = HooksasyncItem()\n        item['name'] = \"Hello %d\" % i\n        yield item\n    raise Exception(\"dead\")", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9fd9a61-a918-4f5a-b19a-b808c88eb9b6": {"__data__": {"id_": "d9fd9a61-a918-4f5a-b19a-b808c88eb9b6", "embedding": null, "metadata": {"page_label": "139", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "867646ba-cf8a-4481-91ea-e3d92801ed56", "node_type": "4", "metadata": {"page_label": "139", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "6af5c1d3e814136c70422f089e9713941a2dda6e57baecf6c805dcfd37f64417", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 139 ]On the second item , I've configured the Item Pipeline to raise a DropItem  exception.\nThe full code for this example is in the ch08/hooksasync  \nfolder on GitHub.\nUsing this project, we can get a better understanding of when certain signals get sent. \nTake a look at the comments between the log lines of the following execution (lines \nhave been omitted for brevity):\n$ scrapy crawl test\n... many lines ...\n# First we get those two signals...\nINFO: Extension, signals.spider_opened fired\nINFO: Extension, signals.engine_started fired\n# Then for each URL we get a request_scheduled signal\nINFO: Extension, signals.request_scheduled fired\n...# when download completes we get response_downloaded\nINFO: Extension, signals.response_downloaded fired\nINFO: DownloaderMiddlewareprocess_response called for example.com\n# Work between response_downloaded and response_received\nINFO: Extension, signals.response_received fired\nINFO: SpiderMiddlewareprocess_spider_input called for example.com\n# here our parse() method gets called... and then SpiderMiddleware used\nINFO: SpiderMiddlewareprocess_spider_output called for example.com\n# For every Item that goes through pipelines successfully...\nINFO: Extension, signals.item_scraped fired\n# For every Item that gets dropped using the DropItem exception...\nINFO: Extension, signals.item_dropped fired\n# If your spider throws something else...\nINFO: Extension, signals.spider_error fired\n# ... the above process repeats for each URL\n# ... till we run out of them. then...\nINFO: Extension, signals.spider_idle fired\n# by hooking spider_idle you can schedule further Requests. If you don't\n# the spider closes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e7699e9-e098-4ccd-af7d-fea0a5aceb08": {"__data__": {"id_": "0e7699e9-e098-4ccd-af7d-fea0a5aceb08", "embedding": null, "metadata": {"page_label": "140", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f9c8a35-dcf5-4e3e-8dde-bb4225b64a02", "node_type": "4", "metadata": {"page_label": "140", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7a6b9a27a5f31ccf606b3d525910ad20f0732415e820fe4ec9c585565547383d", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 140 ]INFO: Closing spider (finished)\nINFO: Extension, signals.spider_closed fired\n# ... stats get printed\n# and finally engine gets stopped.\nINFO: Extension, signals.engine_stopped fired\nIt may feel a bit limited to have just 11 signals, but every Scrapy default middleware \nis implemented using just them, so they must be sufficient. Please note that  \nfrom every signal except spider_idle , spider_error , request_scheduled , \nresponse_received , and response_downloaded , you can also return deferreds \ninstead of actual values.\nExample 2 - an extension that measures \nthroughput and latencies\nIt's interesting to measure how throughput (in items per second) and latencies (time \nsince schedule and time after download) change as we add pipelines in Chapter 9 , \nPipeline Recipes .\nThere is already a Scrapy extension that measures throughput, the Log Stats \nextension ( scrapy/extensions/logstats.py  in scrapy's GitHub), and we use it \nas a starting point. In order to measure latencies, we hook the request_scheduled , \nresponse_received , and item_scraped  signals. We timestamp each and subtract \nthe appropriate to calculate latencies that we accumulated to calculate averages. By \nobserving the callback arguments that these signals provide, we notice something \nannoying. item_scraped  gets just Response s, request_scheduled  gets just the \nRequest s, and response_received  gets both. Luckily, we don't have to do any \nhacking to pass-through values. Every Response  has a Request  member, which \npoints back to its Request  and even better it has meta dict  that we saw in Chapter \n5, Quick Spider Recipes , which is the same as the original Request s' no matter if there \nwere any redirects. Excellent, we can store our timestamps there!\nActually, this wasn't my idea. The same mechanism is used \nby the AutoThrottle extension ( scrapy/extensions/\nthrottle.py )\u2014using request.meta.get('download_\nlatency')  where download_latency  is calculated by the \nscrapy/core/downloader/webclient.py  downloader. \nThe fastest way to improve at writing middlewares is by \nfamiliarizing yourself with Scrapy's default middlewares' code.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac93db33-e2f2-4774-b83c-16306a01caa8": {"__data__": {"id_": "ac93db33-e2f2-4774-b83c-16306a01caa8", "embedding": null, "metadata": {"page_label": "141", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3587c476-0adc-4da5-ac3d-a134a161679a", "node_type": "4", "metadata": {"page_label": "141", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "2bf0df6381c0712c21a1dc2655af17143df66d3c37a1d87d05df0d3ed78113f6", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 141 ]Here is the code for our extension:\nclass Latencies(object):\n   @classmethod\n   def from_crawler(cls, crawler):\n     return cls(crawler)\n   def __init__(self, crawler):\n     self.crawler = crawler\n     self.interval = crawler.settings.getfloat('LATENCIES_INTERVAL')\n        if not self.interval:\n           raise NotConfigured\n     cs = crawler.signals\n     cs.connect(self._spider_opened, signal=signals.spider_opened)\n     cs.connect(self._spider_closed, signal=signals.spider_closed)\n     cs.connect(self._request_scheduled, signal=signals.request_\nscheduled)\n     cs.connect(self._response_received, signal=signals.response_\nreceived)\n     cs.connect(self._item_scraped, signal=signals.item_scraped)\n     self.latency, self.proc_latency, self.items = 0, 0, 0\n   def _spider_opened(self, spider):\n     self.task = task.LoopingCall(self._log, spider)\n     self.task.start(self.interval)\n   def _spider_closed(self, spider, reason):\n     if self.task.running:\n         self.task.stop()\n   def _request_scheduled(self, request, spider):\n     request.meta['schedule_time'] = time()\n   def _response_received(self, response, request, spider):\n     request.meta['received_time'] = time()\n   def _item_scraped(self, item, response, spider):\n     self.latency += time() - response.meta['schedule_time']\n     self.proc_latency += time() - response.meta['received_time']\n     self.items += 1\n   def _log(self, spider):\n     irate = float(self.items) / self.interval\n     latency = self.latency / self.items if self.items else 0", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50cbe6a5-d26f-472c-9adc-39be7884befb": {"__data__": {"id_": "50cbe6a5-d26f-472c-9adc-39be7884befb", "embedding": null, "metadata": {"page_label": "142", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d0626c5-a740-49b2-9fa3-81bec6a92524", "node_type": "4", "metadata": {"page_label": "142", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c3f6bb709b06d84295cadec9df0524089c5549da261c6613fd874449251e420b", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 142 ]     proc_latency = self.proc_latency / self.items if self.items else 0\n     spider.logger.info((\"Scraped %d items at %.1f items/s, avg \nlatency: \"\n        \"%.2f s and avg time in pipelines: %.2f s\") %\n        (self.items, irate, latency, proc_latency))\n     self.latency, self.proc_latency, self.items = 0, 0, 0\nThe first two methods are very important because they are very typical. They \ninitialize the middleware using a Crawler  object. You will find such code on almost \nevery nontrivial middleware. from_crawler(cls, crawler)  is the way of grabbing \nthe Crawler  object. Then, we notice in the __init__()  method accessing crawler.\nsettings  and raise a NotConfigured  exception if it isn't set. You will see many \nFooBar  extensions checking the corresponding FOOBAR_ENABLED  setting and raise \nif it isn't set or if it's False . This is a very common pattern allowing middleware \nto be included for convenience in the corresponding settings.py  settings (for \nexample, ITEM_PIPELINES ) but being disabled by default, unless explicitly enabled \nby their corresponding flag settings. Many default Scrapy middleware (for example, \nAutoThrottle or HttpCache) use this pattern. In our case, our extension remains \ndisabled unless LATENCIES_INTERVAL  is set.\nA bit later in __init__() , we find ourselves registering callbacks for all the signals \nwe are interested in using crawler.signals.connect() , and we initialize a few \nmember variables. The rest of the class implements signal handlers. On _spider_\nopened() , we initialize a timer that calls our _log()  method every LATENCIES_\nINTERVAL  seconds, and on _spider_closed() , we stop that timer. In _request_\nscheduled()  and _response_received() , we store timestamps in request.\nmeta , and in _item_scraped() , we accumulate the two latencies (from scheduled/\nreceived until now) and increase the number of Item s scraped. Our _log()  method \ncalculates a few averages, formats and prints a message, and resets the accumulators \nto start another sampling period.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea3b924f-432d-4dd1-aa1d-4504faffe5e5": {"__data__": {"id_": "ea3b924f-432d-4dd1-aa1d-4504faffe5e5", "embedding": null, "metadata": {"page_label": "143", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87da16f3-4133-4ca6-8b5b-86bed7e5d269", "node_type": "4", "metadata": {"page_label": "143", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "97db4beb5789bd8578d6a558a8032ebc49ac03ebde4fa24b668b17b339ecb495", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 143 ]Whoever has written something similar in a multithreaded \ncontext will appreciate the absence of mutexes in the preceding \ncode. They may not be particularly complicated in this case, \nbut still, writing single-threaded code is easier and scales well \nin more complex scenarios.\nWe can add this extension's code in a latencies.py  module at the same level as \nsettings.py . To enable it, we add two lines in our settings.py :\nEXTENSIONS = { 'properties.latencies.Latencies': 500, }\nLATENCIES_INTERVAL = 5\nWe can run it as usual:\n$ pwd\n/root/book/ch08/properties\n$ scrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=1000 -s LOG_LEVEL=INFO\n...\nINFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\nINFO: Scraped 0 items at 0.0 items/sec, average latency: 0.00 sec and \naverage time in pipelines: 0.00 sec\nINFO: Scraped 115 items at 23.0 items/s, avg latency: 0.84 s and avg time \nin pipelines: 0.12 s\nINFO: Scraped 125 items at 25.0 items/s, avg latency: 0.78 s and avg time \nin pipelines: 0.12 s\nThe first log line comes from the Log Stats extension, while subsequent ones come from \nour extension. We can see a throughput of 24 items per second, an average overall \nlatency of 0.78 sec, and that we are spending almost no time processing after download. \nLittle's law gives the number of items in our system as  430.45 19 N S T = \u22c5 = \u22c5 \u2245. No matter \nwhat we set the CONCURRENT_REQUESTS  and CONCURRENT_REQUESTS_PER_DOMAIN  \nsettings to, despite us not hitting 100% CPU, we don't seem to be able to make it  \ngo above 30 for some weird reason. More on this in Chapter 10 , Understanding  \nScrapy's Performance .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7ae68f5-9604-4a49-a76a-a3789a2972d7": {"__data__": {"id_": "b7ae68f5-9604-4a49-a76a-a3789a2972d7", "embedding": null, "metadata": {"page_label": "144", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a077399-770d-4ab8-b0fa-b7f1eee32c1b", "node_type": "4", "metadata": {"page_label": "144", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "50ab3ef92dbd516c8d2d10161b48fa42a1b395f56be040e88d2ec0edd94ceeb2", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 144 ]Extending beyond middlewares\nThis section is here for the curious reader more than the practitioner. You certainly \ndon't need to know these in order to write basic/intermediate Scrapy extensions.\nIf you have a look at scrapy/settings/default_settings.py  you will see quite a few \nclass names among the default settings. Scrapy extensively uses a dependency-injection-\nlike mechanism that allows us to customize and extend many of its internal objects. For \nexample, one may want to support more protocols for URLs beyond file, HTTP, HTTPS, \nS3, and FTP that are defined in the DOWNLOAD_HANDLERS_BASE  setting. To do so, one has \nto just create a Download Handler class and add a mapping in the DOWNLOAD_HANDLERS  \nsetting. The most difficult part is to discover what the interface for your custom classes \nmust be (that is, which methods to implement) because most interfaces aren't explicit. \nYou have to read the source code and see how these classes get used. Your best bet \nis starting with an existing implementation and altering it to your satisfaction. That \nsaid, these interfaces become more and more stable with recent versions of Scrapy, \nand I attempt to document them along with some core Scrapy classes on the following \ndiagram (I omit the middleware hierarchy that was presented earlier).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19cd4ef5-b878-46db-92d4-ccf3ac0ddbca": {"__data__": {"id_": "19cd4ef5-b878-46db-92d4-ccf3ac0ddbca", "embedding": null, "metadata": {"page_label": "145", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d04fe36c-7c1e-4c5e-bbac-4b09e88491ff", "node_type": "4", "metadata": {"page_label": "145", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "03f1d2d84590535964188168a842235a10558a914f107dcea3864b4589fad5ea", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 8[ 145 ]\nScrapy interfaces and core objects", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 51, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "384a1e60-6729-453f-9033-ea2e643bd01a": {"__data__": {"id_": "384a1e60-6729-453f-9033-ea2e643bd01a", "embedding": null, "metadata": {"page_label": "146", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2a834bf-6d74-40b4-816e-d8c54cf34a3e", "node_type": "4", "metadata": {"page_label": "146", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "3c92caa935ee11d3f7719c68e824b8d6f3715fd09800125b6a12cbe68157bef4", "class_name": "RelatedNodeInfo"}}, "text": "Programming Scrapy[ 146 ]The core objects are in the upper-left corner. When someone uses scrapy crawl , \na CrawlerProcess  object is used to create our familiar Crawler  object. The \nCrawler  object is the most important Scrapy class. It holds settings , signals , \nand our spider . It also holds all the extensions in an ExtensionManager  object \nnamed extensions . crawler.engine  leads us to another very important class, \nExecutionEngine . This holds Scheduler , Downloader , and Scraper . URLs get \nscheduled by Scheduler , downloaded by Downloader , and postprocessed by \nScraper . It's no wonder that Downloader  keeps DownloaderMiddleware  and \nDownloadHandler , while Scraper  holds both SpiderMiddleware  and ItemPipeline . \nThe four MiddlewareManager  have their own little hierarchy. Output feeds in Scrapy \nare implemented as an extension; FeedExporter . It uses two independent hierarchies, \none defining output formats and the other the storage types. This allows us, by \nadjusting output URLs, to export to anything from XML files in S3 to Pickle-encoded \noutput on the console. Both hierarchies can also be extended independently using the \nFEED_STORAGES  and FEED_EXPORTERS  settings. Finally contracts that are used by the \nscrapy check  command have their own hierarchy and can be extended using the \nSPIDER_CONTRACTS  setting.\nSummary\nCongratulations, you just completed a quite in-depth introduction to Scrapy and \nTwisted programming. You will likely go through this chapter a few times and  \nuse it as a reference. By far, the most popular extension that one needs is Item \nProcessing Pipelines. We will see how to solve many common problems using  \nthem in the next chapter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00e7d5c2-a11b-4814-961e-23d8796fe9a2": {"__data__": {"id_": "00e7d5c2-a11b-4814-961e-23d8796fe9a2", "embedding": null, "metadata": {"page_label": "147", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09db70e1-3b7d-4701-b128-4698645f97dc", "node_type": "4", "metadata": {"page_label": "147", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "aed01d944d1aa709fdbd5535ebadead687ae6659762250092fc2dfa7e17251ab", "class_name": "RelatedNodeInfo"}}, "text": "[ 147 ]Pipeline Recipes\nIn the previous chapter, we explored the programming techniques that we use to \nwrite Scrapy middlewares. In this chapter, we will focus on writing correct and \nefficient pipelines by showcasing various common use cases, including consuming \nREST APIs, interfacing with databases, performing CPU-intensive tasks, and \ninterfacing with legacy services.\nFor this chapter, we will use several new servers that you can see on the right-hand \nside of the following diagram:\nThe system for this chapter\nVagrant should have already set them up for us, and we should be able to ping them \nfrom dev using their hostname, such as ping es  or ping mysql . Without further \nado, let's start exploring using REST APIs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fd2499f-16f5-4a2b-90cf-fcf54bed62ec": {"__data__": {"id_": "0fd2499f-16f5-4a2b-90cf-fcf54bed62ec", "embedding": null, "metadata": {"page_label": "148", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd90ccbb-7c15-42c9-86be-319284631003", "node_type": "4", "metadata": {"page_label": "148", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "6f0c319eab6601bb92bdcab0500886e892f310d29bde53708aaf57ce43fa6aac", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 148 ]Using REST APIs\nREST is a set of technologies that is used to create modern web services. Its main \nbenefit is that it is simpler and more lightweight than SOAP or proprietary web-\nservice mechanisms. Software designers observed a similarity between the CRUD  \n(Create, Read, Update, Delete ) functionality that web services often provide and \nbasic HTTP operations (GET, POST, PUT, DELETE). They also observed that much \nof the information that is required for a typical web-service call could be compacted \non a resource URL. For example, http://api.mysite.com/customer/john  is a \nresource URL that allows us to identify the target server ( api.mysite.com ), the fact \nthat I'm trying to perform operations related to customers  (table) in that server, and \nmore specifically something that has to do with someone named john  (row\u2014primary \nkey). This, when combined with other web concepts, such as secure authentication, \nbeing stateless, caching, XML or JSON as payload, and so on, provides a powerful \nyet simple, familiar, and effortlessly cross-platform way to provide and consume web \nservices. It's no wonder that REST took the software industry by storm.\nIt's quite common some of the functionality that we want to use in a Scrapy pipeline \nto be provided in the form of a REST API. In the following sections, we will \nunderstand how to access such functionality.\nUsing treq\ntreq  is a Python package that tries to be the equivalent of the Python requests  \npackage for Twisted-based applications. It allows us to perform GET, POST, and \nother HTTP requests easily. To install it, we use pip install treq , but it's already \npreinstalled in our dev.\nWe prefer treq  over Scrapy's Request /crawler.engine.download()  API because \nit is equally simple, but it has performance benefits as we will see in Chapter 10 , \nUnderstanding Scrapy's Performance .\nA pipeline that writes to Elasticsearch\nWe will start with a spider that writes Item s on an ES (Elasticsearch ) server.  \nYou may feel that starting with ES\u2014even before MySQL\u2014as a persistence mechanism \nis a bit unusual, but it's actually the easiest thing one can do. ES can be schema-less, \nwhich means that we can use it without any configuration. treq  is also sufficient for \nour (very simple) use case. If we need more advanced ES functionality, we should \nconsider using txes2  and other Python/Twisted ES packages.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d6b2daa-3afd-4c53-bc88-b8f47961f321": {"__data__": {"id_": "5d6b2daa-3afd-4c53-bc88-b8f47961f321", "embedding": null, "metadata": {"page_label": "149", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23f48073-953d-4899-93ef-0651343e3e8f", "node_type": "4", "metadata": {"page_label": "149", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "127e8b16a1b0d7764e6eaad512bf70d4d1d039ddc405f8987ba5d91cb8c2601c", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 149 ]With our vagrant machine, we already have an ES server running. Let's log in on our \ndev and verify that it's running fine:\n$ curl http://es:9200\n{\n  \"name\" : \"Living Brain\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : { ... },\n  \"tagline\" : \"You Know, for Search\"\n}\nWe should be able to see the same results by visiting http://localhost:9200  in \nour host's browser. If we visit http://localhost:9200/properties/property/_\nsearch , we will see a response indicating that ES globally tried but didn't find any \nindex related to properties. Congratulations, we have just used ES's REST API.\nIn the course of this chapter, we are going to insert properties in the \nproperties collection. You will likely need to reset the properties \ncollection, and you can do this with curl  and a DELETE request:\n$ curl -XDELETE http://es:9200/properties\nThe full code of the pipeline implementations for this chapter have extra details \nsuch as more extensive error handling, but I will keep the code here simple by \nhighlighting the key points.\nYou can download the source code of this book from GitHub: git \nclone https://github.com/scalingexcellence/scrapybook\nThis chapter is in the ch09  directory and this example in particular is in \nch09/properties/properties/pipelines/es.py .\nIn essence, this spider consists of just four lines of code:\n@defer.inlineCallbacks\ndef process_item(self, item, spider):\n    data = json.dumps(dict(item), ensure_ascii=False).encode(\"utf-  \n8\")\n    yield treq.post(self.es_url, data)\nThe first two lines define a standard process_item()  method that is able to yield  \nDeferred s (refer to Chapter 8 , Programming Scrapy ).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f16c85c-87d3-4267-b5d2-a49f008bc64e": {"__data__": {"id_": "7f16c85c-87d3-4267-b5d2-a49f008bc64e", "embedding": null, "metadata": {"page_label": "150", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1dd4c53-d2a6-4606-8bd9-bc5be7f62886", "node_type": "4", "metadata": {"page_label": "150", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "bad0040be157c453e6a7fce53f170692e21dbd716a3995bce02fb0505af46b89", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 150 ]The third line prepares our data  for insertion. We first convert our Items  to dicts . \nWe then encode them in the JSON format using json.dumps() . ensure_ascii=False  \nmakes the output more compact by not escaping non-ASCII characters. We then encode \nthese JSON strings to UTF-8, the default encoding according to the JSON standard.\nThe last line uses the post()  method of treq  to perform a POST request that inserts \nour documents in ElasticSearch. es_url , such as http://es:9200/properties/\nproperty  is stored in our settings.py  file (the ES_PIPELINE_URL  setting), and it \nprovides essential information, such as the IP and port of our ES server ( es:9200 ), the \ncollection name ( properties ), and the object type ( property ) that we want to write to.\nIn order to enable the pipeline, we have to add it on an ITEM_PIPELINES  setting \ninside settings.py  and initialize our ES_PIPELINE_URL  setting:\nITEM_PIPELINES = {\n    'properties.pipelines.tidyup.TidyUp': 100,\n    'properties.pipelines.es.EsWriter': 800,\n}\nES_PIPELINE_URL = 'http://es:9200/properties/property'\nAfter doing so, we can go to the appropriate directory:\n$ pwd\n/root/book/ch09/properties\n$ ls\nproperties  scrapy.cfg\nThen we can run our spider:\n$ scrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=90\n...\nINFO: Enabled item pipelines: EsWriter...\nINFO: Closing spider (closespider_itemcount)...\n   'item_scraped_count': 106,\nIf we now visit http://localhost:9200/properties/property/_search , we will \nbe able to see the number of inserted items in the hits/total  field of the response as \nwell as the first 10 results. We can also add a ?size=100  parameter to get more results. \nBy adding the q= argument in the search URL, we can search for specific keywords \neverywhere or just in certain fields. More relevant results will appear first. For example, \nhttp://localhost:9200/properties/property/_search?q=title:london  gives us \nproperties with \"London\" in their title. For more complex queries, one can consult ES's \ndocumentation at https://www.elastic.co/guide/en/elasticsearch/reference/\ncurrent/query-dsl-query-string-query.html .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8437303b-3315-4973-bc25-743ae12768c6": {"__data__": {"id_": "8437303b-3315-4973-bc25-743ae12768c6", "embedding": null, "metadata": {"page_label": "151", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa3dd2b3-eaed-4004-8670-47e0622c88d4", "node_type": "4", "metadata": {"page_label": "151", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "35fe982843a79cffcd9c7d8e7266fadc625030819fb3abbb870da1a2b9586782", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 151 ]ES needed no configuration because it auto-detects the schema (types of fields) \nfrom the first property that we provide it. By visiting http://localhost:9200/\nproperties/ , one is able to see the mappings that it has auto-detected.\nLet's have a quick look at performance and rerun a scrapy crawl easy -s \nCLOSESPIDER_ITEMCOUNT=1000  as we did at the end of the last chapter. The average \nlatency jumped from 0.78 seconds to 0.81 seconds due to the average time in \npipelines increasing from 0.12 seconds to 0.15 seconds. The throughput remains the \nsame ~25 items per second.\nIs it a great idea to use pipelines to insert Items in our datebases? The \nanswer is no. Usually, databases provide orders of magnitude more \nefficient ways to bulk insert entries, and we should definitely use \nthem instead. This would mean bulking Items and batch inserting \nthem or performing inserts as a post-processing step at the end of a \ncrawl. We will see such approaches in our last chapter. Still, many \npeople use item pipelines to insert to databases and using Twisted \nAPIs instead of generic/blocking ones is the right way to implement \nthis approach.\nA pipeline that geocodes using the Google \nGeocoding API\nWe have area names for our properties, and we would like to geocode them, that is, \nfind their respective coordinates (latitude, longitude). We can use these coordinates \nto put properties on maps or order search results according to their distance from a \nlocation. Building such functionality requires complex databases, sophisticated text \nmatching, and complex spatial computations. Using the Google Geocoding API, we \ncan avoid developing any of these. Let's try this by opening it in a browser or using \ncurl  to retrieve data for the following URL:\n$ curl \"https://maps.googleapis.com/maps/api/geocode/json?sensor=false&ad\ndress=london\"\n{\n   \"results\" : [\n         ...\n         \"formatted_address\" : \"London, UK\",\n         \"geometry\" : {\n            ...\n            \"location\" : {\n               \"lat\" : 51.5073509,\n               \"lng\" : -0.1277583", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ada16fa-0a5b-4fcf-bc85-b700eab5fd8a": {"__data__": {"id_": "5ada16fa-0a5b-4fcf-bc85-b700eab5fd8a", "embedding": null, "metadata": {"page_label": "152", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9bf1ed3f-0350-4171-b261-32fc86adf950", "node_type": "4", "metadata": {"page_label": "152", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "d58582f54c2c9dedee45ff11569170a22f78472657be9784629ffc822bf955ae", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 152 ]            },\n            \"location_type\" : \"APPROXIMATE\",\n            ...\n   ],\n   \"status\" : \"OK\"\n}\nWe can see a JSON object, and if we search for \"location\", we will quickly find the \ncoordinates of what Google considers the center of London. If we keep searching, \nwe will see that there are other locations in the same document. The first one is the \nmost relevant. As a result, results[0].geometry.location , if it exists, has the \ninformation we need.\nThe Google Geocoding API is accessible using the same techniques as before ( treq ). \nWith just a few lines, we can find the location of an address (look at geo.py  in the \npipelines  directory) as follows:\n@defer.inlineCallbacks\ndef geocode(self, address):\n   endpoint = 'http://web:9312/maps/api/geocode/json'\n   parms = [('address', address), ('sensor', 'false')]\n   response = yield treq.get(endpoint, params=parms)\n   content = yield response.json()\n   geo = content['results'][0][\"geometry\"][\"location\"]\n   defer.returnValue({\"lat\": geo[\"lat\"], \"lon\": geo[\"lng\"]})\nThis function forms a URL that is similar to the one we used before, but we now point \nto a fake implementation that makes execution faster, less intrusive, available offline, \nand more predictable. You can use endpoint = 'https://maps.googleapis.com/\nmaps/api/geocode/json'  to hit Google's servers, but please keep in mind that they \nhave strict limits on the requests they allow. The address  and the sensor  values are \nURL-encoded automatically using the params  argument of treq's get()  method. treq.\nget()  returns a deferred, and we yield  it in order to resume when a response is \navailable. A second yield , now on response.json() , is required for us to wait until \nresponse's body is completely loaded and parsed into Python objects. At this point, we \nfind the location information of the first result, format it as a dict , and return it using \ndefer.returnValue()  - the appropriate way to return values from methods that use \ninlineCallbacks . If anything goes wrong, the method throws exceptions that Scrapy \nreports to us.\nBy using geocode() , process_item()  becomes a single line as follows:\nitem[\"location\"] = yield self.geocode(item[\"address\"][0])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd45c0c4-8a82-417a-9488-311ee81969b9": {"__data__": {"id_": "fd45c0c4-8a82-417a-9488-311ee81969b9", "embedding": null, "metadata": {"page_label": "153", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c54a1e3-b4a3-4db2-a522-983b4aee673a", "node_type": "4", "metadata": {"page_label": "153", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "871a61829e792d20472fa48da7c8a04c6e33b5d64a8eae508692294c6e05bd8e", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 153 ]Let's enable this pipeline by adding it to our settings' ITEM_PIPELINES  with a \npriority number that is smaller than ES's so that ES gets our location values:\nITEM_PIPELINES = {\n    ...\n    'properties.pipelines.geo.GeoPipeline': 400,\nLet's run a quick crawl with debug data enabled:\n$ scrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=90 -L DEBUG\n...\n{'address': [u'Greenwich, London'],\n...\n 'image_urls': [u'http://web:9312/images/i06.jpg'],\n 'location': {'lat': 51.482577, 'lon': -0.007659},\n 'price': [1030.0],\n...\nWe can now see the location  field set for our Items. This is great! If we temporarily \nrun it using the real Google API's URL though, we will soon get exceptions like this:\nFile \"pipelines/geo.py\" in geocode (content['status'], address))\nException: Unexpected status=\"OVER_QUERY_LIMIT\" for  \naddress=\"*London\"\nThis is a check that we've put in place in the full code to ensure that the status  field \nof the Geocoding API's response has the OK value. Unless that's true, the data that \nwe get back won't have the format we expect and can't be safely used. In this case, \nwe get the OVER_QUERY_LIMIT  status, which clearly indicates that we did something \nwrong. This is an important problem that we will likely face in many cases. With \nScrapy's high performance engine, being able to cache and throttle requests to \nresources becomes a necessity.\nWe can visit the Geocoder API's documentation to read about its limits; \" Users of the \nfree API: 2500 requests per 24 hour period, 5 requests per second \". Even if we use the paid \nversion of the Google Geocoding API, it's also throttled at 10 requests per second, \nwhich means that this discussion is still relevant.\nThe implementations that follow may look complex, but they \nhave to be judged in context. Creating such components in \na typical multithreaded environment would require thread \npools and synchronization that leads to quite complex code.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68e96cb7-0fee-48c7-a55d-f994f175fa50": {"__data__": {"id_": "68e96cb7-0fee-48c7-a55d-f994f175fa50", "embedding": null, "metadata": {"page_label": "154", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ef3dec6-3329-47a8-8a9f-d19f50b2eac4", "node_type": "4", "metadata": {"page_label": "154", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "1b2d91907a43b4fe87bacc769e1391b3034b5acb888249b33645cfed1a6226f0", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 154 ]Here is a simple and good enough implementation of a throttling engine using \nTwisted's techniques:\nclass Throttler(object):\n    def __init__(self, rate):\n        self.queue = []\n        self.looping_call = task.LoopingCall(self._allow_one)\n        self.looping_call.start(1. / float(rate))\n    def stop(self):\n        self.looping_call.stop()\n    def throttle(self):\n        d = defer.Deferred()\n        self.queue.append(d)\n        return d\n    def _allow_one(self):\n        if self.queue:\n            self.queue.pop(0).callback(None)\nThis allows us to enqueue Deferreds in a list and fire them one by one each time that \n_allow_one()  gets called; _allow_one()  checks whether the queue is empty and if \nit's not, it calls the callback()  of the oldest deferred (FIFO). We call _allow_one()  \nperiodically using Twisted's task.LoopingCall()  API. It's easy to use Throttler . \nWe initialize it in our pipeline's __init__  and clean it up when our spider stops:\nclass GeoPipeline(object):\n    def __init__(self, stats):\n        self.throttler = Throttler(5)  # 5 Requests per second\n    def close_spider(self, spider):\n        self.throttler.stop()\nJust before we use the resource that we want to throttle (in our case calling \ngeocode()  in process_item() ), we yield  throttler's throttle()  method:\nyield self.throttler.throttle()\nitem[\"location\"] = yield self.geocode(item[\"address\"][0])\nOn the first yield , the code will pause and will resume after sufficient time elapses. \nFor example, if at some point there are 11 deferreds queued, and we have a rate limit \nof five requests per second, our code will resume after the queue empties in about \n11/5 = 2.2  seconds.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a329bb8-9774-47b7-a955-61ac9d2c941f": {"__data__": {"id_": "4a329bb8-9774-47b7-a955-61ac9d2c941f", "embedding": null, "metadata": {"page_label": "155", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d35f839f-bda0-44e3-b13a-16d7442529b8", "node_type": "4", "metadata": {"page_label": "155", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "419ab9f98996c5c66de08e0fd2a9fdf8a92f8ac601dee9f0fae008ba59063a9a", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 155 ]Using Throttler , we no longer get errors but our spider is dead slow. We observe \nthat our demo properties have just a few distinct locations. This is a great opportunity \nfor caching. We could use a simple Python dict  to do this, but we would get race \nconditions, which cause spurious API calls. Here is a cache that doesn't have this \nproblem and demonstrates some interesting features of Python and Twisted:\nclass DeferredCache(object):\n    def __init__(self, key_not_found_callback):\n        self.records = {}\n        self.deferreds_waiting = {}\n        self.key_not_found_callback = key_not_found_callback\n    @defer.inlineCallbacks\n    def find(self, key):\n        rv = defer.Deferred()\n        if key in self.deferreds_waiting:\n            self.deferreds_waiting[key].append(rv)\n        else:\n            self.deferreds_waiting[key] = [rv]\n            if not key in self.records:\n                try:\n                    value = yield self.key_not_found_callback(key)\n                    self.records[key] = lambda d: d.callback(value)\n                except Exception as e:\n                    self.records[key] = lambda d: d.errback(e)\n            action = self.records[key]\n            for d in self.deferreds_waiting.pop(key):\n                reactor.callFromThread(action, d)\n        value = yield rv\n        defer.returnValue(value)\nThis cache looks a bit different to what one would typically expect. It consists of two \ncomponents:\n\u2022 self.deferreds_waiting : This is a queue of deferreds that wait for a value \nfor a given key\n\u2022 self.records : This is a dict  with already seen key-action pairs", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e95c6fba-5404-4a4b-ba3c-816248317c40": {"__data__": {"id_": "e95c6fba-5404-4a4b-ba3c-816248317c40", "embedding": null, "metadata": {"page_label": "156", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a2a826c-0d5d-4894-aca3-36f6217171e2", "node_type": "4", "metadata": {"page_label": "156", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "24743ca6494ffda85963d4fdaad4a8ec05e9cdebd190fcb2758de1258632b896", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 156 ]If we look at the middle of the find()  implementation, we observe that if we \ndon't find a key in self.records , we call a predefined callback  function to \nretrieve the missing value ( yield self.key_not_found_callback(key) ). This \ncallback function may throw an exception. How do we store values or exceptions \nin a compact way in Python? Since Python is a functional language, we store little \nfunctions ( lambdas ) that call either deferred's callback  or errback  in self.\nrecords  depending on whether there was an exception or not. The value or the \nexception gets attached to the lambda  function while defining it. This attachment \nof variables to functions is called closure and is one of the most distinctive and \npowerful features of most functional programming languages.\nIt's a bit unusual to cache exceptions, but this means that if you look \nup a key for first time and key_not_found_callback(key)  \nthrows an exception, the same exception will be rethrown in any \nsubsequent lookup for the same key without performing extra calls.\nThe rest of the find()  implementation provides us with a mechanism that helps us \navoid race conditions. If the lookup for a key is already in process, there will be a \nrecord in the self.deferreds_waiting dict . In this case, we don't make another \ncall to key_not_found_callback() , but we just add ourselves to the list of deferreds \nwaiting for that key. When key_not_found_callback()  returns and the value for \nthis key becomes available, we fire every deferred that is waiting for this key. We \ncould directly perform action(d)  instead of using reactor.callFromThread() , but \nthen we would have to handle any exceptions that are thrown downstream, and we \nwould create unnecessary long deferred chains.\nIt's very easy to use this cache. We initialize it in __init__()  and set the callback \nfunction as one that performs the API call. In process_item() , we look up using the \ncache as follows:\ndef __init__(self, stats):\n    self.cache = DeferredCache(self.cache_key_not_found_callback)\n@defer.inlineCallbacks\ndef cache_key_not_found_callback(self, address):\n    yield self.throttler.enqueue()\n    value = yield self.geocode(address)\n    defer.returnValue(value)\n@defer.inlineCallbacks\ndef process_item(self, item, spider):\n    item[\"location\"] = yield self.cache.find(item[\"address\"][0])\n    defer.returnValue(item)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2400, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "728151aa-b55b-4999-bef5-fb14879177ed": {"__data__": {"id_": "728151aa-b55b-4999-bef5-fb14879177ed", "embedding": null, "metadata": {"page_label": "157", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d374a79-2716-4c7f-a9eb-4cc498671e55", "node_type": "4", "metadata": {"page_label": "157", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "1031d8e7d5cb2d32037dc3dd353a1020e521e88ed7d4976958f51187d954a0cc", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 157 ]The code in Git contains some more error handling code, retries calls in case of failure due \nto throttling (a simple while  loop), and also contains code that updates spider's statistics.\nThe full code for this example is in ch09/properties/\nproperties/pipelines/geo2.py .\nIn order to enable this pipeline, we disable (comment out) our previous \nimplementation and add this to ITEM_PIPELINES  in settings.py  as follows:\nITEM_PIPELINES = {\n    'properties.pipelines.tidyup.TidyUp': 100,\n    'properties.pipelines.es.EsWriter': 800,\n    # DISABLE 'properties.pipelines.geo.GeoPipeline': 400,\n    'properties.pipelines.geo2.GeoPipeline': 400,\n}\nWe can then run the spider with the following code:\n$ scrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=1000\n...\nScraped... 15.8 items/s, avg latency: 1.74 s and avg time in pipelines: \n0.94 s\nScraped... 32.2 items/s, avg latency: 1.76 s and avg time in pipelines: \n0.97 s\nScraped... 25.6 items/s, avg latency: 0.76 s and avg time in pipelines: \n0.14 s\n...\n: Dumping Scrapy stats:...\n   'geo_pipeline/misses': 35,\n   'item_scraped_count': 1019,\nWe will observe that the latency of crawling starts high while populating the cache, \nbut then, it reverts to its previous values. Statistics also indicate 35 misses, which is \nthe exact number of different locations that are used in the demo dataset. Obviously, \nthere were 1019 - 35= 984  hits in the case above. If we use the real Google API and \nincrease the allowed number of API requests per second slightly, for example from \n5 to 10 by changing Throttler(5)  to Throttler(10) , we will get retries recorded \nin the geo_pipeline/retries  stat. If there are any errors, for example, if a location \ncan't be found using the API, an exception will be thrown, and this is captured in \nthe geo_pipeline/errors  stat. If the location somehow (we will see how in later \nsections) is already set, it will be indicated in the geo_pipeline/already_set  stat. \nFinally, if we check ES for properties by navigating to http://localhost:9200/\nproperties/property/_search , we will see entries with location values, such as \n{...\"location\": {\"lat\": 51.5269736, \"lon\": -0.0667204}...} , as expected \n(make sure you don't see old values by clearing the collection before your run).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5892d2a-167a-49c7-914f-956c062c0627": {"__data__": {"id_": "f5892d2a-167a-49c7-914f-956c062c0627", "embedding": null, "metadata": {"page_label": "158", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72a7ede3-dc86-4a0d-b5be-09f55f33385c", "node_type": "4", "metadata": {"page_label": "158", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "9364212f23db948fdb0b798971380bd7ad3461fdd64153798799330b6fca7b11", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 158 ]Enabling geoindexing on Elasticsearch\nNow that we have locations, we can, for example, sort the results by distance. Here is \nan HTTP POST request (done using curl ) that returns properties that have \"Angel\" \nin their title and are sorted by their distance from the point {51.54, -0.19} :\n$ curl http://es:9200/properties/property/_search -d '{\n    \"query\" : {\"term\" : { \"title\" : \"angel\" } },\n    \"sort\": [{\"_geo_distance\": {\n        \"location\":      {\"lat\":  51.54, \"lon\": -0.19},\n        \"order\":         \"asc\",\n        \"unit\":          \"km\", \n        \"distance_type\": \"plane\" \n}}]}'\nThe only problem is that if we try to run it, we will see it failing with a  \"failed to \nfind mapper for [location] for geo distance based sort\"  error message. \nThis indicates that our location field doesn't have the proper format for spatial \noperations. In order to set the proper type, we will have to manually override the \ndefaults. First, we save the autodetected mapping in a file as a starting point:\n$ curl 'http://es:9200/properties/_mapping/property' > property.txt\nThen we edit property.txt  as follows:\n\"location\":{\"properties\":{\"lat\":{\"type\":\"double\"},\"lon\":{\"type\":\"d  \nouble\"}}}\nWe replace this line of code with the following one:\n\"location\": {\"type\": \"geo_point\"}\nWe also delete {\"properties\":{\"mappings\":  and two }} at the end of the file. We \nare then done with the file. We can now delete the old type and create a new one \nwith our explicit schema as follows:\n$ curl -XDELETE 'http://es:9200/properties'\n$ curl -XPUT 'http://es:9200/properties'\n$ curl -XPUT 'http://es:9200/properties/_mapping/property' --data  \n@property.txt\nWe can now rerun a quick crawl, and we will be able to run the curl  command that \nwe saw earlier in this section and get results sorted by distance. Our search returns \nJSONs with properties with an extra sort  field with its distance from the search \npoint in km.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8efcdd85-eb0c-416e-a49f-b8d9ab608a05": {"__data__": {"id_": "8efcdd85-eb0c-416e-a49f-b8d9ab608a05", "embedding": null, "metadata": {"page_label": "159", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "560c1cab-acdd-48b5-acb2-adb67a47578f", "node_type": "4", "metadata": {"page_label": "159", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "79971ec0e75e78086fda6e139e6fcaa3b36c574b51f9fc14de1a0324dc9d8c07", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 159 ]Interfacing databases with standard \nPython clients\nThere are many important databases, including MySQL, PostgreSQL, Oracle, Microsoft \nSQL Server, and SQLite, that adhere to the Python Database API Specification 2.0. \nTheir drivers are often complex and very well tested, and it would be a big waste \nif they had to be reimplemented for Twisted. One can use these database clients in \nTwisted applications, such as Scrapy using the twisted.enterprise.adbapi  library. \nWe will use MySQL as an example to demonstrate its usage, but the same principles \napply to any other compliant database.\nA pipeline that writes to MySQL\nMySQL is a great and very popular database. We will write a pipeline that writes \nitems to it. We already have a MySQL instance running on our virtual environment. \nWe will need to perform some basic administration using the MySQL command-line \ntool, which is also preinstalled on our dev machine, as follows:\n$ mysql -h mysql -uroot -ppass\nWe will get a MySQL prompt indicated by mysql> , and we can now create a simple \ndatabase table with a few fields, as follows:\nmysql> create database properties;\nmysql> use properties\nmysql> CREATE TABLE properties (\n  url varchar(100) NOT NULL,\n  title varchar(30),\n  price DOUBLE,\n  description varchar(30),\n  PRIMARY KEY (url)\n);\nmysql> SELECT * FROM properties LIMIT 10;\nEmpty set (0.00 sec)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "decd53f1-5f7a-4afe-b645-707889a1216c": {"__data__": {"id_": "decd53f1-5f7a-4afe-b645-707889a1216c", "embedding": null, "metadata": {"page_label": "160", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "515f0944-1f53-4423-8c6d-a87eab0f7f95", "node_type": "4", "metadata": {"page_label": "160", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "5137eaf114cbf75230d6b4c0c9c8a79582a638b06861c37c33b4f24231ffd4ae", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 160 ]Great, now that we have a MySQL database and a table named properties  with a \nfew fields, we are ready to create our pipeline. Keep the MySQL console open as we \nwill get back to it in a bit to check whether the values were inserted. In case we need \nto exit it, we just type exit .\nIn the course of this section, we are going to insert properties \nin the MySQL database. If you need to erase them, use the \nfollowing command:\nmysql> DELETE FROM properties;\nWe will use the MySQL client for Python. We will also install a little utility module \nthat is named dj-database-url  to help us parse connection URLs (it just saves us \nfrom having distinct settings for IP, port, password, and so on.) We can install these \ntwo using pip install dj-database-url MySQL-python , but we have them already \ninstalled in our dev environment. Our MySQL pipeline is very simple, as follows:\nfrom twisted.enterprise import adbapi\n...\nclass MysqlWriter(object):\n    ...\n    def __init__(self, mysql_url):\n        conn_kwargs = MysqlWriter.parse_mysql_url(mysql_url)\n        self.dbpool = adbapi.ConnectionPool('MySQLdb',\n                                            charset='utf8',\n                                            use_unicode=True,\n                                            connect_timeout=5,\n                                            **conn_kwargs)\n    def close_spider(self, spider):\n        self.dbpool.close()\n    @defer.inlineCallbacks\n    def process_item(self, item, spider):\n        try:\n            yield self.dbpool.runInteraction(self.do_replace, item)\n        except:\n            print traceback.format_exc()\n        defer.returnValue(item)\n    @staticmethod\n    def do_replace(tx, item):\n        sql = \"\"\"REPLACE INTO properties (url, title, price,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3a11132-ca32-40b9-8213-790109da8f69": {"__data__": {"id_": "e3a11132-ca32-40b9-8213-790109da8f69", "embedding": null, "metadata": {"page_label": "161", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73adafef-25da-451f-b493-566bd11d5c40", "node_type": "4", "metadata": {"page_label": "161", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "9bf311ccf3edf21bc16fd51a96e6efe9366c7d9e3f753eab36545bdacc929e6f", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 161 ]        description) VALUES (%s,%s,%s,%s)\"\"\"\n        args = (\n            item[\"url\"][0][:100],\n            item[\"title\"][0][:30],\n            item[\"price\"][0],\n            item[\"description\"][0].replace(\"\\r\\n\", \" \")[:30]\n        )\n        tx.execute(sql, args)\nThe full code for this example is in ch09/properties/\nproperties/pipelines/mysql.py .\nEssentially, most of it is boilerplate spider code. The code that we have omitted \nfor brevity parses a URL in the format mysql://user:pass@ip/database  that \nis contained in the MYSQL_PIPELINE_URL  setting to individual arguments. In our \nspider's __init__() , we pass them to adbapi.ConnectionPool() , which uses the \ninfrastructure of adbapi  to initialize a MySQL connection pool. The first argument is \nthe name of the module that we want to import. In our MySQL case, this is MySQLdb . \nWe set a few extra arguments for the MySQL client to properly handle Unicode and \ntimeouts. All these arguments go to the underlying MySQLdb.connect()  function \nevery time adbapi  needs to open new connections. On spider close, we call the \nclose()  method for that pool.\nOur process_item()  method essentially wraps dbpool.runInteraction() . This \nmethod queues a callback method that will be called at some later point when a \nTransaction  object from one of the connections in the connection pool becomes \navailable. The Transaction  object has an API that is similar to a DB-API cursor. In \nour case, the callback method is do_replace() , which is defined a few lines later. \n@staticmethod  means that the method refers to the class and not a specific class \ninstance, thus, we can omit the usual self  argument. It's good practice to make \nmethods static if they don't use any members, but even if you forget it, it's okay. This \nmethod prepares a SQL string, a few arguments, and calls the execute()  method of \nTransaction  to perform the insertion. Our SQL uses REPLACE INTO  instead of the \nmore common INSERT INTO  to replace existing entries with the same primary key if \nthey already exist. This is convenient in our case. If we wanted to use SQL that returns \ndata, such as the SELECT  statements, we would use dbpool.runQuery() , and we may \nwant to change the default cursor that is used by setting the cursorclass  argument \nof adbapi.ConnectionPool()  to, for example, cursorclass=MySQLdb.cursors.\nDictCursor  as it's more convenient for data retrieval.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6de09143-cde8-4e52-b50f-029d6a6a5eac": {"__data__": {"id_": "6de09143-cde8-4e52-b50f-029d6a6a5eac", "embedding": null, "metadata": {"page_label": "162", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4d672b8-ec88-44f3-bc90-9d86c5d48109", "node_type": "4", "metadata": {"page_label": "162", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "143c08b7289764a0cedb3c1f9ffd0a5c1dfc89b296a1006791594b943b967397", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 162 ]In order to use this pipeline, we have to add it in our ITEM_PIPELINES  dict  in \nsettings.py , as well as set the MYSQL_PIPELINE_URL  appropriately:\nITEM_PIPELINES = { ...\n    'properties.pipelines.mysql.MysqlWriter': 700,\n...\nMYSQL_PIPELINE_URL = 'mysql://root:pass@mysql/properties'\nExecute the following command:\nscrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=1000\nAfter running this command, we can go back to the MySQL prompt and see the \nrecords on the database as follows:\nmysql> SELECT COUNT(*) FROM properties;\n+----------+\n|     1006 |\n+----------+\nmysql> SELECT * FROM properties LIMIT 4;\n+------------------+--------------------------+--------+-----------+\n| url              | title                    | price  | description\n+------------------+--------------------------+--------+-----------+\n| http://...0.html | Set Unique Family Well   | 334.39 | website c\n| http://...1.html | Belsize Marylebone Shopp | 388.03 | features                       \n| http://...2.html | Bathroom Fully Jubilee S | 365.85 | vibrant own\n| http://...3.html | Residential Brentford Ot | 238.71 | go court\n+------------------+--------------------------+--------+-----------+\n4 rows in set (0.00 sec)\nThe performance, both latency and throughput, remains exactly the same as before. \nThis is quite impressive.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76348446-a0bb-448f-8a1c-fcccb1a16999": {"__data__": {"id_": "76348446-a0bb-448f-8a1c-fcccb1a16999", "embedding": null, "metadata": {"page_label": "163", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13970d91-016a-481f-b307-33161b97b8eb", "node_type": "4", "metadata": {"page_label": "163", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "fd3db9141e5cebfdc27621d57262a458f07b8f8ec63ab322c65e233d57b79a43", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 163 ]Interfacing services using  \nTwisted-specific clients\nUntil now, we saw how to use REST-like APIs using treq . Scrapy can interface \nwith many other services using Twisted-specific clients. For example, if we want to \ninterface MongoDB, and we search for \"MongoDB Python\", we will get PyMongo , \nwhich is blocking/synchronous and shouldn't be used with Twisted unless we use \nthreads as described in the pipeline that handle blocking operations in a later section. \nIf we search for \"MongoDB Twisted Python\", we get txmongo , which is perfectly fine \nto use with Twisted and Scrapy. Usually, the communities behind Twisted clients are \nsmaller, but this is still a better option than writing our own client. We will use such a \nTwisted-specific client to interface with the Redis key-value store.\nA pipeline that reads/writes to Redis\nThe Google Geocoding API limit is per-IP. One may have access to multiple IPs \n(for example, many servers) and would like to avoid making duplicate requests \nfor addresses that another machine has already geocoded. This also applies for the \naddresses that one has seen recently in previous runs. We wouldn't like to waste our \nprecious quotas.\nTalk to the API vendor to ensure that this is okay with their \npolicies. You may have to, for example, discard cached records \nevery few minutes/hours or caching may not be allowed at all.\nWe can use Redis key-value cache as, essentially, a distributed dict . We already run \na Redis instance in our vagrant environment, and we should be able to connect to it \nand perform basic operations using redis-cli  from dev:\n$ redis-cli -h redis\nredis:6379> info keyspace\n# Keyspace\nredis:6379> set key value\nOK\nredis:6379> info keyspace\n# Keyspace\ndb0:keys=1,expires=0,avg_ttl=0\nredis:6379> FLUSHALL", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1788, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94c532ce-919b-4b9b-958c-361d74d37de7": {"__data__": {"id_": "94c532ce-919b-4b9b-958c-361d74d37de7", "embedding": null, "metadata": {"page_label": "164", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aac50e13-4da0-4918-a806-b3ba375fdb04", "node_type": "4", "metadata": {"page_label": "164", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "824bcbd3969413cbe4135bb6d0b99ceda14e2492dcba117b5ddbf34a351493de", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 164 ]OK\nredis:6379> info keyspace\n# Keyspace\nredis:6379> exit\nBy Googling \"Redis Twisted\", we find the txredisapi  library. What makes it \nfundamentally different is that it isn't just a wrapper around synchronous Python \nlibraries, but this is a proper Twisted library that connects to Redis using reactor.\nconnectTCP() , implements Twisted protocols, and so on. We use it in a similar \nmanner to other libraries, but it is bound to be slightly more efficient when used \nin a Twisted application. We can install it along with a utility library, dj_redis_\nurl, which parses Redis configuration URLs, by using pip (sudo pip install \ntxredisapi dj_redis_url ), and as usual, it's preinstalled in our dev.\nWe initialize our RedisCache  pipeline as follows:\nfrom txredisapi import lazyConnectionPool\nclass RedisCache(object):\n...\n    def __init__(self, crawler, redis_url, redis_nm):\n        self.redis_url = redis_url\n        self.redis_nm = redis_nm\n        args = RedisCache.parse_redis_url(redis_url)\n        self.connection = lazyConnectionPool(connectTimeout=5,\n                                             replyTimeout=5,\n                                             **args)\n        crawler.signals.connect(\n                self.item_scraped,signal=signals.item_scraped)\nThis pipeline is quite simple. In order to connect with a Redis server, we need the \nhost, port, and so on, which we all store in a URL format. We parse the format using \nour parse_redis_url()  method (omitted for brevity). It's also very common to use \na namespace that prefixes our keys, which, in our case, we store in redis_nm . We \nthen use lazyConnectionPool()  of txredisapi  to open a connection to the server.\nThe last line has an interesting function. What we're aiming to do is to wrap the geo-\npipeline with this pipeline. If we don't have a value in Redis, we won't set a value, \nand our geo-pipeline will use the API to geocode the address as before. After it does \nso, we have to have a way to cache these key-value pairs in Redis, and we do this \nby connecting to the signals.item_scraped  signal. The callback we define (our \nitem_scraped()  method, which we will see in a bit) will be called at the very end, at \nwhich point the location will be set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae5712b0-124d-42cc-a99c-14c7d65c700a": {"__data__": {"id_": "ae5712b0-124d-42cc-a99c-14c7d65c700a", "embedding": null, "metadata": {"page_label": "165", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7789da2-8273-4181-b40c-9154cd41895c", "node_type": "4", "metadata": {"page_label": "165", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ca4eb178301ee7975b6db67d12e2d87ad4f4101fb1811146419e69b9655fe3f1", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 165 ]The full code for this example is in ch09/properties/\nproperties/pipelines/redis.py .\nWe keep this cache simple by looking up and recording addresses and locations for \nevery Item . This makes sense for Redis because it very often runs on the same server, \nwhich makes it very fast. If that's not the case one may want to add a dict -based \ncache that is similar to the one that we have in our geo-pipeline. This is how we \nprocess incoming Items:\n@defer.inlineCallbacks\ndef process_item(self, item, spider):\n    address = item[\"address\"][0]\n    key = self.redis_nm + \":\" + address\n    value = yield self.connection.get(key)\n    if value:\n        item[\"location\"] = json.loads(value)\n    defer.returnValue(item)\nThis is nothing more than one would expect. We get the address, prefix it, and look it \nup in Redis using get()  of txredisapi  connection . We store JSON-encoded objects \nas values in Redis. If a value is set, we use JSON to decode it and set it as a location.\nWhen an Item  reaches the end of all our pipelines, we recapture it in order to store to \nRedis location values. Here is how we do this:\n    from txredisapi import ConnectionError\n    def item_scraped(self, item, spider):\n        try:\n            location = item[\"location\"]\n            value = json.dumps(location, ensure_ascii=False)\n        except KeyError:\n            return\n        address = item[\"address\"][0]\n        key = self.redis_nm + \":\" + address\n        quiet = lambda failure: failure.trap(ConnectionError)\n        return self.connection.set(key, value).addErrback(quiet)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40979d75-6731-4286-9dc2-567a27b3fb29": {"__data__": {"id_": "40979d75-6731-4286-9dc2-567a27b3fb29", "embedding": null, "metadata": {"page_label": "166", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "651c4294-609b-410a-8e65-22a4caad1033", "node_type": "4", "metadata": {"page_label": "166", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "36eba286cab2c82df9ae2fcaec55b3691c65a19e18b692b30a5ebe07056a3316", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 166 ]There are no big surprises here either. If we find a location, we get the address, prefix \nit and use them as keys and values for the txredisapi  connection's set()  methods. \nYou will notice that this function doesn't use @defer.inlineCallbacks  because it \nisn't supported while handling signals.item_scraped . This means that we can't \nuse our very convenient yield  for connection.set() , but what we can do is return \na deferred that Scrapy will use to chain any further signal listeners. In any case, if a \nconnection to Redis can't be made to connection.set() , it will throw an exception. \nWe can ignore this exception quietly by adding a custom error handler to the \ndeferred that connection.set()  returns. In this error handler, we take the failures \nthat are passed as arguments, and we tell them to trap()  any ConnectionError . \nThis is a nice feature of Twisted's Deferred  API. By using trap()  on the expected \nexceptions, we can quietly ignore them in a compact form.\nTo enable this pipeline, all we have to do is add it to our ITEM_PIPELINES  settings \nand provide a REDIS_PIPELINE_URL  inside settings.py . It is important to give  \nthis a priority value that sets it before the geo-pipeline otherwise it will be too late to \nbe useful:\nITEM_PIPELINES = { ...\n    'properties.pipelines.redis.RedisCache': 300,\n    'properties.pipelines.geo.GeoPipeline': 400,\n...\nREDIS_PIPELINE_URL = 'redis://redis:6379'\nWe can run this spider as usual. The first run will be similar to before, but any \nsubsequent run will be as follows:\n$ scrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=100\n...\nINFO: Enabled item pipelines: TidyUp, RedisCache, GeoPipeline, \nMysqlWriter, EsWriter\n...\nScraped... 0.0 items/s, avg latency: 0.00 s, time in pipelines: 0.00 s\nScraped... 21.2 items/s, avg latency: 0.78 s, time in pipelines: 0.15 s\nScraped... 24.2 items/s, avg latency: 0.82 s, time in pipelines: 0.16 s\n...\nINFO: Dumping Scrapy stats: {...\n   'geo_pipeline/already_set': 106,\n   'item_scraped_count': 106,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b89a4454-7b2d-4728-8e37-770b0d0a1b46": {"__data__": {"id_": "b89a4454-7b2d-4728-8e37-770b0d0a1b46", "embedding": null, "metadata": {"page_label": "167", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd23dcdb-4230-496a-a80f-e38106f2907a", "node_type": "4", "metadata": {"page_label": "167", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e82d5c4dc033447fed77e6d1c8768769fa3ccd465b20afde35022cf6238b04cd", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 167 ]We can see that both the GeoPipeline  and the RedisCache  are enabled and that \nRedisCache comes first. Also notice in the stats geo_pipeline/already_set: \n106. These are items that GeoPipeline finds prepopulated from our Redis cache, \nand in all these cases, it won't make a Google API call. If the Redis cache is empty, \nyou will see a few keys being handled using the Google API as expected. In terms \nof performance, what we observe is that the start-behavior that was induced by \nGeoPipeline is now gone. Indeed, as we now use the cache, we bypass the five \nrequests per second API limit. If we use Redis, we should consider using expiring \nkeys to make our system refresh its cached data periodically.\nInterfacing CPU-intensive, blocking, or \nlegacy functionality\nThis final section talks about accessing the most non-Twisted-like workloads. Despite \nthe tremendous benefits of having efficient asynchronous code, it's neither practical \nnor realistic to assume that every library will be rewritten for Twisted and Scrapy. \nUsing Twisted's thread pools and the reactor.spawnProcess()  method, we can use \nany Python library and binaries that are written in any language.\nA pipeline that performs CPU-intensive or \nblocking operations\nAs we highlighted in Chapter 8 , Programming Scrapy,  the reactor is ideal for short, \nnonblocking tasks. What can we do if we have to do something more complex or \nsomething that involves blocking? Twisted provides thread pools that can be used \nto execute slow operations in some thread other than the main (Twisted's reactor) \nusing the reactor.callInThread()  API call. This means that the reactor will \nkeep running its processing and reacting to events while the computation takes \nplace. Please keep in mind that processing that is happening in the thread pool isn't \nthread safe. This means that you have all the traditional synchronization problems \nof multithreaded programming when you use global state. Let's start with a simple \nversion of this pipeline, and we will build towards the complete code:\nclass UsingBlocking(object):\n    @defer.inlineCallbacks\n    def process_item(self, item, spider):\n        price = item[\"price\"][0]\n        out = defer.Deferred()\n        reactor.callInThread(self._do_calculation, price, out)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a8ca033-bdc0-4ebc-ad68-ee5a372aabb2": {"__data__": {"id_": "1a8ca033-bdc0-4ebc-ad68-ee5a372aabb2", "embedding": null, "metadata": {"page_label": "168", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8f8372e-28d4-459c-a3c4-acc0d9e92a7d", "node_type": "4", "metadata": {"page_label": "168", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e3937c377555e4d19308166d85f9fd0ac660feeff78a0124eba72cb3e24f7692", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 168 ]        item[\"price\"][0] = yield out\n        defer.returnValue(item)\n    def _do_calculation(self, price, out):\n        new_price = price + 1\n        time.sleep(0.10)\n        reactor.callFromThread(out.callback, new_price)\nIn the preceding pipeline, we see the basic primitives in action. For every Item , we \nextract the price, and we want to process it using the _do_calucation()  method. \nThis method uses time.sleep() , a blocking operation. We will let it run in another \nthread using the reactor.callInThread()  call. This takes the function to call as \narguments and any number of arguments that pass to our function. Obviously, we \npass the price  but we also create and pass a Deferred  that is named out. When our \n_do_calucation()  completes its calculations, we will use the out callback to return \nthe value. In the next step, we yield this Deferred  and set the new value for the \nprice, and we finally return the Item .\nInside _do_calucation() , we notice a trivial calculation\u2014an increase of the price \nby one\u2014and then a sleep of 100ms. That's a lot of time, and if called in the reactor \nthread, it would prevent us from being able to process more than 10 pages per \nsecond. By running it in another thread, we don't have this problem. Tasks will \nqueue up in the thread pool waiting for a thread to become available and as soon \nas this happens, that thread will sleep for 100ms. The final step is to fire the out \ncallback. Normally, we could do this using out.callback(new_price) , but since \nwe are now in another thread, it's not safe to do so. If we were doing so, the code of \nDeferred  and, consequently, Scrapy's functionality would be called from another \nthread, which would sooner or later result in corrupted data. Instead of doing this, \nwe use reactor.callFromThread() , which also takes a function as argument  \nand any number of extra arguments to be passed to our function. This function  \nwill be queued and called from the reactor thread, which in turn will unblock \nprocess_item()  objects yield  and resume Scrapy's operation for this Item .\nWhat happens if we have global state, for example counters, moving averages, and \nso on, that we need to use in our _do_calucation() ? Let's, for example, add two \nvariables, beta  and delta , as follows:\nclass UsingBlocking(object):\n    def __init__(self):\n        self.beta, self.delta = 0, 0\n    ...\n    def _do_calculation(self, price, out):\n        self.beta += 1\n        time.sleep(0.001)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1585ce30-9668-4d0f-ac1c-74624f36322c": {"__data__": {"id_": "1585ce30-9668-4d0f-ac1c-74624f36322c", "embedding": null, "metadata": {"page_label": "169", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8382130-67d3-4698-8a03-9f5e6787daec", "node_type": "4", "metadata": {"page_label": "169", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c669cc7db7dfc9e427b05bf8332507f52f87c2a6cb4a5bbecaf97b155a7f0398", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 169 ]        self.delta += 1\n        new_price = price + self.beta - self.delta + 1\n        assert abs(new_price-price-1) < 0.01\n        time.sleep(0.10)...\nThe preceding code is wrong and gives us assertion errors. That's because if a \nthread switch happens between self.beta  and self.delta , and another thread \nresumes calculating the price using these beta /delta  values, it will find them in an \ninconsistent state ( beta  being larger than delta ), thus, calculate erroneous results. \nThe short sleep makes this more likely, but even without it, the race condition would \nsoon demonstrate itself. To prevent this from happening, we have to use a lock, for \nexample, Python's threading.RLock()  recursive lock. Using it, we ensure that no \ntwo threads will execute the critical section it protects at the same time:\nclass UsingBlocking(object):\n    def __init__(self):\n        ...\n        self.lock = threading.RLock()\n    ...\n    def _do_calculation(self, price, out):\n        with self.lock:\n            self.beta += 1\n            ...\n            new_price = price + self.beta - self.delta + 1\n        assert abs(new_price-price-1) < 0.01 ...\nThe preceding code is now correct. Please note that we don't need to protect the \nentire code but just enough to cover the use of global state.\nThe full code for this example is in ch09/properties/\nproperties/pipelines/computation.py .\nTo use this pipeline, we just have to add it to the ITEM_PIPELINES  setting inside \nsettings.py  as follows:\nITEM_PIPELINES = { ...\n    'properties.pipelines.computation.UsingBlocking': 500,\nWe can run the spider as usual. The pipeline latency jumps significantly by 100 \nms, as expected, but we will surprisingly find that throughput remains exactly the \nsame\u2014about 25 items per second.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79e94695-9480-4661-8700-a24c9252926d": {"__data__": {"id_": "79e94695-9480-4661-8700-a24c9252926d", "embedding": null, "metadata": {"page_label": "170", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1999008-d6b1-4f19-a3d0-998631528ad6", "node_type": "4", "metadata": {"page_label": "170", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "efea18c252157631d3d260b49c6aa87b1e4324d539c415f0e14954470c924b48", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 170 ]A pipeline that uses binaries or scripts\nThe most agnostic interface one can have to a piece of legacy functionality is that of \na standalone executable or script. It may take a few seconds to start (for example, \nloading data from databases), but after that, it will likely be able to process many \nvalues with a small latency. Even in this case, Twisted has us covered. We can use \nthe reactor.spawnProcess()  API and the relevant protocol.ProcessProtocol  to \nrun executables of any kind. Let's take a look at an example. Our sample script will \nbe as follows:\n#!/bin/bash\ntrap \"\" SIGINT\nsleep 3\nwhile read line\ndo\n    # 4 per second\n    sleep 0.25\n    awk \"BEGIN {print 1.20 * $line}\"\ndone\nThis is a simple bash script. As soon as it starts, it disables Ctrl + C. This is to overcome \na peculiarity of the system that propagates Ctrl + C to subprocesses and terminates \nthem prematurely causing Scrapy itself to not terminate while waiting indefinitely for \na result from these processes. After disabling Ctrl + C, it sleeps for three seconds to \nemulate boot time. Then it reads lines from the input, waits 250ms, and then returns \nthe resulting price, which is the original that is multiplied by 1.20 as calculated by the \nawk Linux command. The maximum throughput that this script could have is  \n1/250ms = 4 Items  per second. Let's test it with a short session as follows:\n$ properties/pipelines/legacy.sh \n12 <- If you type this quickly you will wait ~3 seconds to get results\n14.40\n13 <- For further numbers you will notice just a slight delay\n15.60\nAs Ctrl + C has been deactivated, we have to terminate the session with Ctrl + D. \nGreat! So, how do we use this script from Scrapy? Again, we start with a slightly \nsimplified version:\nclass CommandSlot(protocol.ProcessProtocol):\n    def __init__(self, args):", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd6ef9ec-3555-4604-b7fa-25d38b984ef5": {"__data__": {"id_": "bd6ef9ec-3555-4604-b7fa-25d38b984ef5", "embedding": null, "metadata": {"page_label": "171", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "852ba98d-947e-4f3e-bd60-85b47344026d", "node_type": "4", "metadata": {"page_label": "171", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "d7b90b81b178e288545eaaeb1ce4b21766b18727b00213ce2a7bb4d937de0422", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 171 ]        self._queue = []\n        reactor.spawnProcess(self, args[0], args)\n    def legacy_calculate(self, price):\n        d = defer.Deferred()\n        self._queue.append(d)\n        self.transport.write(\"%f\\n\" % price)\n        return d\n    # Overriding from protocol.ProcessProtocol\n    def outReceived(self, data):\n        \"\"\"Called when new output is received\"\"\"\n        self._queue.pop(0).callback(float(data))\nclass Pricing(object):\n    def __init__(self):\n        self.slot = CommandSlot(['properties/pipelines/legacy.sh'])\n    @defer.inlineCallbacks\n    def process_item(self, item, spider):\n        item[\"price\"][0] = yield self.slot.legacy_calculate(item[\"price\"][0])\n       defer.returnValue(item)\nWe find the definitions of a ProcessProtocol  named CommandSlot  and our \nPricing  spider here. Inside __init__() , we create the new CommandSlot , which in \nits constructor initializes an empty queue and starts a new process using reactor.\nspawnProcess() . This call takes as its first argument a ProcessProtocol  that \nis used to send and receive data from the process. In this case, it's self  because \nspawnProcess()  is called from within the protocol  class. The second argument is \nthe name of the executable. The third argument, args , keeps all the command-line \narguments for this binary as a sequence of strings.\nInside pipeline's process_item() , we essentially delegate all the work in the \nlegacy_calculate()  method of CommandSlot , which returns a Deferred  that we \nyield . legacy_calculate()  creates a Deferred , enqueues it, and writes the price to \nthe process using transport.write() . transport  is provided by ProcessProtocol  \nin order to allow us to communicate with the process. Whenever we receive data \nfrom the process, outReceived()  gets called. By enqueuing Deferred  and since \nprocessing from our shell script happens in order, we can just pop the oldest \nDeferred  from the queue and fire it with the received value. That's all. We can \nenable this pipeline by adding it to ITEM_PIPELINES  and running it as usual:\nITEM_PIPELINES = {...\n    'properties.pipelines.legacy.Pricing': 600,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7a629a0-636f-4d05-a893-46ec916239ec": {"__data__": {"id_": "c7a629a0-636f-4d05-a893-46ec916239ec", "embedding": null, "metadata": {"page_label": "172", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "803f1263-84ad-4d19-9e7e-9ee38a82ab5c", "node_type": "4", "metadata": {"page_label": "172", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ea55b8a9a0d7bb1722f88cbf5e2722fda1cb30000b87d51d80d650fef3717cb6", "class_name": "RelatedNodeInfo"}}, "text": "Pipeline Recipes[ 172 ]If we perform a run, the one thing that we will observe is that the performance \nis horrible. As we would expect, our process becomes a bottleneck and limits the \nthroughput to four Items  per second. To increase it, all we need to do is modify the \npipeline slightly to allow multiple such processes to run in parallel, as follows:\nclass Pricing(object):\n    def __init__(self):\n        self.concurrency = 16\n        args = ['properties/pipelines/legacy.sh']\n        self.slots = [CommandSlot(args) \n                      for i in xrange(self.concurrency)]\n        self.rr = 0\n    @defer.inlineCallbacks\n    def process_item(self, item, spider):\n        slot = self.slots[self.rr]\n        self.rr = (self.rr + 1) % self.concurrency\n        item[\"price\"][0] = yield\n                         slot.legacy_calculate(item[\"price\"][0])\n        defer.returnValue(item)\nThis is nothing more than starting 16 instances and sending prices in each of them in \na round-robin fashion. This pipeline now provides a maximum throughput of 16*4 = \n64 items per second. We can confirm it with a quick crawl as follows:\n$ scrapy crawl easy -s CLOSESPIDER_ITEMCOUNT=1000\n...\nScraped... 0.0 items/s, avg latency: 0.00 s and avg time in pipelines: \n0.00 s\nScraped... 21.0 items/s, avg latency: 2.20 s and avg time in pipelines: \n1.48 s\nScraped... 24.2 items/s, avg latency: 1.16 s and avg time in pipelines: \n0.52 s\nThe latency, as expected, increased by 250 ms, but the throughput is still ~25 items/s.\nPlease keep in mind that the preceding method uses transport.write()  to queue \nall the prices in this shell script's input. This may or may not be okay for your \napplication, especially if it uses way more data than just a few numbers. The full \ncode on Git enqueues both values and callbacks, and it doesn't send a new value to \nthe script unless the result for the previous one has been received. You may find this \nway friendlier to your legacy applications, but it adds some complexity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8916d278-7451-4e51-8c4c-844938a342d7": {"__data__": {"id_": "8916d278-7451-4e51-8c4c-844938a342d7", "embedding": null, "metadata": {"page_label": "173", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1af94268-07e7-437d-a480-295645bf151c", "node_type": "4", "metadata": {"page_label": "173", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7d57abba2f574601c3fb13b8ecee08745c9b9b930a1b70d94c6c570e249a5ccd", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 9[ 173 ]Summary\nYou just studied quite a few sophisticated Scrapy pipelines. By now, you have seen \neverything you may need in terms of Twisted programming, and you know how \nto implement complex functionality including processing, and storing Items  using \nItem Processing Pipelines. We saw how performance changes by adding more \npipeline stages in terms of latency and throughput. Usually, latency and throughput \nare considered inversely proportional, but this is under the assumption of constant \nconcurrency (for example, a limited number of threads). In our case, we started with \na concurrency of 250.77 19 N S T = \u22c5 = \u22c5 \u2245, and after adding pipeline stages, we ended \nup 253.33 83 N= \u22c5 \u2245 with without facing any performance problems. That's the power \nof Twisted programming! It's now time to move on to Chapter 10 , Understanding \nScrapy's Performance,  to make perfect sense of Scrapy's performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5672abde-9b9a-4fdb-8e8e-69c76099376f": {"__data__": {"id_": "5672abde-9b9a-4fdb-8e8e-69c76099376f", "embedding": null, "metadata": {"page_label": "174", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3edb30d8-c393-4de2-b658-be1a77b4a650", "node_type": "4", "metadata": {"page_label": "174", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7ef5bbbbec048bcd4c6365e326e1d4e69dcd9e0df18743961fd02ffdf02ce54a", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43d021b0-e35e-4997-8cd0-44001ed9d7d3": {"__data__": {"id_": "43d021b0-e35e-4997-8cd0-44001ed9d7d3", "embedding": null, "metadata": {"page_label": "175", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9acc31be-fb6d-4ad3-9653-3b8567a06c7f", "node_type": "4", "metadata": {"page_label": "175", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b30fb5ec20a657e268f4af480ec9ea7f58db90cf407a7de71e471c94bb50bada", "class_name": "RelatedNodeInfo"}}, "text": "[ 175 ]Understanding Scrapy's \nPerformance\nGenerally, it's easy to get performance wrong. With Scrapy, it's not just easy\u2014it's \nalmost certain because there are quite a few counterintuitive behaviors. Unless \nyou have a good understanding of Scrapy's internals, you will find yourself \nworking hard, optimizing performance while getting zero gains. That is part \nof the complexity of working with high-performance, low-latency, and highly-\nconcurrent environments. Amdahl's law still holds true while optimizing bottleneck \nperformance, but unless you identify the real bottleneck, optimizations on any other \npart of the system will not increase the number of items you scrape per second \n(throughput). More intuition can be gained by reading the classic The Goal  by Dr. \nGoldratt , a business book that explains, with some excellent metaphors, the idea of \nthe bottleneck, latency, and throughput. The same concepts hold identically true \nto software too. This chapter will help you identify the bottleneck on your Scrapy \nconfiguration and will help you avoid obvious mistakes.\nPlease keep in mind that this is a quite advanced chapter and some mathematics are \ninvolved. The calculations are simple and accompanied with diagrams and plots that \ndemonstrate the same concepts. If you don't like math, just ignore the formulas and \nyou will still gain a significant insight into how Scrapy's performance works.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "720d0810-6731-4588-af7e-e73cff16cda9": {"__data__": {"id_": "720d0810-6731-4588-af7e-e73cff16cda9", "embedding": null, "metadata": {"page_label": "176", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "acb8b2fe-19dd-4459-8e9f-e244c68af468", "node_type": "4", "metadata": {"page_label": "176", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "faca7fe007b1d1f1e834d6d657cfc227d9c05645a7029ed475d6b2e4580f93d4", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 176 ]Scrapy's engine \u2013 an intuitive approach\nParallel systems look a lot like piping systems. In computer science, we use the \nqueue symbol to represent queues and processing elements ( Figure 1.  on the left). A \nfundamental law for queue systems is Little's law, which asserts that the number of \nelements in the queuing system ( N) in equilibrium is equal to the throughput of the \nsystem ( T) multiplied by the total queuing/service time ( S); N = T \u2219 S . The other two \nforms, T = N / S  and S = N / T , are also useful for calculations.\nFigure 1. Little's law, queuing systems, and pipes\nThere's a similar law for the geometry of a pipe ( Figure 1.  on the right). The volume of a \npipe ( V) equals the length of the pipe L multiplied by cross-sectional area (A); V = L \u2219 A .\nIf we imagine length representing service time ( L ~ S ), volume representing elements \nin the processing system ( V ~ N ), and across-sectional area representing throughput \n(A ~ N ), then Little's law and the volume formula are the same thing.\nDoes this analogy make sense? The answer is almost. If we imagine \nunits of work as small drops of liquid moving with constant speed \ninside the pipe, then L ~ S  absolutely makes sense because the \nlonger the pipe, the more time a drop will spend in it. V ~ N  also \nmakes sense because the larger the pipe, the more drops it will be \nable to fit in it. Annoyingly, we can also squeeze more drops in a \npipe by applying more pressure. A ~ T  is where the analogy falls \nover. In pipes, the real throughput, that is, the number of drops \nthat goes in/out of it per second, is called \"volumetric flow rate\" \nand unless special conditions are met (orifices), it is proportional \nto A2 instead of A. This is because a wider pipe doesn't mean just \nmore liquid out, but also liquid moving faster because there's \nmore space between the walls of the pipe. For the purposes of \nthis chapter though, we can ignore these geeky details and live \nin a fantasy world where pressure and speed are constant and \nthroughput is directly proportional to the cross-sectional area.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7838610a-58a8-4e0b-9605-7ed75c3f322e": {"__data__": {"id_": "7838610a-58a8-4e0b-9605-7ed75c3f322e", "embedding": null, "metadata": {"page_label": "177", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa2afdb0-3728-4031-a02a-204891e8c9b3", "node_type": "4", "metadata": {"page_label": "177", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "cd19ef37efa9514caf323abf71e5ccfd0d722d9954947708b64779a4d392b9e0", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 177 ]Little's law is very similar to the simple volume formula, and this is what makes \nthis \"pipe model\" so intuitive and powerful. Let's examine the examples of Figure 1  \n(on the right) in a bit more detail. Let's assume that the pipe system represents the \ndownloader of Scrapy. The first one\u2014a very \"thin\" downloader\u2014may have a total \nvolume/concurrency level ( N) = 8 concurrent requests. The length/latency ( S) could \nbe something, such as S = 250 ms, for a fast website. Given N and S, we can now \ncalculate the volume/throughput of the processing element T = N/S = 8/0.25 = 32  \nrequests per second.\nYou will note that latency is mostly out of our control because it depends on the \nperformance of the remote server and our network's latencies. What we can easily \ncontrol is the level of concurrency ( N) on the downloader by increasing it from 8 to \n16 or 32 parallel requests, as we see in the second and third pipe on Figure 1 . With \nconstant length (outside our control), we can only increase volume by increasing the \ncross-section , that is, increasing throughput! In Little's law terms, with 16 Requests in \nparallel, we have T = N/S = 16/0.25 = 64  requests per second, and with 32 requests in \nparallel, we get T = N/S = 32/0.25 = 128  requests per second. Excellent! It seems like we \ncan make a system infinitely fast by increasing concurrency. Before we rush to such \nconclusions though, we should also consider the effects of cascading queuing systems.\nCascading queuing systems\nWhen you connect several pipes with different cross-sectional areas/throughputs one \nafter the other, intuitively one can understand that the flow of the overall system will \nbe limited by the flow of the narrowest (smallest throughput: T) pipe (see Figure 2 ).\nFigure 2. Cascading queuing systems with different capacities", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2468823a-ea15-47dd-a3e9-1e7069d87574": {"__data__": {"id_": "2468823a-ea15-47dd-a3e9-1e7069d87574", "embedding": null, "metadata": {"page_label": "178", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a4d7b75-e30a-4b37-9bce-dcbbbae793de", "node_type": "4", "metadata": {"page_label": "178", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f22e28c2d83948ca1373f6913759a2e6c0fe6d2584a297430de97a20c7b407d3", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 178 ]You can also observe that the placement of the narrowest pipe\u2014the bottleneck\u2014defines \nhow \"full\" other pipes are. If you think about fullness relating it with the memory \nrequirements for your system, you realize that the placement of the bottleneck is very \nimportant. It's better to have a configuration that keeps full pipes where one unit of \nwork costs us little. In Scrapy, a unit of work (crawling a page) consists mostly of a \nURL (a few bytes) before the downloader and the URL plus the server's response (way \nlarger) after it.\nThis is why it's wise in a Scrapy system to place the bottleneck \nin the downloader.\nIdentifying the bottleneck\nA very important benefit of our piping system metaphor is that it makes the process \nof identifying the bottleneck visually intuitive. If you look at Figure 2 , you will notice \nthat everything before \"the bottleneck\" is full while everything after it isn't.\nThe good news is that, in most systems, we can monitor how full a queuing system is \nusing the system's metrics relatively easily. By careful inspection of Scrapy's queues, \nwe can understand where the bottleneck is, and if it's not in the downloader, we can \nadjust the settings in order to make it so. Any improvement that doesn't improve \nthe bottleneck will give no throughput benefit. The only thing one can achieve \nby hacking other parts of the system is to make things worse, likely moving the \nbottleneck somewhere else. This feels a bit like tail chasing, and it can take for ages \nand make you feel despair. You have to follow a systematic approach, identify the \nbottleneck, and \"know where to hit with a hammer\" before you hack any code or \nconfiguration. As you will see in many cases, including most examples of this book, \nthe bottleneck is not where one would expect it to be.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1daeba27-ba33-4235-8409-54ce054bd6ed": {"__data__": {"id_": "1daeba27-ba33-4235-8409-54ce054bd6ed", "embedding": null, "metadata": {"page_label": "179", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f568230d-3a92-4131-a13c-e1a0b50a617e", "node_type": "4", "metadata": {"page_label": "179", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "5c5fb67e484cf72af868dd7a97561cf490360efc94f796ab9d9f74763c7980a6", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 179 ]Scrapy's performance model\nLet's return to Scrapy and see its performance model in detail (see Figure 3 ).\nFigure 3. Scrapy's performance model\nScrapy consists of the following:\n\u2022 The scheduler : This is where multiple Request  get queued until the \ndownloader is ready to process them. They consist mostly of just URLs and, \nthus, are quite compact, which means that having many of them doesn't hurt \nthat much and allows us to keep the downloader fully utilized in case of \nirregular flow of incoming Request .\n\u2022 The throttler : This is a safety valve that feeds back from the scraper (the \nlarge tank) and if the aggregated size of Response  in progress is larger than 5 \nMB it stops the flow of further Request  into the downloader. This can cause \nunexpected performance fluctuations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee66eb48-2e1f-42ac-927e-8f38364ca8f8": {"__data__": {"id_": "ee66eb48-2e1f-42ac-927e-8f38364ca8f8", "embedding": null, "metadata": {"page_label": "180", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "445396a4-bf24-41df-bb07-c1006109eca4", "node_type": "4", "metadata": {"page_label": "180", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "64465723cd7ad9da1ec5a0f5bb123bba7afd83bfb75cf1e727fed42101141a94", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 180 ]\u2022 The downloader : This is the most important component of Scrapy in terms \nof performance. It poses a complex limit on the number of Request  it can \nperform in parallel. Its delay (the length of the pipe) is equal to the time it \ntakes the remote server to respond, plus any network/operating system and \nPython/Twisted delays. We can adjust the number of parallel Requests , but \nwe, typically, have little control over delays. The capacity of the downloader \nis limited by the CONCURRENT_REQUESTS*  settings, as we shall soon see.\n\u2022 The Spider : This is the part of the scraper that turns Response  to Item  and \na further Request . We write these, and typically they aren't a performance \nbottleneck as long as we follow the rules.\n\u2022 Item pipelines : This is the second part of the scraper that we write. \nOur spiders might generate hundreds of Items  per Request , and only \nCONCURRENT_ITEMS  will be processed in parallel at a time. This is important \nbecause if, for example, you're doing database accesses in your pipelines, \nyou might unintentionally flood your database and the default (100) seems \ndangerously high.\nBoth spiders and pipelines should have asynchronous code and may induce as \nmuch latency as necessary but still shouldn't be the bottleneck. Rarely, our spiders/\npipelines do heavy processing. If this is the case, then our server's CPU might \nbecome the bottleneck.\nGetting component utilization  \nusing telnet\nIn order to understand how Request s/Item s flow though the pipes, we aren't really \nable to measure the flows (although this would be a cool feature). Instead, we can \neasily measure how much liquid, that is, Request s/Response s/Item s, exists in each \nof Scrapy's processing stages.\nScrapy runs the telnet service via which we can get performance information. We \ncan connect to it by using the telnet command on port 6023 . We then get a Python \nprompt inside Scrapy. Be careful, if you do something blocking there, such as time.\nsleep() , it will halt the crawler's functionality. Several interesting metrics get \nprinted by the built-in est()  function. Some of them are either very specialized or \ncan be deduced from a few core metrics. I will only show you the latter in the rest of \nthe chapter. Let's explore them with an example run. While we run a crawl, we open \na second terminal on our dev machine, telnet on port 6023 , and run est() .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9640be44-c19f-45f1-876b-ec09e99ee6eb": {"__data__": {"id_": "9640be44-c19f-45f1-876b-ec09e99ee6eb", "embedding": null, "metadata": {"page_label": "181", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0410652a-7d46-411e-bd7e-f8b3011562dd", "node_type": "4", "metadata": {"page_label": "181", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "cc1f129cce7e890c9c93a9c50ec9bb274852207e8a956ceede9f309963c9d4ec", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 181 ]The code from this chapter is in the ch10  directory. This \nexample in particular is in the ch10/speed  directory.\nOn the first terminal, we run the following code:\n$ pwd\n/root/book/ch10/speed\n$ ls\nscrapy.cfg  speed\n$ scrapy crawl speed -s SPEED_PIPELINE_ASYNC_DELAY=1\nINFO: Scrapy 1.0.3 started (bot: speed)\n...\nDon't worry about what this scrapy crawl speed  and its arguments mean for now. \nWe will explain all of them in the rest of the chapter. On the second terminal, run  \nthe following:\n$ telnet localhost 6023\n>>> est()\n...\nlen(engine.downloader.active)                   : 16\n...\nlen(engine.slot.scheduler.mqs)                  : 4475\n...\nlen(engine.scraper.slot.active)                 : 115\nengine.scraper.slot.active_size                 : 117760\nengine.scraper.slot.itemproc_size               : 105\nThen press Ctrl + D on the second terminal to exit telnet and get back to the first \nterminal, and press Ctrl + C to stop the crawl.\nWe ignore dqs for now. If you have enabled persistence support \nby setting the JOBDIR  setting, you will also get non-zero dqs \n(len(engine.slot.scheduler.dqs) ), which you should \nadd to the size of mqs to follow the rest of analysis.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84d4066e-d111-433c-b1ea-ce9fa6c0078f": {"__data__": {"id_": "84d4066e-d111-433c-b1ea-ce9fa6c0078f", "embedding": null, "metadata": {"page_label": "182", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "160275d7-e07b-4540-b4fb-84e02885897b", "node_type": "4", "metadata": {"page_label": "182", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "1e656224f6cec3de6a0dbf0ed874da7ca9ed4d45c8cddff3f26ef30ee9a2afe8", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 182 ]Let's see what these core metrics mean in this example. mqs indicates that there are \nquite a few (4,475 requests) waiting on our scheduler. That's okay. len(engine.\ndownloader.active)  indicates that, right now, there are 16 requests actively being \ndownloaded by the downloader. This is equal to what we've set for CONCURRENT_\nREQUESTS  on the settings of this spider so that's excellent. len(engine.scraper.\nslot.active)  tells us that there are 115 Response s actively being processed in the \nscraper. The total size of those Response s is 115 kb told to us by (engine.scraper.\nslot.active_size) . Out of those Response s, 105 Item s are currently in process \nby our pipelines, (engine.scraper.slot.itemproc_size) , which means that the \nrest of them (10) are in progress in our spider. Overall\u2014we see that the bottleneck \nseems to be the downloader as, before that, we have a huge queue of work ( mqs) and \nthe downloader is fully utilized; after that, we have a high but more or less stable \namount of work (you can confirm this by performing est()  a few times).\nAnother interesting source of information is the stats  object\u2014the one that typically \ngets printed at the end of a crawl. We can access it at any point as a dict  from telnet \nvia stats.get_stats()  and print it nicely using the p() function:\n$ p(stats.get_stats())\n{'downloader/request_bytes': 558330,\n...\n 'item_scraped_count': 2485,\n...}\nThe most interesting metric for us right now is item_scraped_count , which is \naccessible directly through stats.get_value('item_scraped_count') . This tells \nus how many items have been scraped up to now and should be increasing with a \nrate that is the throughput of the system ( Item s/second).\nOur benchmark system\nFor Chapter 10 , Understanding Scrapy's Performance , I wrote a simple benchmark \nsystem that allows us to evaluate performance under different scenarios. The code \nis somewhat cumbersome, and you can find it in speed/spiders/speed.py , but we \nwon't go into it in depth there.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "895f0c71-f883-44c9-a79a-91536a2e3ae0": {"__data__": {"id_": "895f0c71-f883-44c9-a79a-91536a2e3ae0", "embedding": null, "metadata": {"page_label": "183", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eccbc706-8b36-4643-ac26-35e67e98a2ee", "node_type": "4", "metadata": {"page_label": "183", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "722e9e5b94938ee735382ebb560841d19120d7fbc3731f476e6a6ce1a9c241ae", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 183 ]The system consists of the following:\n\u2022 The handlers of the http://localhost:9312/benchmark/...  directories on \nour web server. We can control the structure (See Figure 4 ) of the fake website \nas well as how quickly pages load by adjusting URL arguments/Scrapy \nsettings. Don't worry about the details\u2014we will see many examples soon. \nFor now, you can notice the differences between http://localhost:9312/\nbenchmark/index?p=1  and http://localhost:9312/benchmark/id:3/\nrr:5/index?p=1 . The first one loads within half a second and has single \n-item detail pages, while the second takes five seconds to load and has three \nitems per detail page. We can also add some hidden garbage data in pages to \nmake them a bit heavier. For example, check out http://localhost:9312/\nbenchmark/ds:100/detail?id0=0 . By default (see speed/settings.py ), \npages render in SPEED_T_RESPONSE  = 0.125 seconds and the fake website has \nSPEED_TOTAL_ITEMS  = 5000 Item s.\nFigure 4. Our benchmarking server creates a fake website with adjustable structure\n\u2022 A spider, SpeedSpider , fakes a few ways of retrieving start_requests()  \ncontrolled by the SPEED_START_REQUESTS_STYLE  setting, and provides a \ntrivial parse_item()  method. By default, we feed all starting URLs directly \nto Scrapy's scheduler using the crawler.engine.crawl()  method.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05fa6c71-e460-49fc-98af-052af603b48a": {"__data__": {"id_": "05fa6c71-e460-49fc-98af-052af603b48a", "embedding": null, "metadata": {"page_label": "184", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6076a4a4-5a25-4e15-a6e8-3eca1aa2cf6c", "node_type": "4", "metadata": {"page_label": "184", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "1fd2b9554cb9085688337f87b93a780c579576cf50d746b7eccf4953c1178b35", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 184 ]\u2022 A pipeline, DummyPipeline , that fakes some processing. It has four different \ntypes of delays that this processing might induce. Blocking/computing/\nsynchronous delay ( SPEED_PIPELINE_BLOCKING_DELAY \u2014this is bad), \nasynchronous delay ( SPEED_PIPELINE_ASYNC_DELAY \u2014this is okay), remote \nAPI call using the treq  library (SPEED_PIPELINE_API_VIA_TREQ \u2014this is \nokay), and a remote API call using Scrapy's crawler.engine.download() \n(SPEED_PIPELINE_API_VIA_DOWNLOADER \u2014this is not that okay). By default, \nthe pipeline doesn't add any delays.\n\u2022 A set of high performance settings in settings.py . Everything that could \neven slightly slow down the system has been disabled. We also disable the \nper-domain request limit because we hit our local server only.\n\u2022 A little metrics capture extension that is similar to the one from Chapter 8 , \nProgramming Scrapy . This periodically prints core metrics.\nWe've already used the system in the previous example, but let's rerun a simulation \nwhile also using Linux's time utility to measure the total execution time. We will see \nthe core metrics being printed as follows:\n$ time scrapy crawl speed\n...\nINFO:  s/edule  d/load  scrape  p/line    done       mem\nINFO:        0       0       0       0       0         0\nINFO:     4938      14      16       0      32     16384\nINFO:     4831      16       6       0     147      6144\n...\nINFO:      119      16      16       0    4849     16384\nINFO:        2      16      12       0    4970     12288\n...\nreal  0m46.561s\nColumn Metric\ns/edule len(engine.slot.scheduler.mqs)\nd/load len(engine.downloader.active)\nscrape len(engine.scraper.slot.active)\np/line engine.scraper.slot.itemproc_size\ndone stats.get_value('item_scraped_count')\nmem engine.scraper.slot.active_size", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acf1d12f-f167-4d6c-9712-41b183869668": {"__data__": {"id_": "acf1d12f-f167-4d6c-9712-41b183869668", "embedding": null, "metadata": {"page_label": "185", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb977b19-ef57-4f49-bb58-c50a9eb3bef1", "node_type": "4", "metadata": {"page_label": "185", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "2c6cfbf78d9aa1fd5a16b7c52da4361caec57fdcb35037f09948ce7ef63ecb17", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 185 ]This level of transparency is remarkable. I've shortened the column names a bit, but \nthey should still make sense. We start with 5,000 URLs in the scheduler and end up \nwith 5,000 items in the done column. The downloader is the fully utilized bottleneck \nhaving 16 active Request s consistently with the settings. The scraper, mainly a \nspider because pipelines are empty as we see in the p/line column, is somewhat \nutilized but not fully as is typically the case past the bottleneck. It takes us 46 seconds \nto scrape 5,000 Item s with N=16 parallel requests, which means that the average time \nper request is 46 \u2219 16/5000 = 147 ms instead of our expected 125 ms, which is okay.\nThe standard performance model\nThe standard performance model holds true when Scrapy is functioning properly \nand the downloader is the performance bottleneck. In this case, you will see some \nrequests in the scheduler, and the maximum number of concurrent requests in \nthe downloader. The scraper (spider and pipelines) will be lightly loaded and the \nnumber of Response s in progress will not be constantly increasing.\nFigure 5. The standard performance model and some experimental results\nThere are three main settings that control the downloader's capacity: CONCURRENT_\nREQUESTS , CONCURRENT_REQUESTS_PER_DOMAIN , and CONCURRENT_REQUESTS_PER_\nIP. The first one gives coarse control. No matter what, there won't be more than \nCONCURRENT_REQUESTS  active at a given time. On the other hand, if you target a \nsingle domain or relatively few domains, the CONCURRENT_REQUESTS_PER_DOMAIN  \nmight limit further the number of active requests. If you set CONCURRENT_REQUESTS_\nPER_IP , CONCURRENT_REQUESTS_PER_DOMAIN  will get ignored, and the effective limit \nwill be the number of requests per single (target) IP. In the case of targeting some \nshared hosting sites, for example, many domains may point to a single server and \nthis helps you not hit that server excessively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "654f7ac5-3dde-4845-9a81-6f6e3895bb89": {"__data__": {"id_": "654f7ac5-3dde-4845-9a81-6f6e3895bb89", "embedding": null, "metadata": {"page_label": "186", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e5efcaa-2557-437b-98d7-f2856ebad259", "node_type": "4", "metadata": {"page_label": "186", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "26e51e0f78ee51d7f3c67b172167c9565d07a434fcc76593edf314b3bfb183c0", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 186 ]To keep our performance exploration simple for now, we disable the per-IP limit \nby leaving CONCURRENT_REQUESTS_PER_IP  to the default value (0) and setting \nCONCURRENT_REQUESTS_PER_DOMAIN  to a very large number (1000000). This \ncombination effectively disables those limits and the downloader's concurrency is \ncontrolled entirely by CONCURRENT_REQUESTS .\nWe expect the throughput of our system to depend on the average time that it takes \nto download a page, which includes the remote server's component and our system's \n(Linux, Twisted/Python) latenciesdownload respons e overhead t t t = + . It's also good to account for \nsome startup and shutdown time. This includes the lag between the time you get a \nResponse  and the time its Items  get out on the other end of your pipeline, as well \nas the time until you get your first responses and some inferior performance while \ncaches are cold.\nOverall, if you need to complete a job of N Request s and our Spider is properly \ntuned, you should be able to complete it in:\n( )\n/N\nCONCURRENT _REQUEST Srespons e overhead\njob start stopt t\nt t\u22c5 +\n= +\nIt is somewhat relieving that we don't have control over most of these parameters. \nWe might be able to control overheadt  slightly using a more powerful server and \nsimilarly / start stop t  (which is hardly ever worth the effort because we pay that cost only \nonce per run). Apart from slight improvements for a given workload of N requests, \nall we can seriously tune is the number of CONCURRENT_REQUESTS , which quite often \ndepends on how hard we are allowed to hit remote servers. If we are okay to set it to \na very large number, at some point we will saturate either our server's CPU capacity \nor the remote's ability to respond in a timely manner, that is, respons e t  will skyrocket \nbecause the target website(s) will be throttling us, ban us, or we just got their  \nservers down.\nLet's run an experiment to check our theory. We will crawl 2,000 items with \n{ } 0.125 ,0.25 ,0.5respons e t s s s \u2208  and CONCURRENT_REQUESTS  { }8,16,32,64 \u2208  as follows:\n$ for delay in 0.125 0.25 0.50; do for concurrent in 8 16 32 64; do\n    time scrapy crawl speed -s SPEED_TOTAL_ITEMS=2000 \\\n    -s CONCURRENT_REQUESTS=$concurrent -s SPEED_T_RESPONSE=$delay\n  done; done", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2309, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41a99df0-b334-477b-8459-7ed49fc26891": {"__data__": {"id_": "41a99df0-b334-477b-8459-7ed49fc26891", "embedding": null, "metadata": {"page_label": "187", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "153ac4c6-6ee3-4845-a037-aa7f54d2c5f2", "node_type": "4", "metadata": {"page_label": "187", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "73dfa90a882a4cd18c7b5ca46b3f98523a81c500d2cf193e0fad91ff151cd0c4", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 187 ]On my laptop, I get the following times (in seconds) for completing 2,000 requests:\nCONCURRENT_REQUESTS 125 ms/req 250 ms/req 500 ms/req\n8 36.1 67.3 129.7\n16 19.4 35.3 66.1\n32 11.1 19.3 34.7\n64 7.4 11.1 19.0\nWarning: geeky calculations ahead! Feel free to skim through this paragraph. We can \nsee some of those results in Figure 5 . By reordering the last equation, we can bring it \nto the simple form / overhead start stop y t x t = \u22c5 +  where x = N /CONCURRENT_REQUESTS \nand job r espons e y t x t = \u22c5 + . Using the least squares ( LINEST  Excel function) and the \npreceding data, we calculate overheadt  = 6 ms and start/ stop t  = 3.1s. overheadt  turns out to be a \nnegligible number but start time is significant and favors long runs with thousands \nof URLs. As a result, a very useful formula that we are going to use to approximate \nthe throughput of the system in Requests/second is the following:\n/N\njob s tart stopTt t=\u2212\nBy running a long job of N Requests , we can measure the jobt aggregated time and \nthen it's straightforward to calculate T.\nSolving performance problems\nNow that we have a thorough understanding of what the expected performance of \nour system should be, let's take a look at what we should do in case we don't get \nthe performance we want. We will present different problematic cases by exploring \nsymptoms, performing example crawls that reproduce them, discussing the root \ncause, and finally providing actions that fix them. The order the cases are presented \nin is from higher-level system issues to lower-level Scrapy technical details. This \nmeans that more common cases may appear after less common ones. Please read the \nentire chapter before you start exploring your performance issues.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0d1b15d-4966-4b13-a5ca-c17dbb3d1c0a": {"__data__": {"id_": "e0d1b15d-4966-4b13-a5ca-c17dbb3d1c0a", "embedding": null, "metadata": {"page_label": "188", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d2fade2-e276-42e7-a997-cc4ae8877b43", "node_type": "4", "metadata": {"page_label": "188", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "4f2928333d70ae12057eb9a4b205d7c0b4962d01d25c827dd58661852e10bac4", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 188 ]Case #1 \u2013 saturated CPU\nSymptoms : At some point you will be increasing the level of concurrency, but you \nwill be getting no performance gains. When you reduce the level of concurrency, \neverything works as expected. Your downloader is well utilized, but it seems like the \naverage time per request is exploding. You find out how loaded the CPU is using the \ntop command in Unix/Linux, ps on Power Shell, or the Task Manager on Windows, \nand it seems quite high.\nExample : Let's assume that you run the following command:\n$ for concurrent in 25 50 100 150 200; do\n   time scrapy crawl speed -s SPEED_TOTAL_ITEMS=5000 \\\n    -s CONCURRENT_REQUESTS=$concurrent\n  done\nYou get the time it takes to scrape 5,000 URLs. The Expected  column is calculated \nbased on the previously derived formula, and the CPU load is observed with top \n(you can run this command on a second terminal to dev):\nCONCURRENT_\nREQUESTS Expected (sec) Actual (sec) % of expectedCPU load\n25 29.3 30.34 97% 52%\n50 16.2 18.7 87% 78%\n100 9.7 14.1 69% 92%\n150 7.5 13.9 54% 100%\n200 6.4 14.2 45% 100%\nFigure 6. Performance flattens out as you increase concurrency beyond a certain level", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8971917a-81c5-4bf1-89ee-ffc8939b3256": {"__data__": {"id_": "8971917a-81c5-4bf1-89ee-ffc8939b3256", "embedding": null, "metadata": {"page_label": "189", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e77a9e55-53ef-4dba-9d06-1b7d6f0f255c", "node_type": "4", "metadata": {"page_label": "189", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "2b781d246536eed9a57210d6343f82d5054b1c9b3e6fa0e9ee4ac5560a2628c7", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 189 ]In our experiment, we hardly perform any processing and that's why we can get \nthat high concurrencies. In a more sophisticated system, you will most likely see this \nbehavior earlier.\nDiscussion : Scrapy heavily uses a single thread and as you reach high levels of \nconcurrency, the CPU might become the bottleneck. The recommended level of CPU \nScrapy should be using, assuming that you don't use any thread pools, is around \n80-90%. Please keep in mind that you can have similar problems with other system \nresources, such as network bandwidth, memory, or disk throughput, but all these are \nless likely and fall into the general system administration realm, so we won't address \nthem any further here.\nSolution : I will assume that your code is, in general, efficient. You can get \naggregated concurrency larger than CONCURRENT_REQUESTS  by running many Scrapy \ncrawlers on the same server. This will help you utilize more of the available cores \nespecially if other services or other threads from your pipelines don't use them. \nIf you need even more concurrency, you can use multiple servers (see Chapter 11 , \nDistributed Crawling with Scrapyd and Real-Time Analytics ), in which case you will \nlikely have more memory, network bandwidth, and hard disk throughput available \nas well. Always double-check that CPU usage is your primary constraint.\nCase #2 \u2013 blocking code\nSymptoms : The behavior that you're observing doesn't make any sense. The system is \nvery slow compared to what you expect and curiously the speed doesn't significantly \nchange when you change CONCURRENT_REQUESTS . The downloader looks almost empty \n(way less than CONCURRENT_REQUESTS ) and the scraper has quite a few Response s.\nExample : You can use two benchmark settings, SPEED_SPIDER_BLOCKING_DELAY  \nand SPEED_PIPELINE_BLOCKING_DELAY  (they have identical effects), to enable a 100-\nms blocking delay per Response . We would expect 100 URLs to take 2-3 seconds at \nthe given concurrency levels, but we consistently get ~13 seconds irrespective of the \nvalue of CONCURRENT_REQUESTS :\nfor concurrent in 16 32 64; do\n  time scrapy crawl speed -s SPEED_TOTAL_ITEMS=100 \\\n  -s CONCURRENT_REQUESTS=$concurrent -s SPEED_SPIDER_BLOCKING_DELAY=0.1\ndone\nCONCURRENT_REQUESTS Total time (sec)\n16 13.9\n32 13.2\n64 12.9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28c31606-5e86-4826-884e-5313d1597091": {"__data__": {"id_": "28c31606-5e86-4826-884e-5313d1597091", "embedding": null, "metadata": {"page_label": "190", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c491f2d-3df4-4af7-951f-5228dfc4fa95", "node_type": "4", "metadata": {"page_label": "190", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "08d278e9de7f8f7b0e3629db0a467e9b57c3f0fe9a9d5c739e244a6aaaa2cd90", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 190 ]Discussion : Any trace of blocking code instantly nullifies Scrapy's concurrency and \nessentially sets CONCURRENT_REQUESTS  = 1. Indeed the simple formula; 100 URLs \u2219  \n100 ms (blocking delay) = 10 seconds + star t /stop t , fully explains the delays that we see.\nFigure 7. Blocking code invalidates concurrency in unpredictable ways\nNo matter whether the blocking code is in one of your pipelines or your spider, \nyou will see scraper being fully utilized and everything before and after it being \nempty. This seems to go against the pipeline physics that we talked about before, \nbut we don't have a parallel system any more, so pipeline rules don't apply. It's so \neasy to make this mistake (for example, using blocking APIs) that you will certainly \nget this wrong at some point. You will note that a similar discussion applies to \ncomputationally complex code. You should be using multiple threads for such code, \nas we've seen in Chapter 9 , Pipeline Recipes , or performing it in batch outside Scrapy, \nan example of which we will see in Chapter 11 , Distributed Crawling with Scrapyd and \nReal-Time Analytics .\nSolution : I will assume that you inherited the code base, and you have no intuition \non where the blocking code is. If the system can be functional without any pipelines, \nthen disable your pipelines and check whether the odd behavior persists. If yes, then \nyour blocking code is in your spider. If not, then enable pipelines one-by-one and \nsee when the problem starts. If the system can't be functional without everything \nrunning, then add some log messages on each pipeline stage (or interleave dummy \npipelines that print timestamps) in between your functional ones. By checking the \nlogs, you will easily detect where your system spends most of its time. If you want \na more long-term/reusable solution, you can trace your Requests using dummy \npipelines that add timestamps at each stage to the meta  fields of Request . At the end, \nhook to the item_scraped  signal and log the timestamps. As soon as you find your \nblocking code, convert it to Twisted/asynchronous or use Twisted's thread pools. To \nsee the effects of this conversion, rerun the previous example while replacing SPEED_\nPIPELINE_BLOCKING_DELAY with SPEED_PIPELINE_ASYNC_DELAY . The change in \nperformance is stunning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66bc73e3-b98a-4189-ab07-d00ec4fa5ab6": {"__data__": {"id_": "66bc73e3-b98a-4189-ab07-d00ec4fa5ab6", "embedding": null, "metadata": {"page_label": "191", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4750bf8b-3a68-4c1e-9779-bf8d701a9b22", "node_type": "4", "metadata": {"page_label": "191", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "26e814b53b9e2b2558c6704b34400893bf17769e36e1d9a12bd10b24299d9de8", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 191 ]Case #3 \u2013 \"garbage\" on the downloader\nSymptoms : You get way less than the expected throughput. The downloader \nsometimes looks like it has more Request s than CONCURRENT_REQUESTS .\nExample : We simulate downloading 1,000 pages with a 0.25 sec response time. \nWith the default concurrency of 16, this should take about ~ 19 sec according to our \nformulas. We use a pipeline that uses crawler.engine.download()  to make an \nextra HTTP request to a fake API that responds within one second. You can try it on \nhttp://localhost:9312/benchmark/ar:1/api?text=hello . Let's run a crawl:\n$ time scrapy crawl speed -s SPEED_TOTAL_ITEMS=1000 -s SPEED_T_\nRESPONSE=0.25 -s SPEED_API_T_RESPONSE=1 -s SPEED_PIPELINE_API_VIA_\nDOWNLOADER=1\n...\ns/edule  d/load  scrape  p/line    done       mem\n    968      32      32      32       0     32768\n    952      16       0       0      32         0\n    936      32      32      32      32     32768\n...\nreal 0m55.151s\nThis is really weird. Not only did our job take three times more time than expected, \nbut we also have more than the 16 active requests that CONCURRENT_REQUESTS  \ndefines in the downloader ( d/load ). The downloader is clearly the bottleneck \nbecause it seems to work over capacity! Let's rerun the crawl, and on another \nconsole, open a telnet connection to Scrapy. We can then check which Request s are \nactive on the downloader:\n$ telnet localhost 6023\n>>> engine.downloader.active\nset([<POST http://web:9312/ar:1/ti:1000/rr:0.25/benchmark/api>,  ... ])\nIt looks like it does mostly API Request s instead of downloading regular pages.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "635e9b56-def0-4ecf-98fa-ea006743d31e": {"__data__": {"id_": "635e9b56-def0-4ecf-98fa-ea006743d31e", "embedding": null, "metadata": {"page_label": "192", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c6439f7-2cb9-47db-b37e-559a7b28adc2", "node_type": "4", "metadata": {"page_label": "192", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "d4b9166b186da99f9df979b6fd10e683d90289f2135489a2f49571189e67529c", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 192 ]Discussion : You would expect that nobody uses crawler.engine.download()  as it \nlooks a bit complex to use, but it is used twice in Scrapy's code base for the robots.txt  \nmiddleware and the media pipeline. As a result, it's reasonably suggested as a solution \nwhen people need to consume web APIs. Using this is way better than using blocking \nAPIs such as the popular requests  Python package that we saw in the previous \nsection. It's also slightly simpler to use than understanding Twisted programming and \nusing treq . Now that this book exists though, this isn't an excuse anymore. Puns aside, \nthis mistake is quite hard to debug, so proactively take a look at the active requests \non your downloader while investigating performance. If you find API or media URLs \nthat aren't directly targeted by your crawl, it means that some of your pipelines use \ncrawler.engine.download()  to perform HTTP requests. Our CONCURRENT_REQUESTS  \nlimit doesn't apply for these Request , which means that we will likely see the \ndownloader loaded with more than CONCURRENT_REQUESTS , which seems paradoxical \nat first sight. Unless the number of spurious Request s falls below CONCURRENT_\nREQUESTS , no new normal page Request s will be fetched from the scheduler.\nFigure 8. Performance is defined by the spurious API requests\nAs a result, it's not a coincidence that the throughput that we get from the system \ncorresponds to what we would get if our original Request  lasted 1 sec (the API \nlatency) instead of 0.25 sec (the page download latency). This case is especially \nconfusing because unless API calls are slower than our page requests, we won't \nnotice any performance degradation.\nSolution : We can solve this problem using treq  instead of crawler.engine.\ndownload() . You will note that this will skyrocket the scraper's performance, which \nmight be bad news for your API infrastructure. I would start with a low number  \nof CONCURRENT_REQUESTS  and increase gradually to make sure I don't overload the \nAPI servers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3395945f-3e01-4358-b8f0-cf17d89f5cb4": {"__data__": {"id_": "3395945f-3e01-4358-b8f0-cf17d89f5cb4", "embedding": null, "metadata": {"page_label": "193", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e61c6e8-2b9a-4906-b237-821659570f63", "node_type": "4", "metadata": {"page_label": "193", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "1fbc359c5a525853be3ed450a3bf6a931ea61951bce6b5eadf24ab1e3b96c940", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 193 ]Here's an example of the same run as before but using treq :\n$ time scrapy crawl speed -s SPEED_TOTAL_ITEMS=1000 -s SPEED_T_\nRESPONSE=0.25 -s SPEED_API_T_RESPONSE=1 -s SPEED_PIPELINE_API_VIA_TREQ=1\n...\ns/edule  d/load  scrape  p/line    done       mem\n    936      16      48      32       0     49152\n    887      16      65      64      32     66560\n    823      16      65      52      96     66560\n...\nreal 0m19.922s\nYou will observe one very interesting thing. The pipeline ( p/line ) seems to have \nmany more items than the downloader ( d/load ). That's perfectly fine and it's \ninteresting to understand why.\nFigure 9. It's perfectly fine to have long pipelines (check \"industrial heat exchanger\" \nin Google images).\nThe downloader is fully loaded with 16 Request s as expected. This means that the \nthroughput of the system is T = N/S = 16/0.25 = 64  Request s per second. We can \nconfirm this by noticing the increase on the done  column. A Request  will spend \n0.25 sec inside the downloader, but it will spend 1 sec inside the pipeline because \nof the slow API request. This means that in the pipeline ( p/line ), we expect to see \non average N = T \u2219 S = 64 \u2219 1 = 64  Item s. That's perfectly fine. Does it mean that \nthe pipeline is now the bottleneck? No because we have no limit on the number \nof Response s that we can process simultaneously on our pipelines. As long as the \nnumber doesn't increase indefinitely, we are fine. More on this in the next section.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "935a4940-81f8-4af2-9295-c1becf22dbc5": {"__data__": {"id_": "935a4940-81f8-4af2-9295-c1becf22dbc5", "embedding": null, "metadata": {"page_label": "194", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4fe3759-8fde-47ee-a62c-293b66b8da52", "node_type": "4", "metadata": {"page_label": "194", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ef6cd1bcf4b470043255e88175394d25fb2c992cfbd9dcc7e13a51dc9c645d93", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 194 ]Case #4 \u2013 overflow due to many or large \nresponses\nSymptoms : The downloader works almost at full capacity and after a while it turns \noff. This pattern repeats itself. The memory usage of your scraper is high.\nExample : Here, we have exactly the same setup as before (using treq ), but the \nresponses are somewhat heavy having about 120 kB of HTML. As you can see, this \ntakes 31 seconds to complete instead of about 20:\n$ time scrapy crawl speed -s SPEED_TOTAL_ITEMS=1000 -s SPEED_T_\nRESPONSE=0.25 -s SPEED_API_T_RESPONSE=1 -s SPEED_PIPELINE_API_VIA_TREQ=1 \n-s SPEED_DETAIL_EXTRA_SIZE=120000\ns/edule  d/load  scrape  p/line    done       mem\n    952      16      32      32       0   3842818\n    917      16      35      35      32   4203080\n    876      16      41      41      67   4923608\n    840       4      48      43     108   5764224\n    805       3      46      27     149   5524048\n...\nreal  0m30.611s\nDiscussion : We may naively try to interpret this latency as \"it takes more time to \ncreate, transfer, or process pages\", but that's not what's happening here. There exists \na hardcoded (at the time of writing) limit for the total size of Response s of max_\nactive_size  = 5000000. Each Response  is assumed to have a size equal to the size  \nof its body and at least 1 kB.\nFigure 10. Irregular number of Request s on the downloader indicates Response  size throttling", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be4ebb7a-4e2e-416c-a43b-a76a6578d47f": {"__data__": {"id_": "be4ebb7a-4e2e-416c-a43b-a76a6578d47f", "embedding": null, "metadata": {"page_label": "195", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3e5aa5a-c374-4387-99a4-a9cc19d555a5", "node_type": "4", "metadata": {"page_label": "195", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "655deb092aed692995d023f0e87e9c1c08bc6c4d77c2de0c7d65444a176e7516", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 195 ]One important detail here is that this limit is maybe the most subtle and essential \nmechanism that Scrapy has to protect itself against slow spiders or pipelines. \nIf the throughput of any of your pipelines is slower than the throughput of the \ndownloader, this will eventually happen. It's easy to hit this limit even with small \nResponses  when we have large pipeline processing time. Here's one such extreme \nexample of very long pipeline where the problems start after 80 seconds:\n$ time scrapy crawl speed -s SPEED_TOTAL_ITEMS=10000 -s SPEED_T_\nRESPONSE=0.25 -s SPEED_PIPELINE_ASYNC_DELAY=85\nSolution : There isn't much you can do for this problem with the existing infrastructure. \nIt would be nice to be able to clear the body of Response  as soon as you don't need it \nanymore\u2014likely after your spider, but doing so won't reset Scraper's counters at the \ntime of writing. All you can really do is try to reduce your pipeline's processing time \neffectively reducing the number of Response s in progress in the Scraper. You can \nachieve this with traditional optimization: checking whether APIs or databases you \npotentially interact with can support your scraper's throughput, profiling the scraper, \nmoving functionality from your pipelines to batch/postprocessing systems, and \npotentially using more powerful servers or distributed crawling.\nCase #5 \u2013 overflow due to limited/excessive \nitem concurrency\nSymptoms : Your spider creates multiple Item s per Response . You get lower than \nexpected throughput and likely the same on/off pattern as in the previous case.\nExample : Here, we have a slightly unusual setup where we have 1,000 requests that \nreturn pages with 100 items each. The response time is 0.25 sec and there's a 3 sec \nitem pipeline processing time. We perform several runs with values of CONCURRENT_\nITEMS  ranging from 10 to 150:\nfor concurrent_items in 10 20 50 100 150; do\ntime scrapy crawl speed -s SPEED_TOTAL_ITEMS=100000 -s  \\\nSPEED_T_RESPONSE=0.25 -s SPEED_ITEMS_PER_DETAIL=100 -s  \\\nSPEED_PIPELINE_ASYNC_DELAY=3 -s \\\nCONCURRENT_ITEMS=$concurrent_items\ndone\n...\ns/edule  d/load  scrape  p/line    done       mem\n    952      16      32     180       0    243714", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d74a19c-8be9-4bb1-9b9a-15116c23594f": {"__data__": {"id_": "9d74a19c-8be9-4bb1-9b9a-15116c23594f", "embedding": null, "metadata": {"page_label": "196", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ce3c3fe-f692-44a2-9dde-97a2ae12cd75", "node_type": "4", "metadata": {"page_label": "196", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7c9be5f1fd8e02129f3fc7d92a8cb31d8b79bfb6929e4c93b7873ee5458d9f55", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 196 ]    920      16      64     640       0    487426\n    888      16      96     960       0    731138\n...\n \nFigure 11. Crawl time as a function of CONCURRENT_ITEMS\nDiscussion : It's worth noting again that this only applies to cases where your spider \ngenerates many Item s per Response . Unless this is the case, you can set CONCURRENT_\nITEMS  = 1 and forget about it. It's also worth noting that this is quite a synthetic \nexample since the throughputs are quite large in the order of 1,300 Item s per \nsecond. We get such high throughput due to low and stable latencies, almost no real \nprocessing, and the very low size of Response s. These conditions aren't common.\nThe first thing that we notice is that, while up to now the scrape  and p/line  columns \nused to show the same number, now p/line  shows CONCURRENT_ITEMS  \u2219 scrape . This \nis expected because scrape  shows Reponse s while p/line  shows Item s.\nThe second interesting thing is the bathtub performance function of Figure 11 . The \nplot makes it look a bit more dramatic than it really is because the vertical axis \nis scaled. On the left side, we have very high latency because we hit the memory \nlimits we mentioned on the previous section. On the right side, we have too much \nconcurrency, and we use too much CPU. Getting the optimum exactly right isn't that \nimportant because it can easily shift left or right.\nSolution : It's very easy to detect both problematic symptoms of this case. If you \nget very high CPU usage, it's good to reduce the number of CONCURRENT_ITEMS . If \nyou hit the 5 MB Response  limit, then your pipeline can't follow your downloader's \nthroughput and increasing CONCURRENT_ITEMS  might be able to quickly fix this. If it \ndoesn't make any difference, then follow the advice in the previous section and ask \nyourself twice if the rest of the system is able to support your Scraper's throughput.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b0147a0-ea81-4048-8867-695fce1b4572": {"__data__": {"id_": "4b0147a0-ea81-4048-8867-695fce1b4572", "embedding": null, "metadata": {"page_label": "197", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9fafd24-eee8-49a4-8eb2-ce712306c18f", "node_type": "4", "metadata": {"page_label": "197", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7ea60710a7dd1ad6667d6fd8b036f8d8c7e85343722b0df3d0939651336fb042", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 197 ]Case #6 \u2013 the downloader doesn't have \nenough to do\nSymptoms : You increase CONCURRENT_REQUESTS , but the downloader can't keep up \nand is underutilized. The scheduler is empty.\nExample : First of all, let's run an example without the problem. We will switch to a  \n1 sec response time because this simplifies calculations making downloader throughput \nT = N/S = N/1 =  CONCURRENT_REQUESTS . Let's assume that we run the following:\n$ time scrapy crawl speed -s SPEED_TOTAL_ITEMS=500 \\\n-s SPEED_T_RESPONSE=1 -s CONCURRENT_REQUESTS=64\ns/edule  d/load  scrape  p/line    done       mem\n     436      64       0       0       0         0\n...\nreal  0m10.99s\nWe get a fully utilized downloader (64 requests) and overall time of 11 seconds, \nwhich matches our model for 500 URLs at 64 requests/second  \n(/ / 500/ 64 3.1 10.91start stop S N T t = + = + =  sec).\nNow, let's do the same crawl, but instead of providing the URLs from a list, as we \ndo by default on all those examples, let's use index pages to extract URLs using \nSPEED_START_REQUESTS_STYLE=UseIndex . This is exactly the mode that we've used \nin every other chapter of this book. Each index page by default gives us 20 URLs:\n$ time scrapy crawl speed -s SPEED_TOTAL_ITEMS=500 \\\n-s SPEED_T_RESPONSE=1 -s CONCURRENT_REQUESTS=64 \\\n-s SPEED_START_REQUESTS_STYLE=UseIndex\ns/edule  d/load  scrape  p/line    done       mem\n       0       1       0       0       0         0\n       0      21       0       0       0         0\n       0      21       0       0      20         0\n...\nreal 0m32.24s\nClearly this doesn't look anything like the previous case. Somehow, the  \ndownloader runs in less than the maximum capacity and the throughput  \nis T ( )/ / 500 /32.2 3.1 17start stop N S t = \u2212 = \u2212 = requests/second.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7b28bef-65d3-4474-b1c0-67f52e416957": {"__data__": {"id_": "f7b28bef-65d3-4474-b1c0-67f52e416957", "embedding": null, "metadata": {"page_label": "198", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3aa269f0-73fa-4dc0-8c3c-f3266ad8ba61", "node_type": "4", "metadata": {"page_label": "198", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "4f003ae9c086a0fffdb1899829ad478635146abca9ae25608a21073f5ecc194f", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 198 ]Discussion : A quick look at the d/load  column will convince us that the downloader \nis underutilized. This is because we don't have enough URLs to feed it. Our scraping \nprocess generates URLs slower than its maximum consuming capacity. In this \ncase, 20 URLs + 1 for the next index page get generated from each index page. The \nthroughput couldn't by any means be more than 20 Requests per second because we \ndon't get source URLs fast enough. This problem is too subtle and easy to overlook.\nSolution : If each index page has more than one next page link, we can utilize them \nto accelerate our URL generation. If we can find pages that show more results (for \nexample, 50) per index page even better. We can observe the behavior by running a \nfew simulations:\n$ for details in 10 20 30 40; do for nxtlinks in 1 2 3 4; do\ntime scrapy crawl speed -s SPEED_TOTAL_ITEMS=500 -s SPEED_T_RESPONSE=1 \\\n-s CONCURRENT_REQUESTS=64 -s SPEED_START_REQUESTS_STYLE=UseIndex \\\n-s SPEED_DETAILS_PER_INDEX_PAGE=$details \\\n-s SPEED_INDEX_POINTAHEAD=$nxtlinks\ndone; done\nFigure 12. Throughput as a function of details and next page links per index page", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a2feebb-3390-43eb-9f3e-c75e45cf1df2": {"__data__": {"id_": "5a2feebb-3390-43eb-9f3e-c75e45cf1df2", "embedding": null, "metadata": {"page_label": "199", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f20fd27b-836f-4a34-8f7f-45cb35b82c5d", "node_type": "4", "metadata": {"page_label": "199", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e0557ffea9a4cfd7edd8099e9d49af1ad69ef3eb2754c0edd5079764c59a44e8", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 10[ 199 ]In Figure 12 , we can see how throughput scales with these two parameters. We \nobserve linear behavior, both in terms of next page links, as well as pages  until we \nreach the system's limits. You can experiment by reordering the crawler's Rule s. \nIf you are using LIFO (default) order, you might see a small improvement if you \ndispatch your index page requests first by putting the Rule  that extracts them last \nin your list. You can also try to set a higher priority to the Request s that hit the \nindex. Both techniques don't give impressive improvements, but you can try them \nby setting SPEED_INDEX_RULE_LAST=1  and SPEED_INDEX_HIGHER_PRIORITY=1 , \nrespectively. Please keep in mind that both these solutions will tend to download \nthe entire index first (due to high priority), thus, generating lots of URLs in the \nscheduler, which will increase memory requirements. They will also give very few \nresults until they finish with the index. For small indices this might be okay, but for \nlarger indices, this is certainly undesirable.\nAn easier and more powerful technique is to shard the index. This requires you \nto use more than one initial index URLs that have maximum distance between \nthem. For example, if the index has 100 pages, you may choose page 1 and 51 as \nthe starting ones. The crawler is then able to use the next links to traverse the index \neffectively in twice the speed. A similar thing can be done if you can find a way \nto traverse the index, for example based on the brand of the products or any other \nproperty that is provided to you, and can split the index in roughly equal segments. \nYou can simulate this using the -s SPEED_INDEX_SHARDS  setting:\n$ for details in 10 20 30 40; do for shards in 1 2 3 4; do\ntime scrapy crawl speed -s SPEED_TOTAL_ITEMS=500 -s SPEED_T_RESPONSE=1 \\\n-s CONCURRENT_REQUESTS=64 -s SPEED_START_REQUESTS_STYLE=UseIndex \\\n-s SPEED_DETAILS_PER_INDEX_PAGE=$details -s SPEED_INDEX_SHARDS=$shards\ndone; done\nThe results are better than the previous technique, and I would recommend this \nmethod if it works for you because it's way simpler and cleaner.\nTroubleshooting flow\nTo summarize, Scrapy is designed to have the downloader as a bottleneck. Start with \na low value of CONCURRENT_REQUESTS  and increase until just before you hit one of \nthe following limits:\n\u2022 CPU usage > 80-90%\n\u2022 Source website latency increasing excessively\n\u2022 Memory limit of 5 Mb of Response s in your scraper", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3fb5069-dbe1-402e-8c45-a02b37f7a667": {"__data__": {"id_": "b3fb5069-dbe1-402e-8c45-a02b37f7a667", "embedding": null, "metadata": {"page_label": "200", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65cb6302-11ec-410a-a09d-d363e6681b30", "node_type": "4", "metadata": {"page_label": "200", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "d347f2fb94bc56856d6e302b0ac78ffcd49db018d5b92b53095ebfe0676379a0", "class_name": "RelatedNodeInfo"}}, "text": "Understanding Scrapy's Performance[ 200 ]At the same time also perform the following:\n\u2022 Keep at least a few Request s at all times in the scheduler's queues (mqs/dqs) \nto prevent the downloader's URL starvation\n\u2022 Never use any blocking code or CPU-intensive code\nFigure 13. Troubleshooting Scrapy's performance problems\nFigure 13  summarizes the procedure of diagnosing and repairing Scrapy's \nperformance problems.\nSummary\nIn this chapter, we tried to give you some interesting cases that highlight the fine \nperformance implications of Scrapy's architecture. Details might change in future \nversions of Scrapy, but the intuition provided by this chapter should remain valid \nfor a long time and might also help you with any high-concurrency asynchronous \nsystems that are based on Twisted, Netty Node.js, or similar frameworks.\nWhen it comes to the question of performance in Scrapy, there are three valid \nanswers: I don't know and I don't care, I don't know but I will find out, or I do know. \nAs we demonstrated many times in this chapter, the naive answer, \"we need more \nservers/memory/bandwidth\" is most likely irrelevant to Scrapy's performance. One \nreally needs to understand where the bottleneck is and elevate it.\nIn our last chapter, Chapter 11 , Distributed Crawling with Scrapyd and Real-Time \nAnalytics , we will focus on elevating the performance further, beyond a single \nserver's capacity by distributing our crawls across multiple servers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c78b4d1-4479-4180-84e6-896b52a49b59": {"__data__": {"id_": "7c78b4d1-4479-4180-84e6-896b52a49b59", "embedding": null, "metadata": {"page_label": "201", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60c59991-ce38-4238-8ec0-9c246db55037", "node_type": "4", "metadata": {"page_label": "201", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "7b47204aabf8498e2546aaecf27e1fb0f58146d61792d843a62d5ab0170d7bec", "class_name": "RelatedNodeInfo"}}, "text": "[ 201 ]Distributed Crawling with \nScrapyd and Real-Time \nAnalytics\nWe have come a long way. We familiarized ourselves with two fundamental web \ntechnologies, HTML and XPath, and then we started using Scrapy to crawl complex \nwebsites. Later, we gained a much deeper appreciation of the various features that \nScrapy provides us with through its settings, and then we moved to an even deeper \nunderstanding of both Scrapy and Python when we explored its internal architecture \nand the asynchronous features of its Twisted engine. In the previous chapter, we \nstudied Scrapy's performance and learned how to address complex and often \ncounter-intuitive performance problems.\nIn this last chapter, I would like to give you some directions on how to further use \nthis amazing technology to scale beyond a single server. We will soon discover \nthat crawling is often an \"embarrassingly parallel\" problem; thus, we can easily \nscale horizontally and exploit the resources of multiple servers. In order to do this, \nwe are going to use a Scrapy middleware as we usually do, but we will also use \nScrapyd\u2014an application that is specially designed to manage Scrapy spider's runs \non remote servers. This will allow us to have on our own servers functionality that is \ncompatible with the one that we presented in Chapter 6 , Deploying to Scrapinghub .\nWe are finally going to perform real-time analytics on the extracted data with a \nsimple system that is based on Apache Spark. Apache Spark is a very popular \nframework for large-scale data processing. We will use its Spark Streaming API to \npresent results that get increasingly more accurate as we collect more data. For me, \nthis final application showcases the power and maturity of Python as a language \nbecause, with just this, we can program the full stack from data extraction to \nanalytics writing code that is expressive, compact, and efficient.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9825965-5697-4662-8c4a-4c232afbdbb6": {"__data__": {"id_": "c9825965-5697-4662-8c4a-4c232afbdbb6", "embedding": null, "metadata": {"page_label": "202", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe439298-cfe0-4463-97ab-99bc311cc931", "node_type": "4", "metadata": {"page_label": "202", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "eec2254603325da65b0b4a162deea0426c365977d2249a254ea4dba9ed1a70aa", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 202 ]How does the title of a property affect  \nthe price?\nThe sample problem that we will try to solve is trying to find out how titles correlate \nwith the prices of properties. We would expect terms such as \"jacuzzi\" or \"pool\" to \nbe correlated with higher prices, while others such as \"discount\" with a lower price. \nCombining this information with location, for example, may be used to provide us with \nreal-time alerts on properties that are bargains given their location and description.\nWhat we want to calculate is the shift of the price for a given term:\n( )/term propertie s w ithterm propertie s w ithout term Shift Price Price Price\u2212 \u2212 \u2212 \u2212 = \u2212\nFor example, if the average rent is $1,000, and we observe that properties with \njacuzzi have an average price of $1,300, while properties without it have an average \nprice of $995, the shift for jacuzzi is \u000b \f\u0014\u0016\u0013\u0013\u001c\u001c\u0018\u0012\u0014\u0013\u0013\u0013\u0016\u0013\u0011\u0018\bMDFX]]L6KLIW \u0010  . If there's a \nproperty with jacuzzi and has just a 5% higher than average price, I would like to \nknow about it!\nPlease note that this metric isn't trivial because term effects get aggregated. For \nexample, titles with both jacuzzi and discount will likely show a combined effect of \nthese keywords. The more data that we collect and analyze, then the more accurate \nour estimates. We will get back to this problem and how we implement a streaming \nsolution in a minute.\nScrapyd\nRight now, we will introduce scrapyd. Scrapyd is an application that allows us to \ndeploy spiders on a server and schedule crawling jobs using them. Let's get a feeling \nof how easy it is to use this. We have it preinstalled in our dev machine, so we  \ncan check this immediately by going back to the code from Chapter 3 , Basic Crawling . \nThe exact same process that we used back then works here with just a single change.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f554e09-0eb2-456d-9f3d-fe63303ab00e": {"__data__": {"id_": "3f554e09-0eb2-456d-9f3d-fe63303ab00e", "embedding": null, "metadata": {"page_label": "203", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7821250-0b01-4a2b-b2bf-6dd63571a49c", "node_type": "4", "metadata": {"page_label": "203", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a0048d22c8e1fe126814417ab4c0861c4bb8021fe36aba55d1adf0540514db53", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 203 ]Let's first have a look on scrapyd's web interface that we can find at  \nhttp://localhost:6800/ .\nScrapyd's web interface\nYou can see that it has different sections for Jobs , Items , Logs  and Documentation .  \nIt also gives us some instructions on how to schedule jobs using its API.\nIn order to do so, we must first deploy the spider to the scrapyd server. The first step \nis to modify the scrapy.cfg  configuration file as follows:\n$ pwd\n/root/book/ch03/properties\n$ cat scrapy.cfg \n...\n[settings]\ndefault = properties.settings\n[deploy]\nurl = http://localhost:6800/\nproject = properties", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c6e2764-d9a1-430b-a1ca-7d80ffa8b4c2": {"__data__": {"id_": "0c6e2764-d9a1-430b-a1ca-7d80ffa8b4c2", "embedding": null, "metadata": {"page_label": "204", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72de8e2d-7d97-49fb-b8b7-f70a531dc765", "node_type": "4", "metadata": {"page_label": "204", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a8d25575b1bad736e3729dc67c99b2973465f6e8cfeda65f9ceb929119f1f3ae", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 204 ]Essentially, all that we need to do is to uncomment the url line. The default settings \nare suitable for us. Now, in order to deploy the spider, we use the scrapyd-deploy  \ntool that is provided by scrapyd-client . scrapyd-client  that used to be part \nof Scrapy, but is now a separate module that can be installed with pip install \nscrapyd-client  (already installed in our dev):\n$ scrapyd-deploy \nPacking version 1450044699\nDeploying to project \"properties\" in http://localhost:6800/addversion.\njson\nServer response (200):\n{\"status\": \"ok\", \"project\": \"properties\", \"version\": \"1450044699\", \n\"spiders\": 3, \"node_name\": \"dev\"}\nAs the deployment was successful, we will be also able to see the project mentioned \nin the Available projects  section in the main page of the scrapyd web interface.  \nWe can now follow the instructions on the same page to submit a job:\n$ curl http://localhost:6800/schedule.json -d project=properties -d \nspider=easy\n{\"status\": \"ok\", \"jobid\": \" d4df...\", \"node_name\": \"dev\"}\nIf we turn back to the Jobs  section of the web interface, we will be able to see the \njob running. We can use the jobid  schedule.json  that returns us to cancel the job \nusing cancel.json  a bit later:\n$ curl http://localhost:6800/cancel.json -d project=properties -d \njob=d4df...\n{\"status\": \"ok\", \"prevstate\": \"running\", \"node_name\": \"dev\"}\nPlease do cancel because otherwise you will be wasting computing resources for  \na while.\nGreat! If we visit the Logs  section, we will be able to see the logs and on the Items  \nsection the Item s that we just crawled. These get cleared periodically to free up \nspace, so they might not be available after a few crawls.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d93531ab-9003-4784-84ba-6c74b997a25e": {"__data__": {"id_": "d93531ab-9003-4784-84ba-6c74b997a25e", "embedding": null, "metadata": {"page_label": "205", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c61a6dc0-556d-4c76-8a8f-55b540a044a5", "node_type": "4", "metadata": {"page_label": "205", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e7aa39a244b9c6e91cd2a5661f901920f5e14dd0ff7476f4d908d6109a8c01d8", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 205 ]If there's a good reason, such as a conflict, we can change the port using http_port , \nwhich is one of many settings that scrapyd has. It's worth being aware of them by \nhaving a look in scrapyd's documentation at http://scrapyd.readthedocs.org/ . \nOne important setting that we do change in our deployment for this chapter is max_\nproc . If you leave it with the default value of 0, scrapyd will allow as many as four \ntimes the number of CPUs that Scrapy jobs run in parallel. As we will be running \nmany scrapyd servers, most likely in a VM, we set this number to four, allowing up \nto four jobs to run in parallel. This has to do with this chapter's needs and in a real \ndeployment the default value will most likely be fine.\nOverview of our distributed system\nDesigning this system was a great experience for me. I started adding features and \ncomplexity to the point where I had to demand that readers have high-end hardware \nto run the examples. What then became an urgent necessity was simplicity\u2014both \nin order to keep the hardware requirements realistic and to ensure that this chapter \nremains focused on Scrapy.\nOverview of the system", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c68d6c15-2f0f-462b-8ae5-49292eac689f": {"__data__": {"id_": "c68d6c15-2f0f-462b-8ae5-49292eac689f", "embedding": null, "metadata": {"page_label": "206", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8b5e6cb-2084-4194-b3f9-2e0d28a2ace9", "node_type": "4", "metadata": {"page_label": "206", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "972a6c1c79f65e078e7538979c3b1138cbf513ae0f85042d10ceef549c413d49", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 206 ]At the end, the system that we are going to use in this chapter will contain our \ndev machine and a few more servers. We will use our dev machine to perform the \nhorizontal crawling of the index and extract batches of URLs. We will then distribute \nthese URL batches to scrapyd nodes in a round-robin fashion and crawl them.  \nAt the end, the .jl files with our Item s will be transferred to a server running \nApache Spark via FTP. What? FTP? Yes, I have chosen FTP and the local filesystem \nover HDFS or Apache Kafka because of its very low memory requirements and the \nfact that it's supported out-of-the-box as a FEED_URI  backend by Scrapy. Please keep \nin mind that, with just a trivial change in the configuration of scrapyd and Spark, \nwe can use Amazon S3 to store these files and enjoy redundancy, scalability, and so \non. There would be nothing interesting and on-topic to learn using any more fancy \ntechnologies, though.\nOne danger with FTP is that Spark may see incomplete files while their \nupload is in-progress. In order to avoid this, we use Pure-FTPd and a \ncallback script that moves uploaded files to /root/items  as soon as \nthe upload completes.\nEvery few seconds, Spark probes a directory ( /root/items ), reads any new \nfiles, forms a mini-batch, and performs analytics. We use Apache Spark because \nit supports Python as one of its programming languages, and it also supports \nstreaming. Up to now, we may have been using examples of relatively short-lived \ncrawls, but many of the real-world crawls don't ever finish. Crawls run indefinitely \n24/7 and provide streams of data that get analyzed, and their results just get more \naccurate with more data. That's exactly the case that we are going to showcase using \nApache Spark.\nThere's nothing special about Apache Spark and Scrapy. You are free to \nuse Map-Reduce, Apache Storm, or any other framework that fits your \nneeds.\nIn this chapter, we don't insert Items  to databases like ElasticSearch or MySQL. The \ntechniques that we presented in Chapter 9 , Pipeline Recipes , would work in exactly the \nsame way here, but their performance would be bad. Very few database systems are \nhappy when you hit them with thousands of write operations per second and that's \nwhat our pipelines would do. If we want to insert in to databases, we have to follow \na process that is similar to the one that we use for Spark, namely batch import the \ngenerated Item  files. You can modify our Spark example process to batch import to \nany database.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68523497-0887-4eee-aca8-9de6ee6bf0f9": {"__data__": {"id_": "68523497-0887-4eee-aca8-9de6ee6bf0f9", "embedding": null, "metadata": {"page_label": "207", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b972f8f1-81bc-4f21-8e89-af214d24de53", "node_type": "4", "metadata": {"page_label": "207", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "b880cb4f1e063e9015721943b0c928499cee367e2fb1cbdc5e0de277febeb3c9", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 207 ]One last thing to keep in mind is that this system is not particularly resilient. We \nassume that nodes are healthy and that any failures don't have a severe business \nimpact. Spark has resilient configurations that provide high availability. Scrapy \ndoesn't provide anything built-in apart from scrapyd's persistent queues, which \nmeans that failed jobs will restart as soon as the node is back. This may or may not \nbe suitable for your needs. If resilience is important for you, you will have to build \na monitoring and distributed queuing solution (for example, based on Kafka or \nRabbitMQ) that will restart failed crawls.\nChanges to our spider and middleware\nIn order to build this system, we need to slightly modify our Scrapy spider and \ndevelop spider middleware. More specifically we will have to perform the following:\n\u2022 Fine tune crawling the index to perform at maximum speed\n\u2022 Write a middleware that batches and sends URLs to scrapyd servers\n\u2022 Use the same middleware to allow batch URLs to be used at start-up\nWe will try to implement these changes as unobtrusively as possible. Ideally, \nthe whole operation should be clean, easy to understand, and transparent to the \nunderlying spider code. This is an infrastructure-level requirement and hacking \n(potentially hundreds) of spiders to enable it is a bad idea.\nSharded-index crawling\nOur first step is to optimize index crawling to make it as fast as possible. Before we \nstart, let's set some expectations. Let's assume that our spider will be crawling with \na concurrency of 16, and we measure the latency of the source web server and it is \nabout 0.25 sec. This gives us a maximum throughput of 16 / 0.25 = 64  pages/second. \nThe index has 50,000 detail pages / 30 details per index page = 1667 index pages.  \nWe expect the index download to take a bit more than 1667 / 64 = 26  sec.\nLet's start with the spider named easy  from Chapter 3 , Basic Crawling . We will \ncomment out the Rule  that performs the vertical crawling first (the one with \ncallback='parse_item' ) because we just want to crawl the index for now.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b66df359-dde9-4911-aabe-45a689637520": {"__data__": {"id_": "b66df359-dde9-4911-aabe-45a689637520", "embedding": null, "metadata": {"page_label": "208", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "387d9dbc-ed88-4627-8c63-ef80b0c85ca6", "node_type": "4", "metadata": {"page_label": "208", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "156da341d733cf99921a04d119c6cc4a4dfbcb71434604d8d9e0c32c1e823d98", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 208 ]You can get all the source code of this book from GitHub. To download \nthis code, go to:\ngit clone https://github.com/scalingexcellence/scrapybook\nThe full code from this chapter will be in the ch11  directory.\nIf we time a scrapy crawl  for just 10 pages before any optimizations, we get the \nfollowing:\n$ ls\nproperties  scrapy.cfg\n$ pwd\n/root/book/ch11/properties\n$ time scrapy crawl easy -s CLOSESPIDER_PAGECOUNT=10\n...\nDEBUG: Crawled (200) <GET ...index_00000.html> (referer: None)\nDEBUG: Crawled (200) <GET ...index_00001.html> (referer: ...index_00000.\nhtml)\n...\nreal  0m4.099s\nIf it takes 4 seconds for 10 pages, we have no hope of completing 1,700 pages in 26. \nBy inspecting the logs, we will realize that each page comes from the previous page's \nnext link, which means that we process at most one page at any given moment. \nEffectively our concurrency is 1. We want to parallelize and get the desired amount \nof concurrency (16 concurrent requests). We will shard the index and allow a few \nextra shards in order to be confident our crawler doesn't starve for URLs. We will \nsplit the index into 20 segments. Practically, any number above 16 will do and will \nincrease the speed, but as we get beyond 20 we see diminishing returns. We will \ncalculate the initial index IDs for each shard with the following expression:\n>>> map(lambda x: 1667 * x / 20, range(20))\n[0, 83, 166, 250, 333, 416, 500, ...  1166, 1250, 1333, 1416, 1500, 1583]\nConsequently, we set our start_urls  to the following:\nstart_urls = ['http://web:9312/properties/index_%05d.html' % id\n              for id in map(lambda x: 1667 * x / 20, range(20))]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2d474bb-9bba-421f-a30f-0767ea823b31": {"__data__": {"id_": "a2d474bb-9bba-421f-a30f-0767ea823b31", "embedding": null, "metadata": {"page_label": "209", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53e143b1-a573-4c0c-8bda-5a12b768494f", "node_type": "4", "metadata": {"page_label": "209", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "91af6bbfd314275e8d18e7325b537885c78e8dc06f049da653fb79086f492687", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 209 ]This would likely be something very different for your index, so it's not worth \nus making it any prettier at this point. If we also set our concurrency settings \n(CONCURRENT_REQUESTS , CONCURRENT_REQUESTS_PER_DOMAIN ) to 16 and we run a \ncrawl, we now get the following:\n$ time scrapy crawl easy -s CONCURRENT_REQUESTS=16 -s CONCURRENT_\nREQUESTS_PER_DOMAIN=16\n...\nreal  0m32.344s\nThis is close enough to what we wanted. We download 1667 pages / 32 sec = 52 \nindex pages per second, which means that we will generate 52 * 30 = 1560  detail page \nURLs per second. We can now uncomment the vertical crawling Rule  and save the \nfile as a new spider distr. We won't need to make any further changes to the spider's \ncode, which shows how powerful and nonintrusive the middleware that we are \nabout to develop is. If we were about to run scrapy crawl  with our dev server only, \nassuming that we can process detail pages about as fast as index pages, it would take \nus no less than 50000 / 52 = 16  minutes to complete the crawl.\nThere are two key takeaways from this section. After studying Chapter 10 , \nUnderstanding Scrapy's Performance,  we are doing actual engineering. We can \ncalculate exactly the performance that we can expect from our system and make \nsure that we don't stop unless we get it (within reason). The second important thing \nto remember is that as index crawling feeds details; crawling the total throughput \nwill be the minimum of their throughputs. If we generate URLs much faster than \nscrapyds can consume them, URLs will be piling up in their queues. On the other \nhand, if we generate URLs too slowly, scrapyds will have excess unutilized capacity.\nBatching crawl URLs\nWe are now ready to develop infrastructure that processes detailed URLs that are \naimed at vertical crawling, batches them, and dispatches them to scrapyds instead of \ncrawling them locally.\nIf we check Scrapy's architecture in Chapter 8 , Programming Scrapy , we can easily \nconclude that this is the job for a spider middleware as it implements process_\nspider_output() , which processes Request s before they reach the downloader and \nhas the power to abort them. We limit our implementation to support spiders that \nare based on CrawlSpider , and we also support only simple GET requests. If we \nneed more complexity, for example, POST or authenticated Requests, we will have \nto develop more complex functionality that propagates arguments, headers, and \npotentially relogins at every batch run.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6132f262-57ae-4b6f-bc6f-f86cc7599bec": {"__data__": {"id_": "6132f262-57ae-4b6f-bc6f-f86cc7599bec", "embedding": null, "metadata": {"page_label": "210", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ebb6fa7-18d2-45c8-9731-195261311d58", "node_type": "4", "metadata": {"page_label": "210", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "4af8e0f8d1d0ec0b35e091b3d5ba68fec75b6346e3f0d5aeeb27a00c3a3af7d1", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 210 ]In order to get started, we will have a quick look on Scrapy's GitHub. We will review \nthe SPIDER_MIDDLEWARES_BASE setting to see what reference implementations \nScrapy provides us with in order to reuse as much as we can. Scrapy 1.0 has the \nfollowing spider middleware: HttpErrorMiddleware , OffsiteMiddleware , \nRefererMiddleware , UrlLengthMiddleware , and DepthMiddleware . After a quick \nlook at their implementations, we see that OffsiteMiddleware  (just 60 lines) does \nsomething quite similar to what we want to do. It restricts the URLs to certain domains \naccording to the allowed_domains  spider attribute. Could we use a similar pattern? \nInstead of dropping URLs as OffsiteMiddleware  does, we will batch them and send \nthem to scrapyds. It turns out that we can. Here's part of the implementation:\ndef __init__(self, crawler):\n    settings = crawler.settings\n    self._target = settings.getint('DISTRIBUTED_TARGET_RULE', -1)\n    self._seen = set()\n    self._urls = []\n    self._batch_size = settings.getint('DISTRIBUTED_BATCH_SIZE', 1000)\n    ...\ndef process_spider_output(self, response, result, spider):\n    for x in result:\n        if not isinstance(x, Request):\n            yield x\n        else:\n            rule = x.meta.get('rule')\n            if rule == self._target:\n                self._add_to_batch(spider, x)\n            else:\n                yield x\ndef _add_to_batch(self, spider, request):\n    url = request.url\n    if not url in self._seen:\n        self._seen.add(url)\n        self._urls.append(url)\n        if len(self._urls) >= self._batch_size:\n            self._flush_urls(spider)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1679, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "083b6a6c-d08f-48b0-a1a2-fe471269d76d": {"__data__": {"id_": "083b6a6c-d08f-48b0-a1a2-fe471269d76d", "embedding": null, "metadata": {"page_label": "211", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9a8ade6-6a44-4351-b38e-5d7af3a0d849", "node_type": "4", "metadata": {"page_label": "211", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "e06139659923713b4972dd6fa6955564cc96969da83da99de3c69b4ea312b615", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 211 ]process_spider_output()  processes both Item  and Request . We want to work \nonly with Request ; thus, we yield  everything else. If we have a look at the source \ncode of CrawlSpider , we notice that the way it maps Request /Response  to Rule  \nis by an integer field named 'rule'  on their meta  dict . We check this number and \nif it points to the Rule  that we target (the DISTRIBUTED_TARGET_RULE  setting), we \ncall _add_to_batch()  to add its URL to the current batch. We then effectively drop \nthis Request . We yield  all other Requests , such as the ones from the next page \nlinks, without change. The _add_to_batch()  method implements a de-duplication \nmechanism. Unfortunately, the sharding process that we described in the previous \nsection means that we may extract a few URLs twice. We use _seen  set to detect \nand drop duplicates. We then add those URLs to the _urls  list, and if its size exceeds \n_batch_size  (the DISTRIBUTED_BATCH_SIZE  setting), it triggers a call to _flush_\nurls() . This method provides the following key functionality:\ndef __init__(self, crawler):\n    ...\n    self._targets = settings.get(\"DISTRIBUTED_TARGET_HOSTS\")\n    self._batch = 1\n    self._project = settings.get('BOT_NAME')\n    self._feed_uri = settings.get('DISTRIBUTED_TARGET_FEED_URL', None)\n    self._scrapyd_submits_to_wait = []\ndef _flush_urls(self, spider):\n    if not self._urls:\n        return\n    target = self._targets[(self._batch-1) % len(self._targets)]\n    data = [\n        (\"project\", self._project),\n        (\"spider\", spider.name),\n        (\"setting\", \"FEED_URI=%s\" % self._feed_uri),\n        (\"batch\", str(self._batch)),\n    ]\n    json_urls = json.dumps(self._urls)\n    data.append((\"setting\", \"DISTRIBUTED_START_URLS=%s\" % json_urls))\n    d = treq.post(\"http://%s/schedule.json\" % target,\n                  data=data, timeout=5, persistent=False)\n    self._scrapyd_submits_to_wait.append(d)\n    self._urls = []\n    self._batch += 1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0deb6b37-d365-4fdb-93c6-5fe75d044021": {"__data__": {"id_": "0deb6b37-d365-4fdb-93c6-5fe75d044021", "embedding": null, "metadata": {"page_label": "212", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c04d2823-7a3b-4c65-8ac6-b5c059b0d586", "node_type": "4", "metadata": {"page_label": "212", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "df002405daab63f402f7aa0d47bf24d409234c93fd8af1105307dcb456149b00", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 212 ]First of all, it uses a batch counter ( _batch ) to decide which scrapyd server to send \nthe batch to. We keep the available servers in _targets  (the DISTRIBUTED_TARGET_\nHOSTS  setting). We then form a POST request to scrapyd's schedule.json . This is \na bit more advanced than the one we performed with curl  before because it passes \nseveral carefully selected arguments. Effectively, based on these arguments, scrapyd \nwill schedule a run that is similar to this one:\nscrapy crawl distr \\\n-s DISTRIBUTED_START_URLS='[\".../property_000000.html\", ... ]' \\\n-s FEED_URI='ftp://anonymous@spark/%(batch)s_%(name)s_%(time)s.jl' \\\n-a batch=1\nBeyond project and spider names, we pass a FEED_URI  setting to the spider. We get \nits value from our own DISTRIBUTED_TARGET_FEED_URL  setting.\nSince Scrapy supports FTP, we can have scrapyds upload crawled Item  files through \nan anonymous FTP to our Spark server. The format contains the name of the spider \n(%(name)s ) and the time ( %(time)s ). If we were using just these, we may have ended \nup with collisions if two files were created at the same time. In order to avoid accidental \noverwrites, we also add a %(batch)s  parameter. Scrapy doesn't know anything about \nbatches by default, so we have to find a way to set this value. One interesting property \nof scrapyd's schedule.json  API is that every argument that isn't a setting or one of \nthe few known arguments is passed to spiders as an argument. Spider arguments, by \ndefault, become spider attributes and, interestingly, unknown FEED_URI  arguments are \nlooked up on spider's attributes. As a result, by passing a batch  argument to schedule.\njson , we can use it in FEED_URI  and avoid collisions.\nThe last step is to compile a DISTRIBUTED_START_URLS  setting with all the detail \nURLs of this batch encoded as JSON. There's no particular reason to use this format \nother than familiarity and simplicity. Any textual format will do.\nPassing lots of data to Scrapy via the command line is, at the very \nleast, not elegant. At some point you want to store arguments \nin a data store (for example, Redis) and just pass Scrapy an ID. \nDoing so would require small changes in _flush_urls() and \nprocess_start_requests() .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6cac2cc-b0c5-4f03-8137-bd876edbb7e2": {"__data__": {"id_": "f6cac2cc-b0c5-4f03-8137-bd876edbb7e2", "embedding": null, "metadata": {"page_label": "213", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19e899f6-58ed-4912-b450-067a8dc6ec76", "node_type": "4", "metadata": {"page_label": "213", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a283cc9fb847bbe3d3f7d624957dc0d5e96d087b044d08ae0b715d42c9f3ffd3", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 213 ]We perform the POST request with treq.post() . Scrapyd doesn't handle persistent \nconnections very well; thus, we disable them with persistent=False . We also set a  \n5 second timeout\u2014just to be on the safe side. Interestingly, we store the deferred for \nthis request to a _scrapyd_submits_to_wait  list that we will talk about it in a second. \nTo close this function, we reset the _urls  list and increase the current _batch .\nSurprisingly, we will find lots of functionality on the close operation handler as \nfollows:\ndef __init__(self, crawler):\n    ...\n    crawler.signals.connect(self._closed, signal=signals.spider_\nclosed)\n@defer.inlineCallbacks\ndef _closed(self, spider, reason, signal, sender):\n    # Submit any remaining URLs\n    self._flush_urls(spider)\n    yield defer.DeferredList(self._scrapyd_submits_to_wait)\n_closed()  is called either because we pressed Ctrl + C or because the crawl \ncompleted. In both cases, we don't want to lose any URLs that belong to the last \nbatch, which haven't yet been sent. That's why the first thing we do in _closed()  \nis to call _flush_urls(spider)  to flush the last batch. The second problem is that \nbeing nonblocking, any of the treq.post()  might or might not have completed \nby the time we stop crawling. In order to avoid losing any batches, we use the _\nscrapyd_submits_to_wait  list that was mentioned earlier, which contains all of the \ntreq.post()  deferreds . We use defer.DeferredList()  to wait until all of them \ncomplete. Since _closed()  uses @defer.inlineCallbacks , we just yield  it and \nresume when all requests complete.\nSummarizing, jobs with batches of URLs in the DISTRIBUTED_START_URLS  setting \nare sent to scrapyds, which run the same spider. Obviously, we need somehow to \nuse this setting to initialize start_urls .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d34280c3-02fc-46d0-9c2c-11a8f3309459": {"__data__": {"id_": "d34280c3-02fc-46d0-9c2c-11a8f3309459", "embedding": null, "metadata": {"page_label": "214", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "990864f0-e4a3-491a-80d8-ef1f888d2243", "node_type": "4", "metadata": {"page_label": "214", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "77a58a5ecefdcaf47e843eecba1593eb3f826611fa1b1bbb02e9ff51885ffe60", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 214 ]Getting start URLs from settings\nYou can feel how well tailored to our needs spider middleware is when you \nnotice that it provides a process_start_requests()  method, which can be used \nto process the start_requests  that spiders provide us. We detect whether the \nDISTRIBUTED_START_URLS  setting is set, and if so, we JSON to decode it and use \nits URLs to yield  relevant Request . For these requests, we set the _response_\ndownloaded()  method of CrawlSpider  as callback, and we set the meta['rule']  \nparameter in order to have their Response  processed by the appropriate Rule . \nFrankly, we look at Scrapy's source code, find the way that CrawlSpider  creates \ntheir Request  and do exactly the same. In this case it is:\ndef __init__(self, crawler):\n    ...\n    self._start_urls = settings.get('DISTRIBUTED_START_URLS', None)\n    self.is_worker = self._start_urls is not None\ndef process_start_requests(self, start_requests, spider):\n    if not self.is_worker:\n        for x in start_requests:\n            yield x\n    else:\n        for url in json.loads(self._start_urls):\n            yield Request(url, spider._response_downloaded,\n                          meta={'rule': self._target})\nOur middleware is ready. We enable it and set its settings in our settings.py :\nSPIDER_MIDDLEWARES = {\n    'properties.middlewares.Distributed': 100,\n}\nDISTRIBUTED_TARGET_RULE = 1\nDISTRIBUTED_BATCH_SIZE = 2000\nDISTRIBUTED_TARGET_FEED_URL = (\"ftp://anonymous@spark/\"\n                               \"%(batch)s_%(name)s_%(time)s.jl\")\nDISTRIBUTED_TARGET_HOSTS = [\n    \"scrapyd1:6800\",\n    \"scrapyd2:6800\",\n    \"scrapyd3:6800\",\n]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c87c573f-5609-4301-9efe-b9ff27db84d7": {"__data__": {"id_": "c87c573f-5609-4301-9efe-b9ff27db84d7", "embedding": null, "metadata": {"page_label": "215", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7a5fa17-2272-4e4f-b9ff-42eb33351f00", "node_type": "4", "metadata": {"page_label": "215", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a5f09bf61a22a728adc9391603c6ad1237d3efb102cdf22ba0d6545723fb322c", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 215 ]Someone may reasonably argue that DISTRIBUTED_TARGET_RULE  shouldn't be a \nsetting as it may differ from one spider to another. You can consider it as a default \nvalue that you can override on your spiders using a custom_settings  attribute,  \nfor example:\ncustom_settings = {\n    'DISTRIBUTED_TARGET_RULE': 3\n}\nWe don't need this in our case though. We can perform a test run that will crawl a \nsingle page that is provided as a setting:\n$ scrapy crawl distr -s \\\nDISTRIBUTED_START_URLS='[\"http://web:9312/properties/property_000000.html\"]'\nAfter this succeeds, we can try a more ambitious one, which crawls a page and FTPs \nit to our Spark server:\nscrapy crawl distr -s \\\nDISTRIBUTED_START_URLS='[\"http://web:9312/properties/property_000000.html\"]' \\\n-s FEED_URI='ftp://anonymous@spark/%(batch)s_%(name)s_%(time)s.jl' -a batch=12\nIf you ssh the Spark server (more on this in a bit), you should be able to see a file, \nsuch as 12_distr_date_time.jl , in the /root/items  directory.\nThis is a sample implementation of middleware that allows you to implement \ndistributed crawling using scrapyd. You can use it as a starting point to implement \none that fits your specific needs. The things you may want to adapt are as follows:\n\u2022 The type of spiders that you support. An alternative solution that doesn't \nlimit itself to CrawlSpider , may, for example, require your spiders to mark \ndistributed requests with an appropriate meta  and employ callback naming \nconventions.\n\u2022 The way that you pass URLs to scrapyds. You may want to use domain-\nspecific knowledge to reduce the amount of information that is passed. For \nexample, in our case, we could pass just properties' IDs.\n\u2022 You can use a more elegant solution with a distributed queue to make the \ncrawler able to recover from failures and allow scrapyds to commit further \nURLs to batches.\n\u2022 You can populate the list of target servers dynamically to support on-\ndemand scaling.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc4f1bb8-5f66-492d-8840-0903fe9f5bcd": {"__data__": {"id_": "fc4f1bb8-5f66-492d-8840-0903fe9f5bcd", "embedding": null, "metadata": {"page_label": "216", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bee99d0-8710-4e05-b350-1a7b57085706", "node_type": "4", "metadata": {"page_label": "216", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "aefa3a87e92f7fc26238cb7d9c1f09e57a754b15e779450ae3c67ef88e32354f", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 216 ]Deploy your project to scrapyd servers\nIn order to be able to deploy the spiders to our three scrapyd servers, we have to \nadd them to our scrapy.cfg  file. Each [deploy:target-name]  section on this file \ndefines a new deployment target:\n$ pwd\n/root/book/ch11/properties\n$ cat scrapy.cfg\n...\n[deploy:scrapyd1]\nurl = http://scrapyd1:6800/\n[deploy:scrapyd2]\nurl = http://scrapyd2:6800/\n[deploy:scrapyd3]\nurl = http://scrapyd3:6800/\nYou can query the available targets with scrapyd-deploy -l :\n$ scrapyd-deploy -l\nscrapyd1             http://scrapyd1:6800/\nscrapyd2             http://scrapyd2:6800/\nscrapyd3             http://scrapyd3:6800/\nIt's easy to deploy to any of them with scrapyd-deploy <target name> :\n$ scrapyd-deploy scrapyd1\nPacking version 1449991257\nDeploying to project \"properties\" in http://scrapyd1:6800/addversion.json\nServer response (200):\n{\"status\": \"ok\", \"project\": \"properties\", \"version\": \"1449991257\", \n\"spiders\": 2, \"node_name\": \"scrapyd1\"}\nThis process leaves us with a few extra directories and files ( build , project.egg-\ninfo , setup.py ) that we can safely delete. Essentially what scrapyd-deploy  does is \nto pack your projects and upload them to the target scrapyd using addversion.json .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aca4ee3b-e73c-48ef-9820-c69dfa3f7e37": {"__data__": {"id_": "aca4ee3b-e73c-48ef-9820-c69dfa3f7e37", "embedding": null, "metadata": {"page_label": "217", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9a340b0-98de-4ddb-912b-980b3e4c3e0b", "node_type": "4", "metadata": {"page_label": "217", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "fa49c40408a158d7d3f1b10e38a0ca590976e6e24986fee694add3339b6102e1", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 217 ]After this, if we query each of those servers using scrapyd-deploy -L , we can \nconfirm that the project has been successfully deployed, as follows:\n$ scrapyd-deploy -L scrapyd1\nproperties\nI also use touch  to create three empty files, scrapyd1-3 , in the project's directory. \nThis way scrapyd*  expands to the names of the files, which are also the names of \nthe target servers. You can then deploy to all servers with a bash loop: for i in \nscrapyd*; do scrapyd-deploy $i; done .\nCreating our custom monitoring \ncommand\nIf you want to monitor the progress of your crawl across many scrapyd servers, you \nhave to do it manually. This is a nice opportunity for us to exercise everything we've \nseen up to now to create a primitive Scrapy command, scrapy monitor , which \nmonitors a set of scrapyd servers. We will name the file: monitor.py , and we add \nCOMMANDS_MODULE = 'properties.monitor'  to our settings.py . With a quick \nlook at scrapyd's documentation, the listjobs.json  API gives us information on \njobs. If we want to find the base URL for a given target, we may correctly guess \nthat it must be somewhere in the code of scrapyd-deploy  so that we can find it in \na single file. If we take a look at https://github.com/scrapy/scrapyd-client/\nblob/master/scrapyd-client/scrapyd-deploy , we will quickly notice a _get_\ntargets()  function (its implementation doesn't add a lot of value, so I omit it) that \ngives us target names and their base URLs. Awesome! We are ready to implement \nthe first part of this command as follows:\nclass Command(ScrapyCommand):\n    requires_project = True\n    def run(self, args, opts):\n        self._to_monitor = {}\n        for name, target in self._get_targets().iteritems():\n            if name in args:\n               project = self.settings.get('BOT_NAME')\n                url = target['url'] + \"listjobs.json?project=\" + project\n               self._to_monitor[name] = url\n        l = task.LoopingCall(self._monitor)\n        l.start(5)  # call every 5 seconds\n        reactor.run()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c55e5a51-0bd0-4cd9-bd65-1b8f79485b83": {"__data__": {"id_": "c55e5a51-0bd0-4cd9-bd65-1b8f79485b83", "embedding": null, "metadata": {"page_label": "218", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "156107b4-353f-4f28-918a-3c38c94f1ef9", "node_type": "4", "metadata": {"page_label": "218", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "f235ab0fcf6c8e85a282819a4f9196e3d13c556d2538cc5a793ba2622075e598", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 218 ]Given what we've seen up to now, this is fairly easy. It populates a dict  _to_\nmonitor  with the names and the API endpoints that we want monitor. We then \nuse task.LoopingCall()  to schedule recurring calls to our _monitor()  method. \n_monitor()  uses treq  and deferred , and we use @defer.inlineCallbacks  to \nsimplify its implementation. Here it is (omitting some error handling and cosmetics):\n@defer.inlineCallbacks\ndef _monitor(self):\n    all_deferreds = []\n    for name, url in self._to_monitor.iteritems():\n        d = treq.get(url, timeout=5, persistent=False)\n        d.addBoth(lambda resp, name: (name, resp), name)\n        all_deferreds.append(d)\n    all_resp = yield defer.DeferredList(all_deferreds)\n    for (success, (name, resp)) in all_resp:\n        json_resp = yield resp.json()\n        print \"%-20s running: %d, finished: %d, pending: %d\" %\n              (name,  len(json_resp['running']),\n              len(json_resp['finished']), len(json_resp['pending']))\nThese few lines contain almost every Twisted technique that we know. We use treq  \nto call scrapyd's API and defer.DeferredList  to process all the responses at once. \nOnce we have all the results in all_resp , we iterate and retrieve their JSON objects. \ntreq  Response ' json()  method returns deferred  instead of actual values that we \nyield  to resume with actual values at some point in the future. As a final step, we \nprint the results. The JSON response contains lists with information on pending, \nrunning, and finished jobs, and we print their length.\nCalculating the shift with Apache Spark \nstreaming\nOur Scrapy system is fully functional at this point. Let's take a quick look at Apache \nSpark's functionality.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e29ca47f-1b41-4bc3-b494-ecc9c043f8c8": {"__data__": {"id_": "e29ca47f-1b41-4bc3-b494-ecc9c043f8c8", "embedding": null, "metadata": {"page_label": "219", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0c0822f-7956-4b29-83a5-dbbd48fa902f", "node_type": "4", "metadata": {"page_label": "219", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "eccc2f89018d0a03a77b5a98df60fc51026d0b198b0cf46a4fb9be138a69d227", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 219 ]The formula WHUP6KLIW  that we presented at the beginning of this chapter is nice and \nsimple, but it can't be implemented efficiently. We can easily calculate 3ULFHwith two \ncounters and ZLWK3ULFH with \u0015WHUPVQ\u0098  counters, and each new price would have to update \njust four of them. Calculating ZLWKRXW3ULFH  though is very problematic because for every \nnew price \u000b \f\u0015 \u0014WHUPVQ\u0098 \u0010  counters would have to be updated. For example, we will have \nto add a jacuzzi price to every ZLWKRXW3ULFH counter but the jacuzzi one. This makes the \nalgorithm infeasible for a large number of terms.\nTo work around this problem, all we have to notice is that if we add the price of \nproperties with a term and the price of properties without that same term, we \nget the price of all the properties (obviously!) _ _ZLWK ZLWKRXW 3ULFH 3ULFH 3ULFH  \u000e \u00a6 \u00a6 \u00a6 . The \naverage price of properties without a term can, thus, be calculated using inexpensive \noperations as follows:\n_ _ZLWKRXW ZLWK\nZLWKRXW\nZLWKRXW ZLWK3ULFH 3 ULFH3ULFH3ULFHQ QQ\u0010  \u0010\u00a6 \u00a6 \u00a6\nUsing this form, the shift formula becomes the following:\n| |with with\nterm\nwith withPrice P rice Price P riceShiftn nn n\uf8eb \uf8f6 \u2212= \u2212 \uf8ec \uf8f7\u2212\uf8ed \uf8f8\u2211 \u2211 \u2211 \u2211\nLet's see how we implement this. Please keep in mind that this isn't Scrapy code, so \nit is very reasonable for it to feel unfamiliar, but you will still most likely be able to \nread and understand it with little effort. You can find the application in boostwords.\npy. Please note that it also contains lots of complex test code that you can safely \nignore. Its core functionality is as follows:\n# Monitor the files and give us a DStream of term-price pairs\nraw_data = raw_data = ssc.textFileStream(args[1])\nword_prices = preprocess(raw_data)\n# Update the counters using Spark's updateStateByKey\nrunning_word_prices = word_prices.updateStateByKey(update_state_\nfunction)\n# Calculate shifts out of the counters\nshifts = running_word_prices.transform(to_shifts)\n# Print the results\nshifts.foreachRDD(print_shifts)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b17a7dfc-24b0-48d5-802b-49a2769446a9": {"__data__": {"id_": "b17a7dfc-24b0-48d5-802b-49a2769446a9", "embedding": null, "metadata": {"page_label": "220", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d655ab0b-800d-40ff-9906-a46518fc0a1a", "node_type": "4", "metadata": {"page_label": "220", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "cdbf87f2973bb0ed7665ca1f22cab3ce6afd6381a6c11a593d17b74140f30af0", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 220 ]Spark uses something called DStream  to represent streams of data. The \ntextFileStream()  method monitors a directory in our filesystem, and when it \ndetects new files it streams data out of them. Our preprocess()  function converts \nthem to streams of term/price pairs. We aggregate these pairs on running counters \nwith Spark's updateStateByKey()  method using our update_state_function()  \nfunction. We finally calculate shifts by running to_shifts()  and print the best \nusing our print_shifts()  function. Most of our functions are trivial and they just \nshape data in an efficient-for-Spark way. The most interesting exception is our to_\nshifts()  function:\ndef to_shifts(word_prices):\n    (sum0, cnt0) = word_prices.values().reduce(add_tuples)\n    avg0 = sum0 / cnt0\n    def calculate_shift((isum, icnt)):\n        avg_with = isum / icnt\n        avg_without = (sum0 - isum) / (cnt0 - icnt)\n        return (avg_with - avg_without) / avg0\n    return word_prices.mapValues(calculate_shift)\nIt's really impressive that it follows the formulas so closely. Despite its simplicity, \nSpark's mapValues()  makes calculate_shift  run efficiently across our (potentially \nmany) Spark servers with minimum network overhead.\nRunning a distributed crawl\nI, typically, use four terminals to have a complete view of the progress of our \ncrawl. I want to make this section self-contained, so I also provide the vagrant ssh  \ncommands that you need to open terminals to the relevant servers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9421257-8b15-4f71-bbe7-03f0cba9668c": {"__data__": {"id_": "f9421257-8b15-4f71-bbe7-03f0cba9668c", "embedding": null, "metadata": {"page_label": "221", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd1a7fd4-54d6-49af-851d-bbfa6d40550a", "node_type": "4", "metadata": {"page_label": "221", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c048567bdae7d59aa2f18a36c930c67458015ff0da02dee56768be4baa531678", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 221 ]\nUsing four terminals to oversee a crawl\nWith one terminal, 1, I like to monitor the CPU and memory usage across the servers. \nThis helps with identifying and repairing potential problems. To set it up, I run  \nthe following:\n$ alias provider_id=\"vagrant global-status --prune | grep 'docker-\nprovider' | awk '{print \\$1}'\"\n$ vagrant ssh $(provider_id)\n$ docker ps --format \"{{.Names}}\" | xargs docker stats\nThe first two somewhat complex lines allow us to ssh the docker provider VM. If we \naren't using a VM but we run on a docker-powered Linux machine, we need just the \nlast line.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8eeb7705-a472-4312-9022-1ac3f11d3165": {"__data__": {"id_": "8eeb7705-a472-4312-9022-1ac3f11d3165", "embedding": null, "metadata": {"page_label": "222", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94b9351a-4afa-4bd5-adf6-1cf345338c7f", "node_type": "4", "metadata": {"page_label": "222", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "cbc3736e17bc43076f2b9a5eddb7e47916ec7ebabe2c7a4f3ece48a11aa66593", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 222 ]The second terminal is also diagnostic 2, and I use it to run scrapy monitor  as follows:\n$ vagrant ssh\n$ cd book/ch11/properties\n$ scrapy monitor scrapyd*\nPlease keep in mind that using scrapyd*  and the empty files with the server  \nnames, scrapy monitor scrapyd*  expands to scrapy monitor scrapyd1 \nscrapyd2 scrapyd3 .\nThe third one, 3, is a terminal to our dev machine from which we kick-off the crawl. \nApart from this, it's mostly idle. To start a new crawl, we perform the following:\n$ vagrant ssh\n$ cd book/ch11/properties\n$ for i in scrapyd*; do scrapyd-deploy $i; done\n$ scrapy crawl distr\nThe last two lines are the essential ones. First, we use a for loop and scrapyd-\ndeploy  to deploy the spider to our servers. Then, we start a crawl with scrapy \ncrawl distr . We can always run smaller crawls using, for example, scrapy crawl \ndistr -s CLOSESPIDER_PAGECOUNT=100  to crawl about 100 index pages, which \ncorresponds to about 3,000 detail pages.\nOur last terminal, 4, connects with the Spark server, and we use it to run the \nstreaming analytics job:\n$ vagrant ssh spark\n$ pwd\n/root\n$ ls\nbook items\n$ spark-submit book/ch11/boostwords.py items\nOnly the last line is essential, and it runs boostwords.py,  giving it our local items  \ndirectory to monitor. Sometimes, I also use watch ls -1 items  to keep an eye on \nthe item files as they arrive.\nWhich exactly are the keywords that most affect prices? I leave this as a surprise for \nthose who managed to follow this far.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b5dbb7f-c518-4603-8b8b-537c65ff3972": {"__data__": {"id_": "7b5dbb7f-c518-4603-8b8b-537c65ff3972", "embedding": null, "metadata": {"page_label": "223", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "178aaf7f-bf08-461a-a57f-c12c4f9ce958", "node_type": "4", "metadata": {"page_label": "223", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "28036a72e5dea4671f8bacd8a786f41d448a50b50037bafb2bab80535b3774b0", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 11[ 223 ]System performance\nIn terms of performance, our results greatly vary depending on our hardware, and \nthe number of CPUs and memory that we give to our VM. In a real deployment, we \nget horizontal scalability allowing us to crawl as fast as our servers allow.\nThe theoretical maximum that one could get with the given settings is 3 servers \u2219 4 \nprocesses/server \u2219 16 requests in parallel \u2219 4 pages/second (as defined by the page \ndownload latencies) = 768 pages/second.\nIn practice, using a Macbook Pro with 4 GB of RAM and 8 cores allocated to a \nVirtualBox VM, I got 50000 URLs in 2:40, which means about 315 pages/second.  \nOn an Amazon EC2 m4.large instance with 2 vCPUs and 8 GB RAM, it took \n6:12 giving 134 pages/second due to limited CPU capacity. On an Amazon EC2 \nm4.4xlarge instance with 16 vCPUs and 64 GB RAM, the crawl completed in 1:44 \ngiving 480 pages/second. On the same machine, I doubled the number of scrapyd \ninstances to 6 (by slightly editing Vagrantfile , scrapy.cfg  and settings.py ) and \nthe crawl completed in 1:15 with a speed of 667 pages/second. In this latter case, our \nweb server seemed to be the bottleneck (this would mean an outage in real life).\nThe distance between the performance that we get and the theoretical maximum \nis more than justified. There are many little latencies that our back-of-the-envelope \ncalculations don't take into account. Despite the fact that we claim a 250 ms page \nload latency, we've already seen in previous chapters that it's larger because, at the \nvery least, we have additional Twisted and OS latencies. Then there are latencies, \nsuch as the transfer time of our URLs from dev to scrapyds, our crawled Item s \nto Spark through FTP, and the time (2.5 seconds on average\u2014refer to scrapyd's \npoll_interval  setting) that it takes scrapyd to discover and schedule jobs. There's \nalso a start time for both the dev and scrapyd crawls that we don't account for. \nI wouldn't try to improve any of these latencies unless I was certain they would \nincrease throughput. My next step would be to increase the size of the crawl to, for \nexample, 500k pages, load balance a few web server instances, and discover the next \ninteresting challenges in our scaling endeavor.\nThe key take-away\nThe most important takeaway of this chapter is that if you are about to perform \ndistributed crawling, always use suitably sized batches.\nDepending on how fast your source websites respond, you may have hundreds, \nthousands, or tens of thousands of URLs. You would like them to be large enough\u2014in \nthe few-minutes level\u2014so that any startup costs are amortized sufficiently. On the \nother hand, you wouldn't like them to be too large as this would turn a machine failure \nto a major risk. In a fault-tolerant distributed system, you would retry failed batches; \nand you wouldn't want this to be hours worth of work.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abe32742-e3bc-4286-ab77-ea0bbed371fe": {"__data__": {"id_": "abe32742-e3bc-4286-ab77-ea0bbed371fe", "embedding": null, "metadata": {"page_label": "224", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80a5ac38-0c86-4869-b25a-e8d6092cbb24", "node_type": "4", "metadata": {"page_label": "224", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "526cf748db65f2ad5618fd429341c5543d21c766085d6cd2f858c1238aa8e671", "class_name": "RelatedNodeInfo"}}, "text": "Distributed Crawling with Scrapyd and Real-Time Analytics[ 224 ]Summary\nI hope you have enjoyed this book on Scrapy as much as I did writing it. You now \nhave a very broad view of Scrapy's capabilities, and you are able to use it for simple \nand complex crawling scenarios. You have also gained an appreciation of  \nthe complexities of developing using such a high-performance system and making \nthe most out of it. By using crawling you can leverage immediate network access to \nlarge real-world datasets on your applications. We have seen the ways to use Scrapy \ndatasets to build mobile apps and perform interesting analytics. I hope that you \nuse Scrapy's power to develop great, innovative applications and make our world a \nbetter place. Good luck!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e89e8891-2c04-4b4d-853d-4f2f4afa0282": {"__data__": {"id_": "e89e8891-2c04-4b4d-853d-4f2f4afa0282", "embedding": null, "metadata": {"page_label": "225", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e961a04-5386-45cd-81a9-071811a90251", "node_type": "4", "metadata": {"page_label": "225", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "0ace241b53228d4b66359318c6cfc09cefb73d97014e4c456f20b7138f313786", "class_name": "RelatedNodeInfo"}}, "text": "[ 225 ]Installing and troubleshooting \nprerequisite software\nInstalling prerequisites\nThis book uses a rich system of virtual servers to demonstrate the uses of Scrapy in \na realistic multiserver deployment. We use industry standard tools\u2014Vagrant and \nDocker\u2014to set this system up. This is a book with strong dependencies on website \ncontent and layout, and if we were using websites outside our control, our examples \nwould break in a few months time. Vagrant and Docker allow us to provide an \nisolated environment where examples run seamlessly now and in the future. As a \nside benefit, we don't hit any remote servers; thus, we can't cause any inconvenience \nto any webmaster. Even if we break something and examples stop working, by using \ntwo commands, vagrant destroy  and vagrant up --no-parallel , we can tear \ndown, rebuild the system, and continue.\nJust before we start, I would like to clarify that this infrastructure is tailored to the \nneeds of a book reader. Especially with regard to Docker, the general consensus is \nthat every Docker container should run a single-process microservice. We don't do \nthat. Many of our Docker containers are a bit heavy and allow us to connect to them \nand perform various operations via vagrant ssh . Our dev machine in particular \nlooks nothing like a microservice. It's our user friendly gateway  to this isolated \nsystem, and we treat it as a fully-featured Linux machine. Unless we bent the \nrules in this way, we would have to use a larger repertoire of Vagrant and Docker \ncommands, delve deeper into troubleshooting them, and this book would quickly \nturn into a Vagrant/Docker book. I hope Docker aficionados will pardon us, and \nevery reader appreciates the ease and benefits that Vagrant and Docker bring.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e02884a7-ef3a-4e64-b487-e10ae0cc87d2": {"__data__": {"id_": "e02884a7-ef3a-4e64-b487-e10ae0cc87d2", "embedding": null, "metadata": {"page_label": "226", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da4c5e64-8583-4745-b487-525f62c94d4a", "node_type": "4", "metadata": {"page_label": "226", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "0e3aaf8192ad04bb7843ae9010c9bac25f87195086bee2bd8794692df0856574", "class_name": "RelatedNodeInfo"}}, "text": "Installing and troubleshooting prerequisite software[ 226 ]The containers for this book aren't by any means suitable \nfor production.\nIt's impossible to test every software/hardware configuration out there. If something \ndoesn't work, please, if possible, fix it and send us a Pull Request on GitHub. If you don't \nknow how to fix it, search for a relevant issue on GitHub or open one if it doesn't exist.\nThe system\nThis is a reference section. Feel free to skip it at first read and return to it when you \nwant to better understand the way that this book's system is structured. We repeat \nparts of this information in relevant chapters.\nWe use Vagrant to set up the following system:\nThe system we use in this book", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d20cc65f-4a99-4f59-a936-4a56d2f5192b": {"__data__": {"id_": "d20cc65f-4a99-4f59-a936-4a56d2f5192b", "embedding": null, "metadata": {"page_label": "227", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f17a5f6e-aba6-43dd-b16c-ce2f8f0ccda5", "node_type": "4", "metadata": {"page_label": "227", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "27402312a928621c660976438ff31ae668d16c871f6b7c82d3c72ea8476fa9f0", "class_name": "RelatedNodeInfo"}}, "text": "Appendix[ 227 ]In the diagram, each box represents a server and the hostname is the first part of its \ntitle (dev, Web, es, and so on.). The second part of the title is the Docker image that it \nuses (scrapybook/dev , scrapybook/web , scrapybook/es , and so on.). Then there's a \nshort description of the software that runs on the server. Lines represent links between \nservers, and their protocols are written next to them. Part of the isolation that Docker \nprovides is the fact that connections beyond the ones that are explicitly declared aren't \nallowed. This means that if, for example, you want to run something that listens in the \n1234  port on the spark server, nobody will be able to connect to it unless you expose \nthis port by adding relevant declarations to the Vagrant file. Please keep this in mind in \ncase you want to install custom software on any of those servers.\nIn most chapters, we use just two machines: dev and web. vagrant ssh  connects us \nto the dev machine. From here, we can easily access every other machine using its \nhostname ( mysql , web, and so on). For example, we can confirm that we can access \nthe web machine by performing ping web . We use and explain various commands \nin each chapter. In Chapter 9 , Pipeline Recipes , we demonstrate how to push data to \nvarious databases. In Chapter 11 , Distributed Crawling with Scrapyd and Real-Time \nAnalytics , we use three scrapyd Docker containers (which are in fact identical to our \ndev machine to reduce download size) with the scrapyd1-3  hostnames. We also  \nuse a server with the spark  hostname, which runs Apache Spark and an FTP service. \nWe can connect to it with vagrant ssh spark  and run Spark jobs.\nWe can find the description of this system in the Vagrantfile  in the top-level \ndirectory on GitHub. As soon as we type vagrant up --no-parallel , the system \nwill start building. This takes a few minutes, especially the first time, as we will see \nin more detail later in the FAQ. One can find this book's code mounted in the ~/book  \ndirectory. Any time someone modifies something in it on the host machine, changes \npropagate automatically. This allows us to hack files with our text editor or IDE and \nquickly check our changes on dev.\nFinally, some listening ports are forwarded to our host computer and expose the \nrelevant services. For example, you can use a simple web browser to access them. If \nyou already use any of these ports on your computer, there will be a conflict and the \nsystem won't build successfully. We will show you how to fix these cases later in the \nFAQ. These are the forwarded ports:\nMachine and service From dev From your (host) computer\nWeb\u2014web Server http://web:9312 http://localhost:9312\ndev\u2014scrapyd http://dev:6800 http://localhost:6800\nscrapyd1\u2014scrapyd http://\nscrapyd1:6800http://localhost:6801\nscrapyd2\u2014scrapyd http://\nscrapyd2:6800http://localhost:6802", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc0a468e-9a50-4ff1-a0e4-44cd6ca95946": {"__data__": {"id_": "dc0a468e-9a50-4ff1-a0e4-44cd6ca95946", "embedding": null, "metadata": {"page_label": "228", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "227d6bfd-12ef-4b74-976a-7e9a1172a4c0", "node_type": "4", "metadata": {"page_label": "228", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "2fe205e4178cb6c320375352e91406b6c54b50cc1e527eae2762dbdf166ec8fc", "class_name": "RelatedNodeInfo"}}, "text": "Installing and troubleshooting prerequisite software[ 228 ]Machine and service From dev From your (host) computer\nscrapyd3\u2014scrapyd http://\nscrapyd3:6800http://localhost:6803\nes\u2014Elasticsearch API http://es:9200 http://localhost:9200\nspark\u2014FTP ftp://spark:21 & \n30000-9ftp://localhost:21 & \n30000-9\nRedis\u2014Redis API redis://redis:6379 redis://localhost:6379\nMySQL\u2014MySQL \ndatabasemysql://mysql:3306 mysql://localhost:3306\nThe ssh is also exposed on some machines and Vagrant takes care of redirecting and \nforwarding ports for us to avoid conflicts. All we have to do is run vagrant ssh \n<hostname>  to the machine that we want.\nInstallation in a nutshell\nThe necessary software that we need to install is as follows:\n\u2022 Vagrant\n\u2022 git\n\u2022 VirtualBox (on Windows and Mac) or Docker (on Linux)\nOn Windows, we also need to enable the git ssh  client. You can visit their websites \nand follow the steps that they describe for your platform. In the following sections, \nwe are going to try to provide step-by-step instructions, which are valid right now. \nThey will certainly become obsolete in the future, so always keep an eye on the \nofficial documentation.\nInstalling on Linux\nWe start with instructions about how to install the system on Linux because it's the \neasiest. I will demonstrate with Ubuntu 14.04.3 LTS (Trusty Tahr), but the process \nshould be very similar for any distribution, and well, the more unusual the distribution, \nthe more\u2014I guess\u2014you know how to fill in the gaps. In order to install Vagrant, go to \nVagrant's website, https://www.vagrantup.com/ , and browse to its download page. \nRight-click on the Debian package, 64-bit version . Copy the link address:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1673, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "729f5c96-5038-41f1-8c24-27e01b876eb5": {"__data__": {"id_": "729f5c96-5038-41f1-8c24-27e01b876eb5", "embedding": null, "metadata": {"page_label": "229", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "baa93e5e-7e1b-4cfe-b6ec-84a834f32975", "node_type": "4", "metadata": {"page_label": "229", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "65ab312faea935c83eb6f0a4e22bcab4fe3933e591b66141bf2324551aa6f591", "class_name": "RelatedNodeInfo"}}, "text": "Appendix[ 229 ]\nWe will use the terminal to install Vagrant because it's the most universal way, \ndespite the fact that we could achieve the same with a few clicks on Ubuntu. To open \nthe terminal, we click the Ubuntu icon in the top-left corner of the screen to open the \nDash. Alternatively , we could press the Windows  button. Then we type terminal  \nand click on the Terminal  icon to open it.\nWe type wget  and paste the link from Vagrant's page. A .deb  file should download \nafter a few seconds. Type sudo dpkg -i <name of the .deb file you just \ndownloaded>  to install the file. That's it, Vagrant just installed successfully.\nTo install git just type the following two lines on the terminal:\n$ sudo apt-get update\n$ sudo apt-get install git\nNow, let's install Docker. We follow the instructions from https://docs.docker.\ncom/engine/installation/ubuntulinux/ . On the terminal we type the following:\n$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 \n--recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n$ echo \"deb https://apt.dockerproject.org/repo ubuntu-trusty main\" | sudo \ntee /etc/apt/sources.list.d/docker.list\n$ sudo apt-get update\n$ sudo apt-get install docker-engine\n$ sudo usermod -aG docker $(whoami)\nWe log out and log in again to apply the group changes, and now, we should be \nable to use docker ps  without a problem. We should now be able to download this \nbook's code and enjoy the book:\n$ git clone https://github.com/scalingexcellence/scrapybook.git\n$ cd scrapybook\n$ vagrant up --no-parallel", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9acb9d48-f704-477a-8820-8620bee4aca0": {"__data__": {"id_": "9acb9d48-f704-477a-8820-8620bee4aca0", "embedding": null, "metadata": {"page_label": "230", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4c97274-2275-4e9b-bc89-c25afe47e7bf", "node_type": "4", "metadata": {"page_label": "230", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "78a54c0a96ea72b9afb24e05fb00261b4f7414a1a7febe859feabb4b24ec1e45", "class_name": "RelatedNodeInfo"}}, "text": "Installing and troubleshooting prerequisite software[ 230 ]Installing on Windows or Mac\nThe process for the Windows and Mac environments is similar, so we will present \nthem together while highlighting their differences.\nInstall Vagrant\nTo install Vagrant, we go to Vagrant's website, https://www.vagrantup.com/ , and \nbrowse to its download page. We choose our operating system and go through the \ninstallation wizard:\nA few clicks later, we should have Vagrant installed. In order to access it, we will \nhave to open the command line or the terminal.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc690db6-089f-44ef-98d3-afbbc92ef03d": {"__data__": {"id_": "bc690db6-089f-44ef-98d3-afbbc92ef03d", "embedding": null, "metadata": {"page_label": "231", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8cb2d50a-b403-4f28-a5df-fa8fb3e5b5fa", "node_type": "4", "metadata": {"page_label": "231", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "cf8bcf1e076066e768c3377d43771449a8899f09cedcf81583e666bd94012ea9", "class_name": "RelatedNodeInfo"}}, "text": "Appendix[ 231 ]How to access the terminal\nOn Windows, we press Ctrl+Esc  or the Win key to open the applications menu and \nsearch for cmd. On Mac, we press Cmd+Space  and search for terminal .\nIn both cases, we get a console window, and when we type vagrant , a few \ninstructions will be printed. That's all we need for now.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d27bbdfe-d8f7-4e2f-805d-8c4f68e43214": {"__data__": {"id_": "d27bbdfe-d8f7-4e2f-805d-8c4f68e43214", "embedding": null, "metadata": {"page_label": "232", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d7c013c-ce75-407d-b7e4-61a1f819cad7", "node_type": "4", "metadata": {"page_label": "232", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "fb2dca05c72a5c19b68ec22152ac8409132f62790021f4e42c3afd11ad40ffbd", "class_name": "RelatedNodeInfo"}}, "text": "Installing and troubleshooting prerequisite software[ 232 ]Install VirtualBox and Git\nIn order to simplify this step, we will install the Docker Toolbox that contains \nboth Git and VirtualBox. If we Google search for docker toolbox install , we end up \nat https://www.docker.com/docker-toolbox  where we can download the \nappropriate version for our operating system. The installation is as easy as Vagrant's:\nEnsure that VirtualBox supports 64-bit images\nAfter the installation of Docker Toolbox, we should be able to find the VirtualBox \nicon in our Windows desktop or Mac's Launchpad (press F4 to open it). It's \nimportant to check early whether our VirtualBox supports 64-bit images.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "823c97ec-8e0e-43b7-91c7-5361c3f40d2d": {"__data__": {"id_": "823c97ec-8e0e-43b7-91c7-5361c3f40d2d", "embedding": null, "metadata": {"page_label": "233", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9360c30c-183b-45b2-ad4e-c6e285753388", "node_type": "4", "metadata": {"page_label": "233", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "15feed0d1703338f26ef87c8759c4d29aeb01e22e5c9fcddff82b706eec504b9", "class_name": "RelatedNodeInfo"}}, "text": "Appendix[ 233 ]\nWe open VirtualBox and click the New  button to create a new virtual machine. We \ncheck the version drop-down menu, check the options, and then click Cancel . We \ndon't really need to create a virtual machine right now.\nIf the drop-down menu included 64-bit images, we can skip to \nthe next section.\nIf the drop-down menu didn't include any 64-bit images or when we try to run a 64-\nbit VM it gives error messages such as VT-x/AMD-V hardware acceleration is not \navailable on your system , we may be in a bit of trouble.\nThis means that VirtualBox can't detect VT-x or AMD-V extensions on our PC. If our \nhardware is old then it's reasonable and expected. If it's new, then these extensions \nmay be disabled by the BIOS. If we are on Windows (most likely), an easy way to \ncheck is with a tool called SecurAble that we can download from https://www.grc.\ncom/securable.htm . If the Hardware Virtualization  is red and says  No, it means \nthat our CPU doesn't support the necessary virtualization extensions. In this case, we \nwon't be able to run Vagrant/Docker, but we will still be able to install Scrapy and \nfollow the examples using the online website ( scrapybook.s3.amazonaws.com ) as \na source. Start using the spider from Chapter 4 , From Scrapy to a Mobile App , which \nshould work out of the box and build from there.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56fe13e5-7223-4838-bdc5-4cac69a914f8": {"__data__": {"id_": "56fe13e5-7223-4838-bdc5-4cac69a914f8", "embedding": null, "metadata": {"page_label": "234", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25de1824-7c41-471e-a874-be67ccfcd70e", "node_type": "4", "metadata": {"page_label": "234", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "eb51a7477a2f2c661476ebd40431bd6504b806339eccf588efa91dbfb2b47d6a", "class_name": "RelatedNodeInfo"}}, "text": "Installing and troubleshooting prerequisite software[ 234 ]If the Hardware Virtualization  is green, we will most likely be able to enable the \nextension from our BIOS. Google search for your PC model and how to change the \nBIOS settings that are related to VT-x or AMD-v. There's usually a key that we can \npress while rebooting that gives us access to BIOS. From here, we have to go to a \nsecurity-related menu and enable the option of Virtualization Technology  (VTx ) \nor something similar. After rebooting, we will be able to run 64-bit virtual machines \nfrom our computer.\nEnable ssh client for Windows\nIf we're on a Mac, we won't need this step, and we can skip to the next section. If \nwe're on Windows, it doesn't provide us with a default ssh client. Fortunately, GIT \n(which we've just installed) has an ssh client that we will activate by adding it on \nWindows Path.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16f87239-f9ab-4251-bf25-05d146002323": {"__data__": {"id_": "16f87239-f9ab-4251-bf25-05d146002323", "embedding": null, "metadata": {"page_label": "235", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57201e05-589e-46d0-b4bb-a6c0a7ef6427", "node_type": "4", "metadata": {"page_label": "235", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a967b8adee1222adf46bbad38ffd6e718b6f26936538627c84277c7f3212060e", "class_name": "RelatedNodeInfo"}}, "text": "Appendix[ 235 ]The ssh binary, by default, lives in C:\\Program Files\\Git\\usr\\bin  (refer to 1). We \nwill add both C:\\Program Files\\Git\\usr\\bin  and C:\\Program Files\\Git\\bin  \nto the path. In order to do so, we put them in a Notepad, and we concatenate them \nadding ; just before each one of them (refer to 3). The end result is as follows:\n;C:\\Program Files\\Git\\bin;C:\\Program Files\\Git\\usr\\bin\nWe now press Ctrl+Esc  or the Win key to open the start menu and then find the \nComputer  option. We right-click it (refer to 4) and select Properties . On the next \nwindow, we choose Advanced System Settings . We then click Environment \nVariables . This is the form where we edit our Path . Click on Path  to edit it. On the \nEdit User Variable  dialog, we paste at the end the two new concatenated paths that \nwe have in Notepad. We should be careful not to accidentally overwrite whatever \nvalue our path previously had; we just append. Then we click OK a few times to exit \nall these dialogs, and our prerequisites are all installed.\nDownload this book's code and set up  \nthe system\nNow that we have a fully functional Vagrant system, we open a new console/\nterminal/command line (we've seen how to do this earlier), type the following \ncommands, and enjoy the book:\n$ git clone https://github.com/scalingexcellence/scrapybook.git\n$ cd scrapybook\n$ vagrant up --no-parallel\nSystem setup and operations FAQ\nNext are the solutions to some of the problems you may run into while working with \nScrapy for the first time:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50d00309-9cf0-494a-aa60-7632293cc74c": {"__data__": {"id_": "50d00309-9cf0-494a-aa60-7632293cc74c", "embedding": null, "metadata": {"page_label": "236", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ae2302c-4b63-44dd-af6d-8ef751b710c8", "node_type": "4", "metadata": {"page_label": "236", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a5bcc7b7a312570148007458e296174179a1608df35d68c92e4819b31c40288c", "class_name": "RelatedNodeInfo"}}, "text": "Installing and troubleshooting prerequisite software[ 236 ]What do I download and how much time does \nit take?\nAs soon as we run vagrant up --no-parallel , we don't have that much visibility \nof what's going on. The wall time is closely related to our download speed and the \nquality of our internet connection. Here's what one would expect to happen with an \ninternet connection able to download about 5 MB per second (38 Mbps):\nThe first three steps aren't necessary if we're on Linux and have Docker already \ninstalled saving us 4' and 450 MBs of download.\nPlease note that all the preceding steps are relevant only to the first time when \nvagrant up --no-parallel  downloads everything. Subsequent runs will typically \ntake less than 10\".", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40531ee5-37a3-4f9a-a585-e533b8c37ccd": {"__data__": {"id_": "40531ee5-37a3-4f9a-a585-e533b8c37ccd", "embedding": null, "metadata": {"page_label": "237", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "981f28d3-ec7c-414f-98d0-54474de618d1", "node_type": "4", "metadata": {"page_label": "237", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "05f047bf335098913cfd1d4aa8557d3f2d2e90c88ffc1a78290df6bc36872b3f", "class_name": "RelatedNodeInfo"}}, "text": "Appendix[ 237 ]What should I do if Vagrant freezes?\nVagrant may freeze due to various reasons, and all we have to do is press Ctrl+C  \ntwice to exit. Then retry vagrant up --no-parallel  and it should resume. We \nmay have to do this a few times depending on the speed and quality of our internet \nconnection. If we open the Windows Task Manager  or the Activity Monitor  on Mac, \nwe can have a more clear view of what Vagrant is doing.\nShort freezes of up to 60 seconds during or after download are expected because \nsoftware gets installed. Longer periods of inactivity, though highly likely, mean that \nsomething went wrong.\nIf we interrupt and resume, vagrant up --no-parallel  may fail with an error that \nis similar to this:\nVagrant cannot forward the specified ports on this VM... The forwarded \nport to 21 is already in use on the host machine.\nThis is also a temporary problem. If we rerun vagrant up --no-parallel , it should \nresume successfully.\nLet's assume the following failure takes place:\n... Command: \"docker\" \"ps\" \"-a\" \"-q\" \"--no-trunc\"\nStderr: bash: line 2: docker: command not found\nIf this happens, shut down and resume the VM as shown in the next answer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4fcf5e4-1486-4698-9e27-c7cc51fc30f9": {"__data__": {"id_": "c4fcf5e4-1486-4698-9e27-c7cc51fc30f9", "embedding": null, "metadata": {"page_label": "238", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56daa91c-dac8-414b-8906-123be8275ce4", "node_type": "4", "metadata": {"page_label": "238", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "917d2c0278b020bdff8b139f730d83ebd7a20ae1640e8159f1ec1784f3965d5c", "class_name": "RelatedNodeInfo"}}, "text": "Installing and troubleshooting prerequisite software[ 238 ]How do I shut down/resume the VM quickly?\nIf we use a VM, the fastest way to shut it down, for example, to save battery on a \nlaptop, is to open VirtualBox, select the VM, and then press Ctrl+V  or Cmd+V , or use \nthe right-click menu and click Save State :\nWe can restore the VM by running vagrant up --no-parallel . dev, and Spark \nservers' ~/ book  directories should work fine.\nHow do I fully reset the VM?\nIf we want to change the number of cores, RAM, or port mappings in the VM, we \nhave to perform a full reset. To do this, we follow the steps of the previous answer but \nnow chose Power Off  or press Ctrl+F  or Cmd+F . We can also achieve the same thing \nprogrammatically by running vagrant global-status --prune . We find the ID (for \nexample, 95d1234) of the host virtual machine that is named \"docker-provider\". We \nthen halt it with vagrant halt , for example, vagrant halt 957d887 .\nWe can then restart the system with vagrant up --no-parallel . Unfortunately, \ndev and spark machines will most likely empty their ~/ book  directories. To fix this \nproblem, run vagrant destroy -f dev spark  and then rerun vagrant up --no-\nparallel . This should fix these problems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0be00f96-1ca3-4829-a8b6-c8d801f47d78": {"__data__": {"id_": "0be00f96-1ca3-4829-a8b6-c8d801f47d78", "embedding": null, "metadata": {"page_label": "239", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e35e66e4-8188-40cd-98a1-63f018ebdd8e", "node_type": "4", "metadata": {"page_label": "239", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "88a04ba1641940186c42bfb3adc8820ea8d752594edf50dc7cb25b8b147082ec", "class_name": "RelatedNodeInfo"}}, "text": "Appendix[ 239 ]How do I resize the virtual machine?\nWe may want to change the size of the VM from, for example, using 2 GB of RAM \nto 1 GB or using 8 cores instead of 4. We can do this by editing the vb.memory  and \nvb.cpus  settings of Vagrantfile.dockerhost . Then, we follow the process of the \nprevious answer to perform a full reset of the Virtual Machine.\nHow do I resolve any port conflicts?\nSometimes, we may have services running on a host that is occupying one of the \nports that this system requires. First of all please keep in mind that if we open the two \nVagrantfile , remove every forwarded_port  statement, and reset as described in a \nbit, we will still be able to run the examples of this book. We just won't be able to easily \ninspect the services on these ports on our host machine (typically via a web browser).\nThat said, we can fix this more properly by remapping the conflicting ports. Let's \nuse a conflict on port 9312 of the web server as an example. The process is slightly \ndifferent depending on whether we run on Linux natively or using a VM.\nOn Linux using Docker natively\nThe problem will demonstrate itself with an error along these lines:\nStderr: Error: Cannot start container a22f...: failed to create \nendpoint web on network bridge: Error starting userland proxy: listen \ntcp 0.0.0.0:9312: bind: address already in use\nOpen the Dockerfile and edit the host  value of the forwarded_port  statement for \nthe web server. We will then destroy the web server using vagrant destroy web  \nand then restart it with vagrant up web , or if the problem happened during your \ninitial load, resume loading with vagrant up --no-parallel .\nOn Windows or Mac using a VM\nHere, we will get a different error message:\nVagrant cannot forward the specified ports on this VM, since they\nwould collide... The forwarded port to 9312 is already in use\non the host machine...\nIn order to fix this, we open Vagrantfile.dockerhost  and remove the existing \nline with the port number. Then add a custom port forwarding statement below, for \nexample, config.vm.network \"forwarded_port\", guest: 9312, host: 9316 . This \nwill forward to 9316 instead. We follow the process of the answer to the question How \ndo I full reset the VM?  to reset the Virtual Machine and everything should work fine.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1bd49f8-d525-4600-bfd7-94b58caf6198": {"__data__": {"id_": "a1bd49f8-d525-4600-bfd7-94b58caf6198", "embedding": null, "metadata": {"page_label": "240", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61471dc9-906e-49a9-a1fc-43540c3c2cfd", "node_type": "4", "metadata": {"page_label": "240", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a4b633117d8230744a674fe97262690b596a8b999b70e96255240b80686421c8", "class_name": "RelatedNodeInfo"}}, "text": "Installing and troubleshooting prerequisite software[ 240 ]How do I make it work behind a corporate \nproxy?\nThere are simple proxies and TLS interception proxies . Simple proxies require us to \nforward our requests to a proxy server before they reach the open internet. They \nmay or may not require authentication, but in any case, the information that we need \nto use it is just a URL that we can obtain from our IT department. It's something \nalong the lines of http://user:pass@proxy.com:8080/ . If we are using Linux \nwithout a VM, most likely we've already set up everything correctly and no further \nadjustments are necessary. If we're using a VM though, we will need to make the \nproxy URL available to Vagrant, Docker provider VM, Ubuntu's aptitude, and \nthe Docker service itself. All these operations get handled in the Vagrantfile.\ndockerhost , and all we need to do is uncomment the line that defines proxy_url  \nand set its value appropriately.\nLet's suppose that we get the following SSL-related problems:\nSSL certificate problem: unable to get local issuer certificate\n...\nIf you'd like to turn off curl's verification of the certificate, use\n the -k (or --insecure) option.\nFrom either Vagrant or while provisioning Docker, we most likely have to deal with \nTLS interception proxies. These proxies aim to monitor both secure and insecure \ntraffic by acting as a \"man in the middle\". They perform https requests verifying \ncertificates as necessary on our behalf, and we perform https connections to them \nverifying their certificates. Our IT department most likely provides us a certificate, \ntypically in the form of a .crt  file. We place a copy of this file in our book's main \ndirectory (where Vagrantfile  is). Further to setting proxy_url  as in the previous \ncase, we now have to further uncomment the line that defines the crt_filename  \nvariable and set its value to the name of our certificate file.\nHow do I connect with the Docker  \nprovider VM?\nIf we are on Linux and we aren't using a VM, then our machine is the Docker \nprovider and we don't have to do anything. If we are using a VM, we can find the \nID of our Docker provider by running vagrant global-status --prune  and then \nfind the machine named docker-provider . We can automate this on Linux and Mac \nwith the following alias:\n$ alias provider_id=\"vagrant global-status --prune | grep 'docker-\nprovider' | awk '{print \\$1}'\"", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c394b4cd-ba3e-4d5e-b491-fef5907482df": {"__data__": {"id_": "c394b4cd-ba3e-4d5e-b491-fef5907482df", "embedding": null, "metadata": {"page_label": "241", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d258ce96-c3af-45f2-bb04-7f477a1d8a2b", "node_type": "4", "metadata": {"page_label": "241", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "15f19d0e65dd604918e3aa9d73d51704cbc8b647ac4fd8b75ba7c12e2f45aa75", "class_name": "RelatedNodeInfo"}}, "text": "Appendix[ 241 ]We can use vagrant ssh <provider id>  or, in case we have the alias, vagrant \nssh $(  provider_id)  to connect to the Docker provider. This is an Ubuntu Trusty \n64-bit Virtual Machine.\nHow much CPU/memory does each  \nserver use?\nIf we use Docker natively or connect to the provider as described in the previous \nanswer, we can see the resources that each individual Docker container consumes \nusing docker stats  as follows:\n$ docker ps --format \"{{.Names}}\" | xargs docker stats\nHere is an example of the output while running the code in Chapter 11 , Distributed \nCrawling with Scrapyd and Real-Time Analytics,  at the point where scrapyd's \ndownloading intensively from the web server:\nHow can I see the size of Docker container \nimages?\nIf we use Docker natively or connect to the provider as we have seen in the previous \nanswer, we can find the size of Docker images as follows:\n$ docker images\nThis book's containers are based on a single image with relatively little extra software \ninstalled on each variation. Consequently, the GBs one may see as virtual sizes \naren't really used in terms of disk space. If we want to see how images are built \nhierarchically and individual sizes, we can set up an alias for the somewhat long \ndockviz  command and then use it as follows:\n$ alias dockviz=\"docker run --rm -v /var/run/docker.sock:/var/run/docker.\nsock nate/dockviz\"\n$ dockviz images -t", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8aee6c0f-75e9-42fc-9f13-9de06dcfb740": {"__data__": {"id_": "8aee6c0f-75e9-42fc-9f13-9de06dcfb740", "embedding": null, "metadata": {"page_label": "242", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ecdd703-75ad-4316-91cf-8c559261756d", "node_type": "4", "metadata": {"page_label": "242", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "ec27ce95789775b977944c52ec9bd8dfeff9d813e4ac44f4a37199165d6a82a7", "class_name": "RelatedNodeInfo"}}, "text": "Installing and troubleshooting prerequisite software[ 242 ]How can I reset the system if Vagrant doesn't \nrespond?\nWe can perform a full reset of the system even if it ended up in a very confused state \nwhere even Vagrant can't reset it anymore. We can do this without resetting the host \nVM, which admittedly takes some time to complete. All we have to do is connect to the \ndocker provider machine and force-stop all the containers, remove their images, and \nrestart Docker. We can do this as follows:\n$ docker stop $(docker ps -a -q)\n$ docker rm $(docker ps -a -q)\n$ sudo service docker restart\nWe could also use the following command:\n$ docker rmi $(docker images -a | grep \"<none>\" | awk \"{print $3}\")\nWe use this to remove any Docker layers that we've downloaded, which means a few \nminutes of download time on our next vagrant up --no-parallel .\nThere's a problem I can't work around, what \ncan I do?\nWe can always use VirtualBox and an image of Ubuntu 14.04.3 (Trusty Tahr) from \nosboxes.org ( http://www.osboxes.org/ubuntu/ ) and follow the process for Linux \ninstallation. The code will then run entirely inside the VM. The only thing that we \nwill miss is port forwarding and synced folders, which means that we either have to \nset them up manually or perform our development inside the VM.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9e39198-5f3b-4476-bb38-91eba4407fb4": {"__data__": {"id_": "e9e39198-5f3b-4476-bb38-91eba4407fb4", "embedding": null, "metadata": {"page_label": "243", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f512b869-bf03-4263-9f54-b16539c58706", "node_type": "4", "metadata": {"page_label": "243", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c7fdb9f4fc9f99741b703c2edef6c5c59c4c258b5f81fb6303a1f6bb54eec2a7", "class_name": "RelatedNodeInfo"}}, "text": "[ 243 ]Index\nA\nAmazon web services  115\nApache Spark Streaming\nused, for calculating shift  218-220\nattribute  14\nautomated data scraping\ncommunities, discovering  7\ncommunities, integrating  7\nforms, replacing  6\nimportance  4\nquality applications, developing  5\nquality minimum viable products,  \ndeveloping  5, 6\nrealistic schedules, providing  5\nrobust applications, developing  5\nB\nbenchmark system  183-185\nbottleneck\nidentifying  178\nC\nChrome\nused, for obtaining XPath expressions  20\ncollection\ncreating  64-66\ncomponent\nutilization getting, telnet used  180-182\ncorporate proxy  240\ncrawl URLs\nbatching  209-213\nCRUD (Create, Read, Update, Delete)  148\ncustom monitoring command\ncreating  217, 218D\ndatabase\ncreating  64-66\ninterfacing, with standard  \nPython clients  159\npopulating, with Scrapy  66-69\ndenial-of-service (DoS) attack  8\ndistributed crawl\nrunning  220-222\ndistributed system\noverview  205-207\nDocker\nURL  229\nDocker container images\nsize, viewing  241\nDocker provider VM  240\ndocker toolbox\nURL  232\nDocument Object Model (DOM)  11\nDomain Name System (DNS)  12\nDOM tree representation  11, 12\nE\nElasticsearch\nabout  148-151\ngeoindexing, enabling  158\nG\nGit\ninstalling  232", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c159c44-2e7d-4745-b8ff-baa5598ffe1b": {"__data__": {"id_": "7c159c44-2e7d-4745-b8ff-baa5598ffe1b", "embedding": null, "metadata": {"page_label": "244", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d52a3df3-cb27-4a26-bc9a-534ab5fb2ed1", "node_type": "4", "metadata": {"page_label": "244", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "5c28af3bfb93feccb26abdbbf181915de0443a9b2e31bb34fc6b60f181aaa372", "class_name": "RelatedNodeInfo"}}, "text": "[ 244 ]Google Geocoding API\nused, for geocoding  151-157\nGumtree\nURL  32\nH\nHTML\nabout  11, 12\ntext representation  15, 16\ntree representation  15, 16\nHTML document  12-14\nHTML element\nabout  13\nselecting, with XPath  16, 17\nI\nitems\naccessing  102, 103\nL\nLinux\ninstalling on  228, 229\nM\nMac\ninstalling on  230\nminimum viable product (MVP)  5\nmobile application\ncreating  69\ndatabase access service, creating  70\ndata, mapping to user interface  72, 73\nexporting  74, 75\nframework, selecting  63, 64\nsharing  74\ntesting  74\nuser interface, setting up  70, 71\nN\nnutshell\ninstalling in  228O\nosboxes.org\nURL  242\nP\nperformance issues, solving\nabout  187\nblocking code  189, 190\ndownloader  197-199\ngarbage on downloader  191-193\noverflow due to limited/excessive item \nconcurrency  195, 196\noverflow due to many or large  \nresponses  194, 195\nsaturated CPU  188, 189\npipeline\nbinaries or scripts, using  170\nCPU blocking operations  167\nCPU intensive operations, performing  167\nreading/writing, to Redis  163-166\nwriting, to Elasticsearch  148-151\nwriting, to MySQL  159-161\nport conflicts\non Linux, Docker used  239\non Windows or Mac, VM used  239\nresolving  239\nprerequisites\ninstalling  225\nproperty title  202\nQ\nqueuing systems\ncascading  177\nR\nrecurring crawls\nscheduling  104\nRedis\npipeline, reading to  163-166\npipeline, writing to  163-167\nREST APIs\nusing  148\nrobots.txt file\nURL  8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1388, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea078fda-849d-4a36-ba6d-d71dc781a018": {"__data__": {"id_": "ea078fda-849d-4a36-ba6d-d71dc781a018", "embedding": null, "metadata": {"page_label": "245", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16a0db4a-3df9-4260-a93f-dc4022691c53", "node_type": "4", "metadata": {"page_label": "245", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "38bb733665d51161e43c13f11f1e56b4769a0a8da41b021c1e31014b23ae029d", "class_name": "RelatedNodeInfo"}}, "text": "[ 245 ]S\nScrapinghub\nproject, starting  98\nsigning in  98\nsigning up  98, 99\nScrapy\nabout  1, 2, 122-124\narchitecture  134-136\nbenefits  2, 3\ncommunity, URL  3\ndatabase, populating with  66-69\ndeferreds and deferred chains  124-127\nengine  176, 177\nextending, beyond middleware  144\nextension  140-143\ninstalling  26\ninstalling, from latest source  28\ninstalling, on Linux  27\ninstalling, on MacOS  26\ninstalling, on Red Hat or CentOS Linux  28\ninstalling, on Ubuntu or Debian Linux  28\ninstalling, on Windows  27\nmisinterpretation, avoiding  8\nperformance model  179, 180\nsettings, using  106\nsignals  138-140\nsimple pipeline  137, 138\nspider  207\ntroubleshooting flow  199, 200\nTwisted and nonblocking I/O  127-134\nupgrading  29\nURL  28\nScrapyd  202-205\nscrapyd-deploy\nURL  217\nscrapyd servers\nproject, deploying to  216, 217\nScrapy project\nabout  40\ncontracts, creating  53\nfiles, saving to  47-49\nitem loaders and housekeeping fields  49-52\nitem, populating  46, 47\nitems, defining  41, 42\nspiders, writing  42-46SecurAble\nURL  233\nservices\ninterfacing, Twisted-specific  \nclients used  163\nsettings, Scrapy\nAmazon web services  115\nanalysis  107\nAutothrottle extension settings  119\nCrawleras clever proxy, using  116\ncrawling, style  112, 113\ncrawls, stopping early  111\ndebugging  120\nessential settings  107\nextending  118\nfeeds  113\nfine-tuning downloading  119\nHTTP caching  111\nimages, downloading  114, 115\nlogging  108, 120\nmedia, downloading  114\nMemory UsageExtension Settings  119\nother media files  114\nperformance  110\nproject related settings  118\nproxies, using  116\nproxying  116\nstats  108\ntelnet  108\ntelnet, using  108, 109\nworking offline, cache used  111, 112\nsharded-index crawling  207-209\nspider\narguments, passing between responses  87\ndeploying  100-102\nproperty spider  88-92\nruns, scheduling  100, 101\nthat crawls, Excel file based  92-95\nthat logs in  78-84\nthat reads AJAXs pages  84-87\nthat reads JSON API  84-87\nssh client\nenabling, for Windows  234, 235\nstandard performance model   185-187\nstart URLs\ngetting, from settings  214, 215", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d02b82e-5585-4893-b764-292f4333a157": {"__data__": {"id_": "8d02b82e-5585-4893-b764-292f4333a157", "embedding": null, "metadata": {"page_label": "246", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48a76c2b-1991-409d-9f66-3a43ea332a37", "node_type": "4", "metadata": {"page_label": "246", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "423664a2fd36771698db83074d84136a50e204cf9f7f6922e214f669ccd8db0b", "class_name": "RelatedNodeInfo"}}, "text": "[ 246 ]system\npeformance  223\nsetting up  226-236\nT\ntag  13\ntelnet\nused, for getting components  \nutilization  180-182\nterminal\naccessing  231\ntree representation  14\ntreq\nusing  148\nTwisted-specific clients\nused, for interfacing services  163\nU\nUR2IM process\nabout  31\nitem  34-39\nrequest and response  33, 34\nURL  32\nURLs\nextracting  55-58\ntwo-direction crawling, with  \nCrawlSpider  61, 62\ntwo-direction crawling, with spider  58-60\nV\nVagrant\nabout  29, 30\ninstalling on  230\nnot responding  242\nURL  228\nvagrant up\nfreeze  237\nVirtualBox\n64-bit images support  232-234\ninstalling  232Virtualization Technology (VTx)  234\nVM\nresetting  238\nresizing  239\nresuming  238\nshutting down  238\nW\nweb scraping\nconsiderations  8\nWindows\ninstalling on  230\nX\nXPath\nabout  11, 12\nHTML element, selecting  16, 17\nXPath expressions\nchanges, anticipating  22, 23\nexamples  21, 22\nobtaining, Chrome used  20\nusing  17-20\nXPath functions\nURL  19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30e07b5d-b825-4220-9ead-f879a148724b": {"__data__": {"id_": "30e07b5d-b825-4220-9ead-f879a148724b", "embedding": null, "metadata": {"page_label": "247", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7db6e3a-3fee-4d15-87c7-da8be83afd01", "node_type": "4", "metadata": {"page_label": "247", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "fc173ee2035fb9aaaf6555f3ab160fd29ea52ba9454560dfaf69ac310024a892", "class_name": "RelatedNodeInfo"}}, "text": "Thank you for buying  \nLearning Scrapy\nAbout Packt Publishing\nPackt, pronounced 'packed', published its first book, Mastering phpMyAdmin for Effective \nMySQL Management , in April 2004, and subsequently continued to specialize in publishing \nhighly focused books on specific technologies and solutions.\nOur books and publications share the experiences of your fellow IT professionals in adapting \nand customizing today's systems, applications, and frameworks. Our solution-based books \ngive you the knowledge and power to customize the software and technologies you're using \nto get the job done. Packt books are more specific and less general than the IT books you have \nseen in the past. Our unique business model allows us to bring you more focused information, \ngiving you more of what you need to know, and less of what you don't.\nPackt is a modern yet unique publishing company that focuses on producing quality,  \ncutting-edge books for communities of developers, administrators, and newbies alike.  \nFor more information, please visit our website at www.packtpub.com .\nAbout Packt Open Source\nIn 2010, Packt launched two new brands, Packt Open Source and Packt Enterprise, in order  \nto continue its focus on specialization. This book is part of the Packt Open Source brand,  \nhome to books published on software built around open source licenses, and offering \ninformation to anybody from advanced developers to budding web designers. The Open \nSource brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty \nto each open source project about whose software a book is sold.\nWriting for Packt\nWe welcome all inquiries from people who are interested in authoring. Book proposals should \nbe sent to author@packtpub.com . If your book idea is still at an early stage and you would \nlike to discuss it first before writing a formal book proposal, then please contact us; one of our \ncommissioning editors will get in touch with you. \nWe're not just looking for published authors; if you have strong technical skills but no writing \nexperience, our experienced editors can help you develop a writing career, or simply get some \nadditional reward for your expertise.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccfa0a3f-b14d-4150-a6fc-dad8bb6c193f": {"__data__": {"id_": "ccfa0a3f-b14d-4150-a6fc-dad8bb6c193f", "embedding": null, "metadata": {"page_label": "248", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11dccb50-ea5b-447f-b4ad-c8fbd5501119", "node_type": "4", "metadata": {"page_label": "248", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "a63454531a9c3cc930f5fcbfc987c10d040fec86c6910cf057872e8a0dd3a254", "class_name": "RelatedNodeInfo"}}, "text": "Instant PHP Web Scraping\nISBN: 978-1-78216-476-0              Paperback: 60 pages\nGet up and running with the basic techniques of web \nscraping using PHP\n1. Learn something new in an Instant! A short, \nfast, focused guide delivering immediate \nresults.\n2. Build a re-usable scraping class to expand on \nfor future projects.\n3. Scrape, parse, and save data from any website \nwith ease.\n4. Build a solid foundation for future web \nscraping topics.\nInstant Web Scraping  \nwith Java\nISBN: 978-1-84969-688-3             Paperback: 72 pages\nBuild simple scrapers or vast armies of Java-based \nbots to untangle and capture the Web\n1. Learn something new in an Instant! A short, fast, \nfocused guide delivering immediate results.\n2. Get your Java environment up and running.\n3. Gather clean, formatted web data into your  \nown database.\n4. Learn how to work around crawler-resistant \nwebsites and legally subvert security measures.\n \nPlease check www.PacktPub.com  for information on our titles", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44f83b38-7e01-4f99-8364-b8afe8ca798b": {"__data__": {"id_": "44f83b38-7e01-4f99-8364-b8afe8ca798b", "embedding": null, "metadata": {"page_label": "249", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54eecb9d-e31c-4c3a-899d-3e1b036f0f60", "node_type": "4", "metadata": {"page_label": "249", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}, "hash": "c0ece57ab6c13d5de97f5167e838e9e33b52fb1b618a0ea73d0c289929694c05", "class_name": "RelatedNodeInfo"}}, "text": "Learning Data Mining with R\nISBN: 978-1-78398-210-3              Paperback: 314 pages\nDevelop key skills and techniques with R to create \nand customize data mining algorithms\n1. Develop a sound strategy for solving predictive \nmodeling problems using the most popular \ndata mining algorithms.\n2. Gain understanding of the major methods of \npredictive modeling.\n3. Packed with practical advice and tips to help \nyou get to grips with data mining.\nMastering Object-oriented Python\nISBN: 978-1-78328-097-1             Paperback: 634 pages\nGrasp the intricacies of object-oriented programming \nin Python in order to efficiently build powerful  \nreal-world applications\n1. Create applications with flexible logging, \npowerful configuration and command-line \noptions, automated unit tests, and good \ndocumentation.\n2. Use the Python special methods to integrate \nseamlessly with built-in features and the \nstandard library.\n3. Design classes to support object persistence  \nin JSON, YAML, Pickle, CSV, XML, Shelve,  \nand SQL.\n \nPlease check www.PacktPub.com  for information on our titles", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"d8047731-f86b-47f5-9208-b627a0d9a405": {"node_ids": ["5e46ff26-85ec-4467-a6a3-0e524961d68b"], "metadata": {"page_label": "Cover", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "bf9bfd21-e034-4a71-9fd3-af6a09b58be6": {"node_ids": ["f75aa2eb-3138-40ae-b6db-4bf7d2d01d41"], "metadata": {"page_label": "FM1", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "53750600-526d-4e85-a907-f0c5560deac2": {"node_ids": ["24033059-a7a5-42fe-8f17-2b4c3643fd32"], "metadata": {"page_label": "FM2", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "794e12a2-dc7f-490f-9b9c-e6c0c5b713d8": {"node_ids": ["f20664ed-1090-4e57-a60d-58a42562dc62"], "metadata": {"page_label": "FM3", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "dcd5cab4-4994-4e98-a677-a4004d4510dd": {"node_ids": ["33c3ec2a-3d9e-4b5f-99b2-0568da0d823a"], "metadata": {"page_label": "FM4", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "12bb99a6-6534-4f08-8a28-adcc6179abd8": {"node_ids": ["4a09093c-28cf-49ed-a09c-078946014068"], "metadata": {"page_label": "FM5", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "095af734-1710-45f9-b28f-5380e3e8c94b": {"node_ids": ["e32c72ec-7ce8-479c-844b-1b127a29ba4e"], "metadata": {"page_label": "FM6", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "41d73e69-bf24-436f-8a6a-f0d38a33ed24": {"node_ids": ["3b27b47c-9b6a-4583-9d07-75cc369c941f"], "metadata": {"page_label": "i", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "000bae20-f570-4e2a-82eb-54c7762d1d14": {"node_ids": ["fc61189e-6a1c-4553-87cc-ddc0bb2c6be3"], "metadata": {"page_label": "ii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "5c974810-a925-4419-b2dc-2920e9794f5a": {"node_ids": ["13376bb6-36e9-439c-9400-f86a73385eac"], "metadata": {"page_label": "iii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d76d575c-27ca-400b-aa40-53acd3d5fa2c": {"node_ids": ["b662a6f4-883e-45fe-9963-dafab7e45245"], "metadata": {"page_label": "iv", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "ccc2ca28-a567-4613-bada-74579d4a3f41": {"node_ids": ["bd92f662-d22f-4c45-937a-48a7eda9ab3c"], "metadata": {"page_label": "v", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "a3c8ca37-a928-419f-9ec9-af78623d7387": {"node_ids": ["899a7c44-ef3a-4270-9a94-df4c7a9b54e0"], "metadata": {"page_label": "vi", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "91c71f01-86f7-463a-9535-51387506932a": {"node_ids": ["0ed21185-a1b0-4acc-b720-9dec8704115e"], "metadata": {"page_label": "vii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "28094a3c-1118-4e0e-9f18-db3d88d6a805": {"node_ids": ["669bc79e-4f2f-4662-9c87-2027f0528f34"], "metadata": {"page_label": "viii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "13b91b49-bb8d-499f-aa64-8a4fe3899b20": {"node_ids": ["c03cb067-fb0f-465f-8654-e590f81083c5"], "metadata": {"page_label": "ix", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "184a637a-3adb-4e11-9f39-d4e7b8e927cd": {"node_ids": ["123d63f5-ae64-414b-96ae-c7a585830a4c"], "metadata": {"page_label": "x", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f27466b4-bc1b-45ce-95d0-10da581e0082": {"node_ids": ["10eb6275-7cbb-45d2-996f-f575626a9e86"], "metadata": {"page_label": "xi", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "c4b0ddfa-a7c2-47c1-ab9f-799a5cef3b70": {"node_ids": ["4fc7bb18-d46a-4f34-94e6-1267be9924f6"], "metadata": {"page_label": "xii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "0d237f8a-d803-4e5c-a620-dd5c08937c88": {"node_ids": ["4c1e8140-0fce-43e4-b745-4876ac87ef31"], "metadata": {"page_label": "xiii", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "22e29a89-0ea5-435c-9231-ae85f0aa253f": {"node_ids": ["4b6861e3-a7c7-480e-948a-382e420bd6c2"], "metadata": {"page_label": "xiv", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "369dfcf4-8fee-48ce-bab0-b705bacd1113": {"node_ids": ["4b3d399e-3ef8-4a13-abf3-2dad1415d57c"], "metadata": {"page_label": "1", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "6b4a884d-dff3-4500-8381-4bf04c2ebcf9": {"node_ids": ["939e9f09-e46c-4bf6-aff2-b0b657232eca"], "metadata": {"page_label": "2", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e878a92b-77bd-4f46-bd89-27e41eaf3d37": {"node_ids": ["5ee25271-4507-4be4-97f0-fbe08eb57183"], "metadata": {"page_label": "3", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "ddc4ec1e-4c5e-4303-8ce5-d01893570a3a": {"node_ids": ["a6846dc8-31c2-4aa6-96d0-e441d5830749"], "metadata": {"page_label": "4", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "84c1476f-04c0-4513-ac00-8d57c2608ec4": {"node_ids": ["a863931e-78a9-4d07-aa8c-23a063c59631"], "metadata": {"page_label": "5", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d4d848c0-2093-4308-aa3e-e837896edf23": {"node_ids": ["fc464a98-8539-444e-a423-069f0226e54a"], "metadata": {"page_label": "6", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "1d1e51dd-15f4-4ead-a918-a966a805368f": {"node_ids": ["f4c8bb9e-30e0-40b2-8a7b-6e7747b77435"], "metadata": {"page_label": "7", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "7a543f68-cc1d-4963-ba9f-b1690bfe8d93": {"node_ids": ["62730dcb-d8be-4c10-8041-db51c4aee266"], "metadata": {"page_label": "8", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "651ec303-63ef-4d10-a912-8b4f3f215f6a": {"node_ids": ["05c682fb-ae90-4680-ac45-f5cc0e71a900"], "metadata": {"page_label": "9", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "531ba80c-edf1-41af-9db0-c50e5d1cb618": {"node_ids": ["6619335f-d4f2-4bcf-807d-0fb231faffa5"], "metadata": {"page_label": "10", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "64000ff4-a3c7-48e5-912b-18ac77dcbfc4": {"node_ids": ["a5a31ceb-ec2e-4978-ae8c-60bb13b108cd"], "metadata": {"page_label": "11", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "0731307d-2717-4ad4-887a-d4eb1a598ddb": {"node_ids": ["2d463acf-35ad-499e-bf22-9d29437813f2"], "metadata": {"page_label": "12", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "4ddffa52-f628-44c6-9fec-9df76e7c6dec": {"node_ids": ["dfe56a87-3d01-4fa3-8d63-0ed914975c5c"], "metadata": {"page_label": "13", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "ed16e6c6-3853-48b7-ab78-1d5917c4fb53": {"node_ids": ["056caece-2f41-415a-847a-6b4619847017"], "metadata": {"page_label": "14", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "6ee774ed-f554-44fd-9e7c-cbec957821bc": {"node_ids": ["08ba64e9-cf35-4f8a-b12e-85b524aa6696"], "metadata": {"page_label": "15", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "493874c9-fc7a-46a5-952d-4cc40d7b5f44": {"node_ids": ["1049d8ed-6225-48cd-858c-2cbd9c21efd7"], "metadata": {"page_label": "16", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "282cd0a3-6fea-423a-aff0-07ce06bfc6c8": {"node_ids": ["5de92e4e-cee4-4c1f-9161-6f76ccdeb20e"], "metadata": {"page_label": "17", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "743ef640-2ca5-4ae4-b3f1-8389eb330a94": {"node_ids": ["197bd7b9-004e-416d-ae04-af04ac5e8a01"], "metadata": {"page_label": "18", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "144cf655-c064-4541-ba7a-f8fd1a26cd71": {"node_ids": ["00c30f45-4cb6-453f-8459-ace6ffc79dbc"], "metadata": {"page_label": "19", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "83a88c77-8a6d-43d4-8ce1-bb6fa9f573ea": {"node_ids": ["186b9a0c-f180-481d-9c09-d25ba9dbe618"], "metadata": {"page_label": "20", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "4b5305d9-064c-4cf7-bc3f-a73299ea7ce9": {"node_ids": ["7d950b9a-9753-405e-a32d-a0893977d55e"], "metadata": {"page_label": "21", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "22a429bb-11a1-451a-85e1-6692cdb307f8": {"node_ids": ["5f198dc6-20f7-49ba-acc6-0fa7b11c3604"], "metadata": {"page_label": "22", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9decd28f-b1e6-4d2c-ac0e-5a5ef3b79ad9": {"node_ids": ["85823400-3c0e-44b4-b2db-a4d392954840"], "metadata": {"page_label": "23", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "2afae01f-4b93-4f6a-9ac4-5aa43801a725": {"node_ids": ["71244b1f-26e3-48fa-8a73-98e33d8c93df"], "metadata": {"page_label": "24", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "35474a31-1650-473f-a6a2-7d226b67a4db": {"node_ids": ["56a51535-bad6-4882-907b-22181ec3c54c"], "metadata": {"page_label": "25", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "cff52d8b-0a61-4d44-97b4-984ce9c21efa": {"node_ids": ["c53e19c7-96ee-4980-8f73-a9c450614c75"], "metadata": {"page_label": "26", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "97e39be8-dab5-4c1d-b529-e582e146bdd1": {"node_ids": ["afbc7918-d391-44c2-bb5b-6ac0093aeb11"], "metadata": {"page_label": "27", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "50ff17f2-18c0-4b6c-ad3c-b5051c6d0348": {"node_ids": ["b4dbda92-63a4-4b5b-88c0-87f66074dc8a"], "metadata": {"page_label": "28", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "a7ea82c2-fb81-481c-a092-c8425d27edc4": {"node_ids": ["329efccc-2202-4f99-9ada-3b82280913d7"], "metadata": {"page_label": "29", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "0b4184a0-8836-4710-8249-a1954c07279c": {"node_ids": ["9b5fa268-c48d-4898-8f41-228a09085fd1"], "metadata": {"page_label": "30", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "0f2074fa-3df6-4cf8-a50f-bf32ab8ff17c": {"node_ids": ["203920f2-f7cf-4859-a903-61cf0367d0ca"], "metadata": {"page_label": "31", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f51e46b4-d1c6-49f8-bf21-278040d32ff7": {"node_ids": ["dec05349-1670-4914-b209-87400343389a"], "metadata": {"page_label": "32", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d9ba5693-c090-498c-a2ff-3dccdd5e502c": {"node_ids": ["e6a8c80a-957b-4ca4-b5cc-227563d63500"], "metadata": {"page_label": "33", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3b1ee236-61e5-46ef-af8f-632cd957e2ff": {"node_ids": ["cfe13f5d-c6de-4f10-9cdc-0ca968cc3dbf"], "metadata": {"page_label": "34", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "598c72cb-9121-430a-abcb-869d79fff9fd": {"node_ids": ["c0e25f00-6d03-43c1-a1c6-96a02073b1b0"], "metadata": {"page_label": "35", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d689d4bd-73e1-42bc-a78e-d3db93795f30": {"node_ids": ["ef77447e-ff08-4aad-802b-589c650fe907"], "metadata": {"page_label": "36", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "b22ec1c1-c5d5-40d3-83ba-53760794e3da": {"node_ids": ["9e67f71f-79db-4666-8c2f-ca65f75f71af"], "metadata": {"page_label": "37", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e8c1ab1e-850a-49e4-9e1e-597421df5649": {"node_ids": ["c93fa4d5-a8e6-41ff-9a26-cc767995488f"], "metadata": {"page_label": "38", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f10e9dc3-2a5a-468a-aaa1-b1d512035d41": {"node_ids": ["aa397134-a271-4afd-8bd7-a511cd337927"], "metadata": {"page_label": "39", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "4dcd26de-aad6-4043-8fb1-05429ef756b5": {"node_ids": ["7a855481-04db-4793-ab18-ed7814a19e75"], "metadata": {"page_label": "40", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "5342f389-52cc-4e81-a12e-9f1a299143a3": {"node_ids": ["506f08b2-4390-49c9-869a-13a1a8235bf2"], "metadata": {"page_label": "41", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9f1ade06-2dd1-48d0-b7a3-bc36a540f6b8": {"node_ids": ["9bc95951-d9f9-4190-90d1-c436f4b69747"], "metadata": {"page_label": "42", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3d56ea21-939a-4134-95be-abb3686a9fa2": {"node_ids": ["0b2ea6b3-f1c9-4363-bcb3-c4ef06e7c97f"], "metadata": {"page_label": "43", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3fb14a6b-6388-4877-b0bf-db13a8abf066": {"node_ids": ["6964ace7-3441-4e1b-89a4-ba86ad81954e"], "metadata": {"page_label": "44", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "bddd971a-2008-42ac-9747-8bc363c7aceb": {"node_ids": ["404b88e0-0922-4a42-b267-bcf7f53fd1c4"], "metadata": {"page_label": "45", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "06c61145-4bdd-4d04-8959-67ffc4fb5178": {"node_ids": ["c9f23c79-02c1-4ad3-b39c-9c0477632566"], "metadata": {"page_label": "46", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "ecc9298c-fa21-413c-aca9-b403893a6a67": {"node_ids": ["9685144f-264c-4eae-8f2e-74ec1dc5533a"], "metadata": {"page_label": "47", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f9aca6cf-b9c3-4718-a1ba-6f8422582f27": {"node_ids": ["fdb5ea14-5a73-4454-ae38-41dc3b924c8b"], "metadata": {"page_label": "48", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "02197a79-7292-489c-a3e3-b093f58eb254": {"node_ids": ["c0c5fde0-62e5-436f-b6ae-4f94114210a4"], "metadata": {"page_label": "49", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f01006f3-e81c-448b-aaf2-ace749dcbc12": {"node_ids": ["9e56240f-eef4-4a53-977a-daf93f9d851c"], "metadata": {"page_label": "50", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "b758595c-9fe9-4453-9f25-07e0f59cc78e": {"node_ids": ["25ad3d92-788b-4979-a823-e4ee9319131c"], "metadata": {"page_label": "51", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "a8c4e4cc-3960-41b9-a068-cd651a991e71": {"node_ids": ["0bc11349-1046-48de-a834-ed5a3d646e40"], "metadata": {"page_label": "52", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "a0bced67-df54-4844-8c99-32c94a26e079": {"node_ids": ["36ac9d76-1c25-4d3a-aa74-63f1b6bccbce"], "metadata": {"page_label": "53", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "5591dfe6-336f-4aaf-9fbe-8803a4fdcc58": {"node_ids": ["33e1c742-b479-4dd8-9490-d20d8eda4a82"], "metadata": {"page_label": "54", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "a31dff71-b5d6-4337-8b47-a4d535b4c5c5": {"node_ids": ["3f0f5697-016f-49fa-966c-87cd7a8c4c8f"], "metadata": {"page_label": "55", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "032c09ea-7615-4c94-9857-1a0f998a0d2f": {"node_ids": ["56a825aa-8929-49ae-868d-e3c83254d5c3"], "metadata": {"page_label": "56", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "265dc7fd-e5e9-42a2-8d38-8ab7264c27db": {"node_ids": ["5f0e4379-7ed0-4d0c-b569-43d18a23e0a3"], "metadata": {"page_label": "57", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "db6741ce-55ee-420d-bb37-8414013e6fc3": {"node_ids": ["1b007f94-19cf-45ef-a92e-0ece64151a9a"], "metadata": {"page_label": "58", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "907c1d0c-b994-4985-a2ec-fa6898e493e6": {"node_ids": ["6f67156c-43ad-4fb9-8824-b011de53a1b8"], "metadata": {"page_label": "59", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3e8e16ce-9737-4e0d-ae5c-460e14a4b254": {"node_ids": ["1fc26ba3-d906-41d5-bfd8-dbe93f14b7d4"], "metadata": {"page_label": "60", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "cc0ace0f-4425-41aa-ad09-94b82b6b300c": {"node_ids": ["4b4386a2-945e-4096-85ce-21d6cc3a574b"], "metadata": {"page_label": "61", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "04f208d3-930b-4487-904f-35de70308c10": {"node_ids": ["23c78c0b-cc84-4a69-ba09-e15c701078f8"], "metadata": {"page_label": "62", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d8fcb4c7-820c-4627-8a61-c90a3d2474c9": {"node_ids": ["d53a1acd-f317-425c-ad58-cfd97da8bbfe"], "metadata": {"page_label": "63", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "aeaba038-126b-481d-9977-6e49ca2805ab": {"node_ids": ["482c9d24-58d7-4a79-99e6-fde5309da079"], "metadata": {"page_label": "64", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3277a520-1d7c-49c3-bc2f-f04e78b01107": {"node_ids": ["db7e80c8-4dcf-43e2-814a-4cd21a1db3bb"], "metadata": {"page_label": "65", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "a72a4de0-7137-4a59-beb1-74ae324df776": {"node_ids": ["90c0e58c-af8c-4c6b-9785-49f713fce8fd"], "metadata": {"page_label": "66", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f1ed563c-14c7-435b-aa8d-d9e2eaed4e62": {"node_ids": ["e8bc582d-1990-4c2b-bd28-b6dd09e6f6c4"], "metadata": {"page_label": "67", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f1259d0c-71a6-434f-8da3-1904ce7961a6": {"node_ids": ["cea2e09b-76f2-4447-8218-23dcf2ed48f7"], "metadata": {"page_label": "68", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "bf6962ae-3762-4ff0-b100-7155941d0796": {"node_ids": ["8b94d391-66e1-4448-8da9-2158ee8da54b"], "metadata": {"page_label": "69", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "5b604ca9-42f9-4a4a-9d21-fb4b8440e738": {"node_ids": ["cc84e1a2-e4e0-4b29-ac40-3b557def75f3"], "metadata": {"page_label": "70", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "730842b9-95f3-4ef6-9aea-37f0bb205b1b": {"node_ids": ["7d99d9c8-1639-44d5-8dcd-59587a13694e"], "metadata": {"page_label": "71", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "12c875a7-0ba4-4910-b9a0-837d53d3bfb4": {"node_ids": ["76752ad2-6966-4150-a02b-b3bdff4bcec3"], "metadata": {"page_label": "72", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f3a9a72e-b45a-478a-9d4e-c38342d2494b": {"node_ids": ["e256b78a-4ac8-4d50-89ff-6638b6bddf53"], "metadata": {"page_label": "73", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "34c9675f-17b6-4882-a9b4-04e1d51c2265": {"node_ids": ["f6f7a3b7-c754-44d4-80c2-10a6f6e5fd75"], "metadata": {"page_label": "74", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d6330fe0-a870-4567-abe7-ae77c30f0378": {"node_ids": ["9b8a1845-6c9f-41f1-8e7a-e43583454239"], "metadata": {"page_label": "75", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "df6399d0-534d-4ba8-899d-20d0cc62bf77": {"node_ids": ["e717cb25-d67f-4ffe-a5b7-616b2e4d642f"], "metadata": {"page_label": "76", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f5b2a9af-88bd-48b7-95e9-333bd1f1eb3b": {"node_ids": ["5f4688a7-7d86-44a7-b197-cb98a15f26ad"], "metadata": {"page_label": "77", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f98a2dfb-d725-424c-af17-2b263c963977": {"node_ids": ["83dbbcd2-17bc-455e-93f5-3b6430631af6"], "metadata": {"page_label": "78", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d6900a33-1c59-4ea3-bc89-37250cab5e54": {"node_ids": ["ec0cba00-2cae-4c3c-85dd-6f105a9c1a38"], "metadata": {"page_label": "79", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "67a5f6f5-c69b-430d-8519-8688e51feadb": {"node_ids": ["0df9465d-1fe9-4c11-8f70-0674b2aae9a8"], "metadata": {"page_label": "80", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9db7cb74-8532-4825-bc1c-e46dc2cc5282": {"node_ids": ["36b9c8ef-5366-4079-9671-7aa7e644c6c5"], "metadata": {"page_label": "81", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "1b8c466c-57cd-4fc9-821b-088f59bcf067": {"node_ids": ["12e606a0-13c0-4de7-91dc-1e4d4ae32e90"], "metadata": {"page_label": "82", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "7a115120-4afe-41cf-8de6-7f7b2b73df43": {"node_ids": ["596b2e91-3ca4-4c9e-ba78-fe2bc7813b90"], "metadata": {"page_label": "83", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "440a2d4c-60d8-471f-8199-ad6d5d5a7f35": {"node_ids": ["acd4af57-f24e-448e-a07e-0b110d6390a9"], "metadata": {"page_label": "84", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "1817412a-eff8-4669-9db7-3a87f3eb975d": {"node_ids": ["7167942a-073e-473c-80dd-3eff469a2da5"], "metadata": {"page_label": "85", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e8f8fa17-04e5-42f6-a76e-4da0c97ce9e0": {"node_ids": ["a5b00580-cdcd-42f2-ab29-1f50f8029d67"], "metadata": {"page_label": "86", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "5061526f-eb15-4894-be93-fdc86eb9ef1f": {"node_ids": ["0a280776-0a65-4e70-9d85-7b261843da8f"], "metadata": {"page_label": "87", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "ea23fb20-3622-41be-9eeb-f7b1ffcd6f3e": {"node_ids": ["3bb6ad28-dc38-404e-b34f-fd29e85b3dab"], "metadata": {"page_label": "88", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "7dbc7fdb-5578-43ad-98c6-24ba1c334a31": {"node_ids": ["c9619577-c5ca-469a-a4ae-c4483deb66a9"], "metadata": {"page_label": "89", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f9073010-c58e-4f72-aaf8-3fc9fbdd9be8": {"node_ids": ["063cbd6b-e0d5-434d-bdf4-768debc755bb"], "metadata": {"page_label": "90", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "7c947301-b1d4-4f9f-b0da-9d0638dd78ca": {"node_ids": ["8d0eef9f-7cce-475d-9571-5aa13c679949"], "metadata": {"page_label": "91", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "c1f50016-dded-490c-b586-327d60cd24fd": {"node_ids": ["3263f8d3-7032-4b3a-b952-66524a167bb5"], "metadata": {"page_label": "92", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "00a2b1cd-e5ec-4a1f-ba02-4d0937b9856f": {"node_ids": ["5810a16d-6893-472f-8b5d-85e8e18912c6"], "metadata": {"page_label": "93", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "4fa9d533-1a89-41bc-82b0-df65b93469b7": {"node_ids": ["a5df5063-dc7e-4e68-96ee-f511149643f5"], "metadata": {"page_label": "94", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "eb05c358-6d20-4f48-976e-fb9949fe9f1e": {"node_ids": ["e88f3eb2-d0d5-4696-aefd-4befcf947bdf"], "metadata": {"page_label": "95", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "25b4ba66-98af-45a2-a5d0-24ac58e101ae": {"node_ids": ["c3e26604-c7c7-48ca-95c2-ee511b882588"], "metadata": {"page_label": "96", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "af6edcfa-ca2e-4446-a1bc-1ee64199c805": {"node_ids": ["59f7f8c5-d043-485e-8a24-23ef521783d7"], "metadata": {"page_label": "97", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e45c5900-a7b3-4807-afee-8d21a304a178": {"node_ids": ["dad6c292-4c19-49f9-993d-06899ac0c8e8"], "metadata": {"page_label": "98", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "58af4e95-83ec-465c-b917-532eaf22841a": {"node_ids": ["216d1c06-32a8-48c2-99cf-a3b534695105"], "metadata": {"page_label": "99", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "43dd8565-3892-47ea-a3ce-1fe64ef5e9fd": {"node_ids": ["4877dcde-10bc-4f12-88f4-3dab34f84ffa"], "metadata": {"page_label": "100", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "8cf45b88-1dfb-4dc5-83ce-3228c341e2b0": {"node_ids": ["bf98c7ef-0995-4b43-bec1-89bf43575f1c"], "metadata": {"page_label": "101", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "188e509c-5349-406e-981a-ddb9c0deaf80": {"node_ids": ["34a13c87-334b-492b-b2a7-aa445877b052"], "metadata": {"page_label": "102", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9fb8712d-f8a5-4e20-b1d1-a0889a32e6be": {"node_ids": ["c4148335-4999-4e3c-91ff-f92df512baaa"], "metadata": {"page_label": "103", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e30d9eea-520f-402a-8e91-da6d4cd8e348": {"node_ids": ["22791e37-5bdb-487e-a96f-c328bdaa02bf"], "metadata": {"page_label": "104", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "5e2c8dd6-9a3c-4920-b630-4ad43af77f2f": {"node_ids": ["f35b98a3-2ea7-4cab-922c-739dca9256ef"], "metadata": {"page_label": "105", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "73362b9f-f597-4ee5-862f-802e822f499f": {"node_ids": ["fd997ac1-cd96-4a8a-840e-843a500dc6db"], "metadata": {"page_label": "106", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "14e656fc-262c-4b05-be19-3d8ae7ebaf0c": {"node_ids": ["7678093a-6c1b-43ed-b2f2-1ed5cdd6501f"], "metadata": {"page_label": "107", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "c967edc4-6a3a-43bc-bb8f-0f443dc3ad91": {"node_ids": ["dacff4fc-3f4d-44c5-adb5-44bb6537064e"], "metadata": {"page_label": "108", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "b78c2017-73bb-4dbb-944a-cc3545317c0f": {"node_ids": ["3294d776-e4ba-4bb6-8cad-113fa51f42ae"], "metadata": {"page_label": "109", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e79a7249-4bcc-4c2f-a34b-411c57c0e680": {"node_ids": ["1b59f1de-1355-4908-9904-f96b8849f0eb"], "metadata": {"page_label": "110", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "b7bd7f6f-6002-4892-af39-65dd40f50eb0": {"node_ids": ["31586065-9298-446f-b62a-d80751687c55"], "metadata": {"page_label": "111", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "64d808ba-1df0-4e93-a70a-6046f91fc9b6": {"node_ids": ["bbd06ac2-fe81-4d34-bb23-7ca7ac23f581"], "metadata": {"page_label": "112", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "1e29db17-a67a-45bb-b96f-5126cca22574": {"node_ids": ["b8d90375-da1d-4489-a9fd-c1a34cad52bf"], "metadata": {"page_label": "113", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "54bd53fb-b3c9-43b0-92d1-cbc7984ce1b4": {"node_ids": ["50bc80b1-5974-4341-990b-406f24607fee"], "metadata": {"page_label": "114", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "90448a71-83ab-49dd-b4e8-c7bb32a0bb19": {"node_ids": ["6ba86243-8cac-41ee-b74b-c7746d4638e1"], "metadata": {"page_label": "115", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "7583019a-8eba-4fbf-b74a-3f19ff212db8": {"node_ids": ["e09c3867-c431-494b-87db-209906c84a9e"], "metadata": {"page_label": "116", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "6b9ed295-4a6f-405f-8676-4e3c226d1d32": {"node_ids": ["86c0d7f7-e884-45be-954d-88d2116193d4"], "metadata": {"page_label": "117", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "cf7d5fdc-0c39-4329-bc87-1e908af0b536": {"node_ids": ["81bb5a0f-67fe-4ae1-8f68-17342acc3df2"], "metadata": {"page_label": "118", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "fa79d2c3-2afc-4ade-8b45-f59d50e199ad": {"node_ids": ["576752bc-8452-4af8-b894-a2559fb1829b"], "metadata": {"page_label": "119", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "412db2f0-0a38-4e23-99fd-1ebf965970b0": {"node_ids": ["98177f92-4733-42a7-bfd3-3f25b500ba46"], "metadata": {"page_label": "120", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "0b458b23-f178-4f3c-98b8-afde6ae5afff": {"node_ids": ["68021843-5627-4056-8fb4-c2e5aa00f90a"], "metadata": {"page_label": "121", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "83bb0a77-8bbd-45d6-8d8b-442686baa93f": {"node_ids": ["67b5cc36-765c-4750-a1f6-42e799fbd1b0"], "metadata": {"page_label": "122", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "aa89f926-9c71-4505-9d25-560d16ed12b8": {"node_ids": ["6ffc0892-86ea-4d16-bc1c-fe67fc19732c"], "metadata": {"page_label": "123", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "da579029-b1e5-494e-94da-2b4308c358fd": {"node_ids": ["609f97b6-26cb-4808-a87f-5ce928d6c058"], "metadata": {"page_label": "124", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "68b9ca2b-3cdc-4e47-ab12-b55a24d96994": {"node_ids": ["13caefff-6538-41c8-8fe3-7016614e0cbb"], "metadata": {"page_label": "125", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "faa98306-561d-4487-bb4c-3457f11054e8": {"node_ids": ["25cc1f12-e825-4d6d-8bad-3e6f99b6b7d6"], "metadata": {"page_label": "126", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3a4c0e3a-68f2-416b-b32e-a35b32c28d1e": {"node_ids": ["0d3a096a-7a30-44ed-b67b-e302b888be7b"], "metadata": {"page_label": "127", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "120c6e9f-a10c-4bb1-b4a2-5d432821ef3c": {"node_ids": ["50aeefcf-09e0-48c3-9dd0-848cc2f952d5"], "metadata": {"page_label": "128", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d53bbfd3-1a0a-4b43-ba15-e59e94eb55f6": {"node_ids": ["b06129b8-7300-46e0-ad67-5c57caf159a7"], "metadata": {"page_label": "129", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "482ed358-fee5-48d6-b8ae-28a68d81bd88": {"node_ids": ["116bfcf3-1466-4eeb-863e-e52124d07c4c"], "metadata": {"page_label": "130", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "1720d139-5059-4eef-bc86-48cf875a2f3d": {"node_ids": ["e6274619-0c2d-4350-9efe-7d1d7a7f14e3"], "metadata": {"page_label": "131", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "79e0157c-ec8d-45ed-b9ae-b05e97cf40d0": {"node_ids": ["e2c2892e-c563-44f4-a6c9-dbc56809415d"], "metadata": {"page_label": "132", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "2cc0ac07-6850-4445-af05-252842503dba": {"node_ids": ["cbad8c5d-9e3a-4359-a41f-ee27b76f5860"], "metadata": {"page_label": "133", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "bf7665bc-412f-41de-ae27-21b1f6e753eb": {"node_ids": ["791b711f-69de-4a21-b01c-4283251863ba"], "metadata": {"page_label": "134", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "88809d3f-2ef8-45ae-b4c1-b65680c677a2": {"node_ids": ["c4c48bf6-02d2-4a2e-bb1b-8c17f995d0ba"], "metadata": {"page_label": "135", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "db2aad2b-71ab-4233-9288-efe47b532330": {"node_ids": ["9eaf72d8-ccab-4694-9f02-b63a9f53941b"], "metadata": {"page_label": "136", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "322b07fd-e851-4ecc-a25a-158ef097b19d": {"node_ids": ["2ad807a7-035a-481b-958d-aa719aa1bd62"], "metadata": {"page_label": "137", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "ff0457e6-8a07-421f-a198-602ffd46e3ca": {"node_ids": ["3ce02b55-97ba-45ce-be17-2c560490c541"], "metadata": {"page_label": "138", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "867646ba-cf8a-4481-91ea-e3d92801ed56": {"node_ids": ["d9fd9a61-a918-4f5a-b19a-b808c88eb9b6"], "metadata": {"page_label": "139", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "8f9c8a35-dcf5-4e3e-8dde-bb4225b64a02": {"node_ids": ["0e7699e9-e098-4ccd-af7d-fea0a5aceb08"], "metadata": {"page_label": "140", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3587c476-0adc-4da5-ac3d-a134a161679a": {"node_ids": ["ac93db33-e2f2-4774-b83c-16306a01caa8"], "metadata": {"page_label": "141", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9d0626c5-a740-49b2-9fa3-81bec6a92524": {"node_ids": ["50cbe6a5-d26f-472c-9adc-39be7884befb"], "metadata": {"page_label": "142", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "87da16f3-4133-4ca6-8b5b-86bed7e5d269": {"node_ids": ["ea3b924f-432d-4dd1-aa1d-4504faffe5e5"], "metadata": {"page_label": "143", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "6a077399-770d-4ab8-b0fa-b7f1eee32c1b": {"node_ids": ["b7ae68f5-9604-4a49-a76a-a3789a2972d7"], "metadata": {"page_label": "144", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d04fe36c-7c1e-4c5e-bbac-4b09e88491ff": {"node_ids": ["19cd4ef5-b878-46db-92d4-ccf3ac0ddbca"], "metadata": {"page_label": "145", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "c2a834bf-6d74-40b4-816e-d8c54cf34a3e": {"node_ids": ["384a1e60-6729-453f-9033-ea2e643bd01a"], "metadata": {"page_label": "146", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "09db70e1-3b7d-4701-b128-4698645f97dc": {"node_ids": ["00e7d5c2-a11b-4814-961e-23d8796fe9a2"], "metadata": {"page_label": "147", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "fd90ccbb-7c15-42c9-86be-319284631003": {"node_ids": ["0fd2499f-16f5-4a2b-90cf-fcf54bed62ec"], "metadata": {"page_label": "148", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "23f48073-953d-4899-93ef-0651343e3e8f": {"node_ids": ["5d6b2daa-3afd-4c53-bc88-b8f47961f321"], "metadata": {"page_label": "149", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d1dd4c53-d2a6-4606-8bd9-bc5be7f62886": {"node_ids": ["7f16c85c-87d3-4267-b5d2-a49f008bc64e"], "metadata": {"page_label": "150", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "fa3dd2b3-eaed-4004-8670-47e0622c88d4": {"node_ids": ["8437303b-3315-4973-bc25-743ae12768c6"], "metadata": {"page_label": "151", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9bf1ed3f-0350-4171-b261-32fc86adf950": {"node_ids": ["5ada16fa-0a5b-4fcf-bc85-b700eab5fd8a"], "metadata": {"page_label": "152", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9c54a1e3-b4a3-4db2-a522-983b4aee673a": {"node_ids": ["fd45c0c4-8a82-417a-9488-311ee81969b9"], "metadata": {"page_label": "153", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "1ef3dec6-3329-47a8-8a9f-d19f50b2eac4": {"node_ids": ["68e96cb7-0fee-48c7-a55d-f994f175fa50"], "metadata": {"page_label": "154", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d35f839f-bda0-44e3-b13a-16d7442529b8": {"node_ids": ["4a329bb8-9774-47b7-a955-61ac9d2c941f"], "metadata": {"page_label": "155", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "2a2a826c-0d5d-4894-aca3-36f6217171e2": {"node_ids": ["e95c6fba-5404-4a4b-ba3c-816248317c40"], "metadata": {"page_label": "156", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "6d374a79-2716-4c7f-a9eb-4cc498671e55": {"node_ids": ["728151aa-b55b-4999-bef5-fb14879177ed"], "metadata": {"page_label": "157", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "72a7ede3-dc86-4a0d-b5be-09f55f33385c": {"node_ids": ["f5892d2a-167a-49c7-914f-956c062c0627"], "metadata": {"page_label": "158", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "560c1cab-acdd-48b5-acb2-adb67a47578f": {"node_ids": ["8efcdd85-eb0c-416e-a49f-b8d9ab608a05"], "metadata": {"page_label": "159", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "515f0944-1f53-4423-8c6d-a87eab0f7f95": {"node_ids": ["decd53f1-5f7a-4afe-b645-707889a1216c"], "metadata": {"page_label": "160", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "73adafef-25da-451f-b493-566bd11d5c40": {"node_ids": ["e3a11132-ca32-40b9-8213-790109da8f69"], "metadata": {"page_label": "161", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "a4d672b8-ec88-44f3-bc90-9d86c5d48109": {"node_ids": ["6de09143-cde8-4e52-b50f-029d6a6a5eac"], "metadata": {"page_label": "162", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "13970d91-016a-481f-b307-33161b97b8eb": {"node_ids": ["76348446-a0bb-448f-8a1c-fcccb1a16999"], "metadata": {"page_label": "163", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "aac50e13-4da0-4918-a806-b3ba375fdb04": {"node_ids": ["94c532ce-919b-4b9b-958c-361d74d37de7"], "metadata": {"page_label": "164", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "c7789da2-8273-4181-b40c-9154cd41895c": {"node_ids": ["ae5712b0-124d-42cc-a99c-14c7d65c700a"], "metadata": {"page_label": "165", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "651c4294-609b-410a-8e65-22a4caad1033": {"node_ids": ["40979d75-6731-4286-9dc2-567a27b3fb29"], "metadata": {"page_label": "166", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "fd23dcdb-4230-496a-a80f-e38106f2907a": {"node_ids": ["b89a4454-7b2d-4728-8e37-770b0d0a1b46"], "metadata": {"page_label": "167", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e8f8372e-28d4-459c-a3c4-acc0d9e92a7d": {"node_ids": ["1a8ca033-bdc0-4ebc-ad68-ee5a372aabb2"], "metadata": {"page_label": "168", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d8382130-67d3-4698-8a03-9f5e6787daec": {"node_ids": ["1585ce30-9668-4d0f-ac1c-74624f36322c"], "metadata": {"page_label": "169", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f1999008-d6b1-4f19-a3d0-998631528ad6": {"node_ids": ["79e94695-9480-4661-8700-a24c9252926d"], "metadata": {"page_label": "170", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "852ba98d-947e-4f3e-bd60-85b47344026d": {"node_ids": ["bd6ef9ec-3555-4604-b7fa-25d38b984ef5"], "metadata": {"page_label": "171", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "803f1263-84ad-4d19-9e7e-9ee38a82ab5c": {"node_ids": ["c7a629a0-636f-4d05-a893-46ec916239ec"], "metadata": {"page_label": "172", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "1af94268-07e7-437d-a480-295645bf151c": {"node_ids": ["8916d278-7451-4e51-8c4c-844938a342d7"], "metadata": {"page_label": "173", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3edb30d8-c393-4de2-b658-be1a77b4a650": {"node_ids": ["5672abde-9b9a-4fdb-8e8e-69c76099376f"], "metadata": {"page_label": "174", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9acc31be-fb6d-4ad3-9653-3b8567a06c7f": {"node_ids": ["43d021b0-e35e-4997-8cd0-44001ed9d7d3"], "metadata": {"page_label": "175", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "acb8b2fe-19dd-4459-8e9f-e244c68af468": {"node_ids": ["720d0810-6731-4588-af7e-e73cff16cda9"], "metadata": {"page_label": "176", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "aa2afdb0-3728-4031-a02a-204891e8c9b3": {"node_ids": ["7838610a-58a8-4e0b-9605-7ed75c3f322e"], "metadata": {"page_label": "177", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9a4d7b75-e30a-4b37-9bce-dcbbbae793de": {"node_ids": ["2468823a-ea15-47dd-a3e9-1e7069d87574"], "metadata": {"page_label": "178", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f568230d-3a92-4131-a13c-e1a0b50a617e": {"node_ids": ["1daeba27-ba33-4235-8409-54ce054bd6ed"], "metadata": {"page_label": "179", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "445396a4-bf24-41df-bb07-c1006109eca4": {"node_ids": ["ee66eb48-2e1f-42ac-927e-8f38364ca8f8"], "metadata": {"page_label": "180", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "0410652a-7d46-411e-bd7e-f8b3011562dd": {"node_ids": ["9640be44-c19f-45f1-876b-ec09e99ee6eb"], "metadata": {"page_label": "181", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "160275d7-e07b-4540-b4fb-84e02885897b": {"node_ids": ["84d4066e-d111-433c-b1ea-ce9fa6c0078f"], "metadata": {"page_label": "182", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "eccbc706-8b36-4643-ac26-35e67e98a2ee": {"node_ids": ["895f0c71-f883-44c9-a79a-91536a2e3ae0"], "metadata": {"page_label": "183", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "6076a4a4-5a25-4e15-a6e8-3eca1aa2cf6c": {"node_ids": ["05fa6c71-e460-49fc-98af-052af603b48a"], "metadata": {"page_label": "184", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "eb977b19-ef57-4f49-bb58-c50a9eb3bef1": {"node_ids": ["acf1d12f-f167-4d6c-9712-41b183869668"], "metadata": {"page_label": "185", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "8e5efcaa-2557-437b-98d7-f2856ebad259": {"node_ids": ["654f7ac5-3dde-4845-9a81-6f6e3895bb89"], "metadata": {"page_label": "186", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "153ac4c6-6ee3-4845-a037-aa7f54d2c5f2": {"node_ids": ["41a99df0-b334-477b-8459-7ed49fc26891"], "metadata": {"page_label": "187", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "5d2fade2-e276-42e7-a997-cc4ae8877b43": {"node_ids": ["e0d1b15d-4966-4b13-a5ca-c17dbb3d1c0a"], "metadata": {"page_label": "188", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e77a9e55-53ef-4dba-9d06-1b7d6f0f255c": {"node_ids": ["8971917a-81c5-4bf1-89ee-ffc8939b3256"], "metadata": {"page_label": "189", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9c491f2d-3df4-4af7-951f-5228dfc4fa95": {"node_ids": ["28c31606-5e86-4826-884e-5313d1597091"], "metadata": {"page_label": "190", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "4750bf8b-3a68-4c1e-9779-bf8d701a9b22": {"node_ids": ["66bc73e3-b98a-4189-ab07-d00ec4fa5ab6"], "metadata": {"page_label": "191", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "1c6439f7-2cb9-47db-b37e-559a7b28adc2": {"node_ids": ["635e9b56-def0-4ecf-98fa-ea006743d31e"], "metadata": {"page_label": "192", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "4e61c6e8-2b9a-4906-b237-821659570f63": {"node_ids": ["3395945f-3e01-4358-b8f0-cf17d89f5cb4"], "metadata": {"page_label": "193", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d4fe3759-8fde-47ee-a62c-293b66b8da52": {"node_ids": ["935a4940-81f8-4af2-9295-c1becf22dbc5"], "metadata": {"page_label": "194", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e3e5aa5a-c374-4387-99a4-a9cc19d555a5": {"node_ids": ["be4ebb7a-4e2e-416c-a43b-a76a6578d47f"], "metadata": {"page_label": "195", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "1ce3c3fe-f692-44a2-9dde-97a2ae12cd75": {"node_ids": ["9d74a19c-8be9-4bb1-9b9a-15116c23594f"], "metadata": {"page_label": "196", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "c9fafd24-eee8-49a4-8eb2-ce712306c18f": {"node_ids": ["4b0147a0-ea81-4048-8867-695fce1b4572"], "metadata": {"page_label": "197", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3aa269f0-73fa-4dc0-8c3c-f3266ad8ba61": {"node_ids": ["f7b28bef-65d3-4474-b1c0-67f52e416957"], "metadata": {"page_label": "198", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f20fd27b-836f-4a34-8f7f-45cb35b82c5d": {"node_ids": ["5a2feebb-3390-43eb-9f3e-c75e45cf1df2"], "metadata": {"page_label": "199", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "65cb6302-11ec-410a-a09d-d363e6681b30": {"node_ids": ["b3fb5069-dbe1-402e-8c45-a02b37f7a667"], "metadata": {"page_label": "200", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "60c59991-ce38-4238-8ec0-9c246db55037": {"node_ids": ["7c78b4d1-4479-4180-84e6-896b52a49b59"], "metadata": {"page_label": "201", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "fe439298-cfe0-4463-97ab-99bc311cc931": {"node_ids": ["c9825965-5697-4662-8c4a-4c232afbdbb6"], "metadata": {"page_label": "202", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d7821250-0b01-4a2b-b2bf-6dd63571a49c": {"node_ids": ["3f554e09-0eb2-456d-9f3d-fe63303ab00e"], "metadata": {"page_label": "203", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "72de8e2d-7d97-49fb-b8b7-f70a531dc765": {"node_ids": ["0c6e2764-d9a1-430b-a1ca-7d80ffa8b4c2"], "metadata": {"page_label": "204", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "c61a6dc0-556d-4c76-8a8f-55b540a044a5": {"node_ids": ["d93531ab-9003-4784-84ba-6c74b997a25e"], "metadata": {"page_label": "205", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "a8b5e6cb-2084-4194-b3f9-2e0d28a2ace9": {"node_ids": ["c68d6c15-2f0f-462b-8ae5-49292eac689f"], "metadata": {"page_label": "206", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "b972f8f1-81bc-4f21-8e89-af214d24de53": {"node_ids": ["68523497-0887-4eee-aca8-9de6ee6bf0f9"], "metadata": {"page_label": "207", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "387d9dbc-ed88-4627-8c63-ef80b0c85ca6": {"node_ids": ["b66df359-dde9-4911-aabe-45a689637520"], "metadata": {"page_label": "208", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "53e143b1-a573-4c0c-8bda-5a12b768494f": {"node_ids": ["a2d474bb-9bba-421f-a30f-0767ea823b31"], "metadata": {"page_label": "209", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9ebb6fa7-18d2-45c8-9731-195261311d58": {"node_ids": ["6132f262-57ae-4b6f-bc6f-f86cc7599bec"], "metadata": {"page_label": "210", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f9a8ade6-6a44-4351-b38e-5d7af3a0d849": {"node_ids": ["083b6a6c-d08f-48b0-a1a2-fe471269d76d"], "metadata": {"page_label": "211", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "c04d2823-7a3b-4c65-8ac6-b5c059b0d586": {"node_ids": ["0deb6b37-d365-4fdb-93c6-5fe75d044021"], "metadata": {"page_label": "212", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "19e899f6-58ed-4912-b450-067a8dc6ec76": {"node_ids": ["f6cac2cc-b0c5-4f03-8137-bd876edbb7e2"], "metadata": {"page_label": "213", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "990864f0-e4a3-491a-80d8-ef1f888d2243": {"node_ids": ["d34280c3-02fc-46d0-9c2c-11a8f3309459"], "metadata": {"page_label": "214", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "b7a5fa17-2272-4e4f-b9ff-42eb33351f00": {"node_ids": ["c87c573f-5609-4301-9efe-b9ff27db84d7"], "metadata": {"page_label": "215", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "6bee99d0-8710-4e05-b350-1a7b57085706": {"node_ids": ["fc4f1bb8-5f66-492d-8840-0903fe9f5bcd"], "metadata": {"page_label": "216", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e9a340b0-98de-4ddb-912b-980b3e4c3e0b": {"node_ids": ["aca4ee3b-e73c-48ef-9820-c69dfa3f7e37"], "metadata": {"page_label": "217", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "156107b4-353f-4f28-918a-3c38c94f1ef9": {"node_ids": ["c55e5a51-0bd0-4cd9-bd65-1b8f79485b83"], "metadata": {"page_label": "218", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "c0c0822f-7956-4b29-83a5-dbbd48fa902f": {"node_ids": ["e29ca47f-1b41-4bc3-b494-ecc9c043f8c8"], "metadata": {"page_label": "219", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d655ab0b-800d-40ff-9906-a46518fc0a1a": {"node_ids": ["b17a7dfc-24b0-48d5-802b-49a2769446a9"], "metadata": {"page_label": "220", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "bd1a7fd4-54d6-49af-851d-bbfa6d40550a": {"node_ids": ["f9421257-8b15-4f71-bbe7-03f0cba9668c"], "metadata": {"page_label": "221", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "94b9351a-4afa-4bd5-adf6-1cf345338c7f": {"node_ids": ["8eeb7705-a472-4312-9022-1ac3f11d3165"], "metadata": {"page_label": "222", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "178aaf7f-bf08-461a-a57f-c12c4f9ce958": {"node_ids": ["7b5dbb7f-c518-4603-8b8b-537c65ff3972"], "metadata": {"page_label": "223", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "80a5ac38-0c86-4869-b25a-e8d6092cbb24": {"node_ids": ["abe32742-e3bc-4286-ab77-ea0bbed371fe"], "metadata": {"page_label": "224", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "2e961a04-5386-45cd-81a9-071811a90251": {"node_ids": ["e89e8891-2c04-4b4d-853d-4f2f4afa0282"], "metadata": {"page_label": "225", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "da4c5e64-8583-4745-b487-525f62c94d4a": {"node_ids": ["e02884a7-ef3a-4e64-b487-e10ae0cc87d2"], "metadata": {"page_label": "226", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f17a5f6e-aba6-43dd-b16c-ce2f8f0ccda5": {"node_ids": ["d20cc65f-4a99-4f59-a936-4a56d2f5192b"], "metadata": {"page_label": "227", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "227d6bfd-12ef-4b74-976a-7e9a1172a4c0": {"node_ids": ["dc0a468e-9a50-4ff1-a0e4-44cd6ca95946"], "metadata": {"page_label": "228", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "baa93e5e-7e1b-4cfe-b6ec-84a834f32975": {"node_ids": ["729f5c96-5038-41f1-8c24-27e01b876eb5"], "metadata": {"page_label": "229", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "a4c97274-2275-4e9b-bc89-c25afe47e7bf": {"node_ids": ["9acb9d48-f704-477a-8820-8620bee4aca0"], "metadata": {"page_label": "230", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "8cb2d50a-b403-4f28-a5df-fa8fb3e5b5fa": {"node_ids": ["bc690db6-089f-44ef-98d3-afbbc92ef03d"], "metadata": {"page_label": "231", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "8d7c013c-ce75-407d-b7e4-61a1f819cad7": {"node_ids": ["d27bbdfe-d8f7-4e2f-805d-8c4f68e43214"], "metadata": {"page_label": "232", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9360c30c-183b-45b2-ad4e-c6e285753388": {"node_ids": ["823c97ec-8e0e-43b7-91c7-5361c3f40d2d"], "metadata": {"page_label": "233", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "25de1824-7c41-471e-a874-be67ccfcd70e": {"node_ids": ["56fe13e5-7223-4838-bdc5-4cac69a914f8"], "metadata": {"page_label": "234", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "57201e05-589e-46d0-b4bb-a6c0a7ef6427": {"node_ids": ["16f87239-f9ab-4251-bf25-05d146002323"], "metadata": {"page_label": "235", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "9ae2302c-4b63-44dd-af6d-8ef751b710c8": {"node_ids": ["50d00309-9cf0-494a-aa60-7632293cc74c"], "metadata": {"page_label": "236", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "981f28d3-ec7c-414f-98d0-54474de618d1": {"node_ids": ["40531ee5-37a3-4f9a-a585-e533b8c37ccd"], "metadata": {"page_label": "237", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "56daa91c-dac8-414b-8906-123be8275ce4": {"node_ids": ["c4fcf5e4-1486-4698-9e27-c7cc51fc30f9"], "metadata": {"page_label": "238", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "e35e66e4-8188-40cd-98a1-63f018ebdd8e": {"node_ids": ["0be00f96-1ca3-4829-a8b6-c8d801f47d78"], "metadata": {"page_label": "239", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "61471dc9-906e-49a9-a1fc-43540c3c2cfd": {"node_ids": ["a1bd49f8-d525-4600-bfd7-94b58caf6198"], "metadata": {"page_label": "240", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d258ce96-c3af-45f2-bb04-7f477a1d8a2b": {"node_ids": ["c394b4cd-ba3e-4d5e-b491-fef5907482df"], "metadata": {"page_label": "241", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "3ecdd703-75ad-4316-91cf-8c559261756d": {"node_ids": ["8aee6c0f-75e9-42fc-9f13-9de06dcfb740"], "metadata": {"page_label": "242", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "f512b869-bf03-4263-9f54-b16539c58706": {"node_ids": ["e9e39198-5f3b-4476-bb38-91eba4407fb4"], "metadata": {"page_label": "243", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d52a3df3-cb27-4a26-bc9a-534ab5fb2ed1": {"node_ids": ["7c159c44-2e7d-4745-b8ff-baa5598ffe1b"], "metadata": {"page_label": "244", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "16a0db4a-3df9-4260-a93f-dc4022691c53": {"node_ids": ["ea078fda-849d-4a36-ba6d-d71dc781a018"], "metadata": {"page_label": "245", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "48a76c2b-1991-409d-9f66-3a43ea332a37": {"node_ids": ["8d02b82e-5585-4893-b764-292f4333a157"], "metadata": {"page_label": "246", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "d7db6e3a-3fee-4d15-87c7-da8be83afd01": {"node_ids": ["30e07b5d-b825-4220-9ead-f879a148724b"], "metadata": {"page_label": "247", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "11dccb50-ea5b-447f-b4ad-c8fbd5501119": {"node_ids": ["ccfa0a3f-b14d-4150-a6fc-dad8bb6c193f"], "metadata": {"page_label": "248", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}, "54eecb9d-e31c-4c3a-899d-3e1b036f0f60": {"node_ids": ["44f83b38-7e01-4f99-8364-b8afe8ca798b"], "metadata": {"page_label": "249", "file_name": "Learning Scrapy.pdf", "file_path": "C:\\Users\\Coki_Zhao\\Desktop\\temp\\Learning Scrapy.pdf", "file_type": "application/pdf", "file_size": 18872298, "creation_date": "2024-08-02", "last_modified_date": "2024-04-09"}}}}